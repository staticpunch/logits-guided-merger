{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e91e4bd-dd42-4c21-b5ea-8f4a2ee3ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model merging training implementation using PyTorch and Transformers.\n",
    "Implements custom data collation and training for merged language models.\n",
    "\"\"\"\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union, Mapping\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    default_data_collator,\n",
    "    is_torch_xla_available,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from accurate_masks import (\n",
    "# from efficient_masks import (\n",
    "    MergerConfig,\n",
    "    Merger,\n",
    "    NewMerger,\n",
    "    init_masks,\n",
    "    set_masks\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    generate, \n",
    "    get_hidden_states, \n",
    "    get_logits,\n",
    "    free_memory\n",
    ")\n",
    "# Configure logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd01048a-09b5-43db-97ad-e81bd9ca6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Option 1: Set specific GPU devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c21b6f1-47a4-4bea-b81b-86f57543f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Handles data loading and preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load and prepare the training dataset.\"\"\"\n",
    "        summarize_train = load_dataset(\n",
    "            \"HuggingFaceTB/smoltalk\",\n",
    "            \"smol-summarize\",\n",
    "            split=\"train\"\n",
    "        )\n",
    "        summarize_train = summarize_train.add_column(\n",
    "            name=\"data_source\",\n",
    "            column=[1 for _ in summarize_train]\n",
    "        )\n",
    "        return summarize_train.shuffle(seed=42).select(range(5000))\n",
    "    \n",
    "    def tokenize(self, element):\n",
    "        \"\"\"Tokenize a single element from the dataset.\"\"\"\n",
    "        templated = self.tokenizer.apply_chat_template(\n",
    "            element[\"messages\"],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        return self.tokenizer(\n",
    "            templated,\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "            add_special_tokens=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe1946d-df3e-4b95-8207-877c5dfebd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n",
    "    \"\"\"\n",
    "    Pads without triggering the warning about how using the pad function is sub-optimal when using a fast tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid errors when using Feature extractors\n",
    "    if not hasattr(tokenizer, \"deprecation_warnings\"):\n",
    "        return tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "\n",
    "    # Save the state of the warning, then disable it\n",
    "    warning_state = tokenizer.deprecation_warnings.get(\"Asking-to-pad-a-fast-tokenizer\", False)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    try:\n",
    "        padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "    finally:\n",
    "        # Restore the state of the warning.\n",
    "        tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n",
    "\n",
    "    return padded\n",
    "\n",
    "@dataclass\n",
    "class MergerDataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        \"\"\"\n",
    "        copied from DataCollatorForLanguageModeling\n",
    "        examples: List[Union[List[int], Any, Dict[str, Any]]]\n",
    "        \"\"\"\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if not isinstance(examples[0], Mapping):\n",
    "            raise ValueError(\"Data collator only processes list of dictionaries.\")\n",
    "\n",
    "        inputs_ids = []\n",
    "        data_sources = []\n",
    "        for i in range(len(examples)):\n",
    "            _ = examples[i].pop(\"attention_mask\")\n",
    "            inputs_ids.append({\"input_ids\": examples[i].pop(\"input_ids\")})\n",
    "            data_sources.append(examples[i].pop(\"data_source\"))\n",
    "            \n",
    "        batch = pad_without_fast_tokenizer_warning(\n",
    "            self.tokenizer, inputs_ids, return_tensors=\"pt\", \n",
    "            pad_to_multiple_of=self.pad_to_multiple_of\n",
    "        )\n",
    "\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        if self.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        # Handle data_source - convert to tensor\n",
    "        batch[\"data_source\"] = torch.tensor(\n",
    "            [src for src in data_sources], dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        for key in examples[0]:\n",
    "            if key in batch:\n",
    "                raise ValueError(\n",
    "                    f\"`{key}` feature is collated. \"\n",
    "                    \"Overriding it with its initial values is prohibitted.\"\n",
    "                )\n",
    "            else:\n",
    "                batch[key] = [x[key] for x in examples]\n",
    "        logger.info_once(f\">>> Collator output keys: {batch.keys()}\")\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0891539-136e-4a0e-95d9-32767e42f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_logits_target(logits_components, data_source):\n",
    "    \"\"\"Select appropriate logits based on data source.\"\"\"\n",
    "    stacked_logits = torch.stack(logits_components)\n",
    "    indices = data_source.unsqueeze(-1).unsqueeze(-1)\n",
    "    return stacked_logits[indices]\n",
    "\n",
    "class MergerTrainer(Trainer):\n",
    "    \"\"\"Custom trainer for merged model training.\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        data_source = inputs.pop(\"data_source\")\n",
    "        effective_idxs = (labels != -100).float().unsqueeze(dim=-1)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits_merged = outputs[\"merger_outputs\"].logits\n",
    "        logits_components = [x.logits for x in outputs[\"components_outputs\"]]\n",
    "\n",
    "        # Compute target logits and KL divergence\n",
    "        logits_target = selective_logits_target(logits_components, data_source)\n",
    "        temperature = 1.0\n",
    "        kl_fct = nn.KLDivLoss(reduction=\"none\")\n",
    "        diff = (\n",
    "            kl_fct(\n",
    "                F.log_softmax(logits_target / temperature, dim=-1),\n",
    "                F.softmax(logits_merged / temperature, dim=-1)\n",
    "            )\n",
    "            * (temperature) ** 2\n",
    "        )\n",
    "        \n",
    "        # Calculate final loss\n",
    "        loss = (diff * effective_idxs).sum(dim=-1)\n",
    "        loss = (loss / effective_idxs.sum(dim=1)).mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5696e43c-d407-4953-8b81-28ca34a23c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    model_name: str = \"...\"  # You can replace this with any causal language model from HuggingFace\n",
    "    dataset_name: str = \"...\"  # Replace with your dataset name (e.g., \"your_username/your_dataset\")\n",
    "    train_split: str = \"train\"  # e.g., \"train[:80%]\" for an 80/20 train/validation split\n",
    "    validation_split: str = None  # e.g., \"train[80%:]\"\n",
    "    output_dir: str = \"./trained_masks\"\n",
    "    per_device_train_batch_size: int = 2\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 16\n",
    "    learning_rate: float = 3e-2\n",
    "    num_train_epochs: int = 4\n",
    "    save_steps: int = 1000\n",
    "    eval_steps: int = 5000\n",
    "    logging_steps: int = 10\n",
    "    logging_dir: str = \"./trained_masks/logs\"\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    report_to: str = None\n",
    "    remove_unused_columns: bool = False\n",
    "    logging_first_step: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75bc334-1c97-4311-93a3-571ffe40f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 15:59:59,838 - INFO - Creating merger with dummy weights ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8674a2cf27574f0fbc297ee289c29bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5056972468014a6cb5d0d0e4a1292018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2827953fe90e4da8aa0746bf8c8a65d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|█▎                                                                                                                                                                            | 2/255 [00:09<19:28,  4.62s/it]2025-01-05 16:00:12,831 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:40<00:00,  6.37it/s]\n",
      "Setting up masks: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:00<00:00, 35078.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize configuration\n",
    "merge_config = MergerConfig(\n",
    "    model_paths=[\n",
    "        \"nguyenthanhdo/llama32_smol_rewrite_50k\",\n",
    "        \"nguyenthanhdo/llama32_smol_summarize_50k\",\n",
    "    ],\n",
    "    mode=\"vector_input\",\n",
    "    constrain_mode=\"01\"\n",
    ")\n",
    "\n",
    "# Setup tokenizer and data processing\n",
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_processor = DataProcessor(tokenizer)\n",
    "train_dataset = data_processor.load_dataset()\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    data_processor.tokenize,\n",
    "    remove_columns=[\"messages\"]\n",
    ")\n",
    "\n",
    "# Initialize merger model\n",
    "merger = NewMerger.from_pretrained(\n",
    "    None,\n",
    "    merge_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "set_masks(merger.merger, strategy=\"uniform\", factors=[0.95, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0881557b-1b22-4f8b-8694-8274dd12d931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 16:00:43,445 - INFO - Initial GPU memory allocated: 0.00 GB\n",
      "2025-01-05 16:00:43,606 - INFO - Final GPU memory allocated: 0.00 GB\n",
      "2025-01-05 16:00:43,607 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Monitor memory usage\n",
    "initial_memory = torch.cuda.memory_allocated()\n",
    "logger.info(f\"Initial GPU memory allocated: {initial_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "final_memory = torch.cuda.memory_allocated()\n",
    "logger.info(f\"Final GPU memory allocated: {final_memory / 1024**3:.2f} GB\")\n",
    "logger.info(f\"Freed GPU memory: {(initial_memory - final_memory) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b80a1459-db79-4b6d-b48f-abe251903369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup training arguments and data collator\n",
    "args = Args()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.output_dir,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    learning_rate=args.learning_rate,\n",
    "    num_train_epochs=args.num_train_epochs,\n",
    "    save_steps=args.save_steps,\n",
    "    evaluation_strategy=args.evaluation_strategy if args.validation_split else \"no\",\n",
    "    eval_steps=args.eval_steps if args.validation_split else None,\n",
    "    logging_steps=args.logging_steps,\n",
    "    logging_dir=args.logging_dir,\n",
    "    report_to=args.report_to,  # Enable TensorBoard logging\n",
    "    remove_unused_columns=args.remove_unused_columns,\n",
    "    logging_first_step=args.logging_first_step\n",
    ")\n",
    "\n",
    "data_collator = MergerDataCollator(\n",
    "    tokenizer,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Initialize and start training\n",
    "trainer = MergerTrainer(\n",
    "    model=merger,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ffdf19-9b4c-4394-a6bf-fe91626a749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 16:00:57,001 - INFO - >>> Collator output keys: dict_keys(['input_ids', 'attention_mask', 'labels', 'data_source'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 32/624 02:24 < 47:33, 0.21 it/s, Epoch 0.20/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
