{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e91e4bd-dd42-4c21-b5ea-8f4a2ee3ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model merging training implementation using PyTorch and Transformers.\n",
    "Implements custom data collation and training for merged language models.\n",
    "\"\"\"\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union, Mapping\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    default_data_collator,\n",
    "    is_torch_xla_available,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from accurate_masks import (\n",
    "# from efficient_masks import (\n",
    "    MergerConfig,\n",
    "    # Merger,\n",
    "    NewMerger,\n",
    "    init_masks,\n",
    "    set_masks\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    generate, \n",
    "    get_hidden_states, \n",
    "    get_logits,\n",
    "    free_memory\n",
    ")\n",
    "# Configure logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd01048a-09b5-43db-97ad-e81bd9ca6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Option 1: Set specific GPU devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c21b6f1-47a4-4bea-b81b-86f57543f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Handles data loading and preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load and prepare the training dataset.\"\"\"\n",
    "        summarize_train = load_dataset(\n",
    "            \"HuggingFaceTB/smoltalk\",\n",
    "            \"smol-summarize\",\n",
    "            split=\"train\"\n",
    "        )\n",
    "        summarize_train = summarize_train.add_column(\n",
    "            name=\"data_source\",\n",
    "            column=[1 for _ in summarize_train]\n",
    "        )\n",
    "        return summarize_train.shuffle(seed=42).select(range(30000))\n",
    "    \n",
    "    def tokenize(self, element):\n",
    "        \"\"\"Tokenize a single element from the dataset.\"\"\"\n",
    "        templated = self.tokenizer.apply_chat_template(\n",
    "            element[\"messages\"],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        return self.tokenizer(\n",
    "            templated,\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "            add_special_tokens=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe1946d-df3e-4b95-8207-877c5dfebd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n",
    "    \"\"\"\n",
    "    Pads without triggering the warning about how using the pad function is sub-optimal when using a fast tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid errors when using Feature extractors\n",
    "    if not hasattr(tokenizer, \"deprecation_warnings\"):\n",
    "        return tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "\n",
    "    # Save the state of the warning, then disable it\n",
    "    warning_state = tokenizer.deprecation_warnings.get(\"Asking-to-pad-a-fast-tokenizer\", False)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    try:\n",
    "        padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "    finally:\n",
    "        # Restore the state of the warning.\n",
    "        tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n",
    "\n",
    "    return padded\n",
    "\n",
    "@dataclass\n",
    "class MergerDataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        \"\"\"\n",
    "        copied from DataCollatorForLanguageModeling\n",
    "        examples: List[Union[List[int], Any, Dict[str, Any]]]\n",
    "        \"\"\"\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if not isinstance(examples[0], Mapping):\n",
    "            raise ValueError(\"Data collator only processes list of dictionaries.\")\n",
    "\n",
    "        inputs_ids = []\n",
    "        data_sources = []\n",
    "        for i in range(len(examples)):\n",
    "            _ = examples[i].pop(\"attention_mask\")\n",
    "            inputs_ids.append({\"input_ids\": examples[i].pop(\"input_ids\")})\n",
    "            data_sources.append(examples[i].pop(\"data_source\"))\n",
    "            \n",
    "        batch = pad_without_fast_tokenizer_warning(\n",
    "            self.tokenizer, inputs_ids, return_tensors=\"pt\", \n",
    "            pad_to_multiple_of=self.pad_to_multiple_of\n",
    "        )\n",
    "\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        if self.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        # Handle data_source - convert to tensor\n",
    "        batch[\"data_source\"] = torch.tensor(\n",
    "            [src for src in data_sources], dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        for key in examples[0]:\n",
    "            if key in batch:\n",
    "                raise ValueError(\n",
    "                    f\"`{key}` feature is collated. \"\n",
    "                    \"Overriding it with its initial values is prohibitted.\"\n",
    "                )\n",
    "            else:\n",
    "                batch[key] = [x[key] for x in examples]\n",
    "        logger.info_once(f\">>> Collator output keys: {batch.keys()}\")\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0891539-136e-4a0e-95d9-32767e42f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_logits_target(logits_components, data_source):\n",
    "    \"\"\"Select appropriate logits based on data source.\"\"\"\n",
    "    stacked_logits = torch.stack(logits_components)\n",
    "    indices = data_source.unsqueeze(-1).unsqueeze(-1)\n",
    "    return stacked_logits[indices]\n",
    "\n",
    "class MergerTrainer(Trainer):\n",
    "    \"\"\"Custom trainer for merged model training.\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        data_source = inputs.pop(\"data_source\")\n",
    "        effective_idxs = (labels != -100).float().unsqueeze(dim=-1)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits_merged = outputs[\"merger_outputs\"].logits\n",
    "        logits_components = [x.logits for x in outputs[\"components_outputs\"]]\n",
    "\n",
    "        # Compute target logits and KL divergence\n",
    "        logits_target = selective_logits_target(logits_components, data_source)\n",
    "        temperature = 1.0\n",
    "        kl_fct = nn.KLDivLoss(reduction=\"none\")\n",
    "        diff = (\n",
    "            kl_fct(\n",
    "                F.log_softmax(logits_target / temperature, dim=-1),\n",
    "                F.softmax(logits_merged / temperature, dim=-1)\n",
    "            )\n",
    "            * (temperature) ** 2\n",
    "        )\n",
    "        \n",
    "        # Calculate final loss\n",
    "        loss = (diff * effective_idxs).sum(dim=-1)\n",
    "        loss = (loss / effective_idxs.sum(dim=1)).mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5696e43c-d407-4953-8b81-28ca34a23c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    model_name: str = \"...\"  # You can replace this with any causal language model from HuggingFace\n",
    "    dataset_name: str = \"...\"  # Replace with your dataset name (e.g., \"your_username/your_dataset\")\n",
    "    train_split: str = \"train\"  # e.g., \"train[:80%]\" for an 80/20 train/validation split\n",
    "    validation_split: str = None  # e.g., \"train[80%:]\"\n",
    "    output_dir: str = \"./trained_masks\"\n",
    "    per_device_train_batch_size: int = 2\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 16\n",
    "    learning_rate: float = 3e-2\n",
    "    num_train_epochs: int = 1\n",
    "    save_steps: int = 50\n",
    "    eval_steps: int = 5000\n",
    "    logging_steps: int = 10\n",
    "    logging_dir: str = \"./trained_masks/logs\"\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    report_to: str = None\n",
    "    remove_unused_columns: bool = False\n",
    "    logging_first_step: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75bc334-1c97-4311-93a3-571ffe40f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 04:43:13,644 - INFO - Creating merger with dummy weights ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a454dd33bf544e6aa4e1955998cf8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40da972517f2465c9268854637fb5ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e652748720b54b21a58a64b78be16861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|█▎                                                                                                                                                                            | 2/255 [00:08<17:28,  4.14s/it]2025-01-06 04:43:24,983 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:33<00:00,  7.68it/s]\n",
      "Setting up masks: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:00<00:00, 36882.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize configuration\n",
    "merge_config = MergerConfig(\n",
    "    model_paths=[\n",
    "        \"nguyenthanhdo/llama32_smol_rewrite_50k\",\n",
    "        \"nguyenthanhdo/llama32_smol_summarize_50k\",\n",
    "    ],\n",
    "    mode=\"vector_input\",\n",
    "    constrain_mode=\"identity\"\n",
    ")\n",
    "\n",
    "# Setup tokenizer and data processing\n",
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_processor = DataProcessor(tokenizer)\n",
    "train_dataset = data_processor.load_dataset()\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    data_processor.tokenize,\n",
    "    remove_columns=[\"messages\"]\n",
    ")\n",
    "\n",
    "# Initialize merger model\n",
    "merger = NewMerger.from_pretrained(\n",
    "    None,\n",
    "    merge_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "set_masks(merger.merger, strategy=\"uniform\", factors=[0.95, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2483c3a0-6ae9-4d36-8a90-684494f08dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file as safe_load_file\n",
    "state_dict = safe_load_file(\"./trained_masks/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b974a02b-0029-482e-b156-468fd1cfb2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503346"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_count = 0\n",
    "for k, v in state_dict.items():\n",
    "    trainable_count += v.numel()\n",
    "trainable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b0df58b-0d9b-4148-a14d-29967375e279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 04:48:59,510 - INFO - Creating merger with dummy weights ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc7afe171d741578783ff751dcc356c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed982d069b6247059218e68243f59bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed975c81318f4b648f6459253268a243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:28<00:00,  9.02it/s]\n",
      "2025-01-06 04:49:32,217 - INFO - Missing keys: ['models.0.model.embed_tokens.weight', 'models.0.model.layers.0.self_attn.q_proj.weight', 'models.0.model.layers.0.self_attn.k_proj.weight', 'models.0.model.layers.0.self_attn.v_proj.weight', 'models.0.model.layers.0.self_attn.o_proj.weight', 'models.0.model.layers.0.mlp.gate_proj.weight', 'models.0.model.layers.0.mlp.up_proj.weight', 'models.0.model.layers.0.mlp.down_proj.weight', 'models.0.model.layers.0.input_layernorm.weight', 'models.0.model.layers.0.post_attention_layernorm.weight', 'models.0.model.layers.1.self_attn.q_proj.weight', 'models.0.model.layers.1.self_attn.k_proj.weight', 'models.0.model.layers.1.self_attn.v_proj.weight', 'models.0.model.layers.1.self_attn.o_proj.weight', 'models.0.model.layers.1.mlp.gate_proj.weight', 'models.0.model.layers.1.mlp.up_proj.weight', 'models.0.model.layers.1.mlp.down_proj.weight', 'models.0.model.layers.1.input_layernorm.weight', 'models.0.model.layers.1.post_attention_layernorm.weight', 'models.0.model.layers.2.self_attn.q_proj.weight', 'models.0.model.layers.2.self_attn.k_proj.weight', 'models.0.model.layers.2.self_attn.v_proj.weight', 'models.0.model.layers.2.self_attn.o_proj.weight', 'models.0.model.layers.2.mlp.gate_proj.weight', 'models.0.model.layers.2.mlp.up_proj.weight', 'models.0.model.layers.2.mlp.down_proj.weight', 'models.0.model.layers.2.input_layernorm.weight', 'models.0.model.layers.2.post_attention_layernorm.weight', 'models.0.model.layers.3.self_attn.q_proj.weight', 'models.0.model.layers.3.self_attn.k_proj.weight', 'models.0.model.layers.3.self_attn.v_proj.weight', 'models.0.model.layers.3.self_attn.o_proj.weight', 'models.0.model.layers.3.mlp.gate_proj.weight', 'models.0.model.layers.3.mlp.up_proj.weight', 'models.0.model.layers.3.mlp.down_proj.weight', 'models.0.model.layers.3.input_layernorm.weight', 'models.0.model.layers.3.post_attention_layernorm.weight', 'models.0.model.layers.4.self_attn.q_proj.weight', 'models.0.model.layers.4.self_attn.k_proj.weight', 'models.0.model.layers.4.self_attn.v_proj.weight', 'models.0.model.layers.4.self_attn.o_proj.weight', 'models.0.model.layers.4.mlp.gate_proj.weight', 'models.0.model.layers.4.mlp.up_proj.weight', 'models.0.model.layers.4.mlp.down_proj.weight', 'models.0.model.layers.4.input_layernorm.weight', 'models.0.model.layers.4.post_attention_layernorm.weight', 'models.0.model.layers.5.self_attn.q_proj.weight', 'models.0.model.layers.5.self_attn.k_proj.weight', 'models.0.model.layers.5.self_attn.v_proj.weight', 'models.0.model.layers.5.self_attn.o_proj.weight', 'models.0.model.layers.5.mlp.gate_proj.weight', 'models.0.model.layers.5.mlp.up_proj.weight', 'models.0.model.layers.5.mlp.down_proj.weight', 'models.0.model.layers.5.input_layernorm.weight', 'models.0.model.layers.5.post_attention_layernorm.weight', 'models.0.model.layers.6.self_attn.q_proj.weight', 'models.0.model.layers.6.self_attn.k_proj.weight', 'models.0.model.layers.6.self_attn.v_proj.weight', 'models.0.model.layers.6.self_attn.o_proj.weight', 'models.0.model.layers.6.mlp.gate_proj.weight', 'models.0.model.layers.6.mlp.up_proj.weight', 'models.0.model.layers.6.mlp.down_proj.weight', 'models.0.model.layers.6.input_layernorm.weight', 'models.0.model.layers.6.post_attention_layernorm.weight', 'models.0.model.layers.7.self_attn.q_proj.weight', 'models.0.model.layers.7.self_attn.k_proj.weight', 'models.0.model.layers.7.self_attn.v_proj.weight', 'models.0.model.layers.7.self_attn.o_proj.weight', 'models.0.model.layers.7.mlp.gate_proj.weight', 'models.0.model.layers.7.mlp.up_proj.weight', 'models.0.model.layers.7.mlp.down_proj.weight', 'models.0.model.layers.7.input_layernorm.weight', 'models.0.model.layers.7.post_attention_layernorm.weight', 'models.0.model.layers.8.self_attn.q_proj.weight', 'models.0.model.layers.8.self_attn.k_proj.weight', 'models.0.model.layers.8.self_attn.v_proj.weight', 'models.0.model.layers.8.self_attn.o_proj.weight', 'models.0.model.layers.8.mlp.gate_proj.weight', 'models.0.model.layers.8.mlp.up_proj.weight', 'models.0.model.layers.8.mlp.down_proj.weight', 'models.0.model.layers.8.input_layernorm.weight', 'models.0.model.layers.8.post_attention_layernorm.weight', 'models.0.model.layers.9.self_attn.q_proj.weight', 'models.0.model.layers.9.self_attn.k_proj.weight', 'models.0.model.layers.9.self_attn.v_proj.weight', 'models.0.model.layers.9.self_attn.o_proj.weight', 'models.0.model.layers.9.mlp.gate_proj.weight', 'models.0.model.layers.9.mlp.up_proj.weight', 'models.0.model.layers.9.mlp.down_proj.weight', 'models.0.model.layers.9.input_layernorm.weight', 'models.0.model.layers.9.post_attention_layernorm.weight', 'models.0.model.layers.10.self_attn.q_proj.weight', 'models.0.model.layers.10.self_attn.k_proj.weight', 'models.0.model.layers.10.self_attn.v_proj.weight', 'models.0.model.layers.10.self_attn.o_proj.weight', 'models.0.model.layers.10.mlp.gate_proj.weight', 'models.0.model.layers.10.mlp.up_proj.weight', 'models.0.model.layers.10.mlp.down_proj.weight', 'models.0.model.layers.10.input_layernorm.weight', 'models.0.model.layers.10.post_attention_layernorm.weight', 'models.0.model.layers.11.self_attn.q_proj.weight', 'models.0.model.layers.11.self_attn.k_proj.weight', 'models.0.model.layers.11.self_attn.v_proj.weight', 'models.0.model.layers.11.self_attn.o_proj.weight', 'models.0.model.layers.11.mlp.gate_proj.weight', 'models.0.model.layers.11.mlp.up_proj.weight', 'models.0.model.layers.11.mlp.down_proj.weight', 'models.0.model.layers.11.input_layernorm.weight', 'models.0.model.layers.11.post_attention_layernorm.weight', 'models.0.model.layers.12.self_attn.q_proj.weight', 'models.0.model.layers.12.self_attn.k_proj.weight', 'models.0.model.layers.12.self_attn.v_proj.weight', 'models.0.model.layers.12.self_attn.o_proj.weight', 'models.0.model.layers.12.mlp.gate_proj.weight', 'models.0.model.layers.12.mlp.up_proj.weight', 'models.0.model.layers.12.mlp.down_proj.weight', 'models.0.model.layers.12.input_layernorm.weight', 'models.0.model.layers.12.post_attention_layernorm.weight', 'models.0.model.layers.13.self_attn.q_proj.weight', 'models.0.model.layers.13.self_attn.k_proj.weight', 'models.0.model.layers.13.self_attn.v_proj.weight', 'models.0.model.layers.13.self_attn.o_proj.weight', 'models.0.model.layers.13.mlp.gate_proj.weight', 'models.0.model.layers.13.mlp.up_proj.weight', 'models.0.model.layers.13.mlp.down_proj.weight', 'models.0.model.layers.13.input_layernorm.weight', 'models.0.model.layers.13.post_attention_layernorm.weight', 'models.0.model.layers.14.self_attn.q_proj.weight', 'models.0.model.layers.14.self_attn.k_proj.weight', 'models.0.model.layers.14.self_attn.v_proj.weight', 'models.0.model.layers.14.self_attn.o_proj.weight', 'models.0.model.layers.14.mlp.gate_proj.weight', 'models.0.model.layers.14.mlp.up_proj.weight', 'models.0.model.layers.14.mlp.down_proj.weight', 'models.0.model.layers.14.input_layernorm.weight', 'models.0.model.layers.14.post_attention_layernorm.weight', 'models.0.model.layers.15.self_attn.q_proj.weight', 'models.0.model.layers.15.self_attn.k_proj.weight', 'models.0.model.layers.15.self_attn.v_proj.weight', 'models.0.model.layers.15.self_attn.o_proj.weight', 'models.0.model.layers.15.mlp.gate_proj.weight', 'models.0.model.layers.15.mlp.up_proj.weight', 'models.0.model.layers.15.mlp.down_proj.weight', 'models.0.model.layers.15.input_layernorm.weight', 'models.0.model.layers.15.post_attention_layernorm.weight', 'models.0.model.layers.16.self_attn.q_proj.weight', 'models.0.model.layers.16.self_attn.k_proj.weight', 'models.0.model.layers.16.self_attn.v_proj.weight', 'models.0.model.layers.16.self_attn.o_proj.weight', 'models.0.model.layers.16.mlp.gate_proj.weight', 'models.0.model.layers.16.mlp.up_proj.weight', 'models.0.model.layers.16.mlp.down_proj.weight', 'models.0.model.layers.16.input_layernorm.weight', 'models.0.model.layers.16.post_attention_layernorm.weight', 'models.0.model.layers.17.self_attn.q_proj.weight', 'models.0.model.layers.17.self_attn.k_proj.weight', 'models.0.model.layers.17.self_attn.v_proj.weight', 'models.0.model.layers.17.self_attn.o_proj.weight', 'models.0.model.layers.17.mlp.gate_proj.weight', 'models.0.model.layers.17.mlp.up_proj.weight', 'models.0.model.layers.17.mlp.down_proj.weight', 'models.0.model.layers.17.input_layernorm.weight', 'models.0.model.layers.17.post_attention_layernorm.weight', 'models.0.model.layers.18.self_attn.q_proj.weight', 'models.0.model.layers.18.self_attn.k_proj.weight', 'models.0.model.layers.18.self_attn.v_proj.weight', 'models.0.model.layers.18.self_attn.o_proj.weight', 'models.0.model.layers.18.mlp.gate_proj.weight', 'models.0.model.layers.18.mlp.up_proj.weight', 'models.0.model.layers.18.mlp.down_proj.weight', 'models.0.model.layers.18.input_layernorm.weight', 'models.0.model.layers.18.post_attention_layernorm.weight', 'models.0.model.layers.19.self_attn.q_proj.weight', 'models.0.model.layers.19.self_attn.k_proj.weight', 'models.0.model.layers.19.self_attn.v_proj.weight', 'models.0.model.layers.19.self_attn.o_proj.weight', 'models.0.model.layers.19.mlp.gate_proj.weight', 'models.0.model.layers.19.mlp.up_proj.weight', 'models.0.model.layers.19.mlp.down_proj.weight', 'models.0.model.layers.19.input_layernorm.weight', 'models.0.model.layers.19.post_attention_layernorm.weight', 'models.0.model.layers.20.self_attn.q_proj.weight', 'models.0.model.layers.20.self_attn.k_proj.weight', 'models.0.model.layers.20.self_attn.v_proj.weight', 'models.0.model.layers.20.self_attn.o_proj.weight', 'models.0.model.layers.20.mlp.gate_proj.weight', 'models.0.model.layers.20.mlp.up_proj.weight', 'models.0.model.layers.20.mlp.down_proj.weight', 'models.0.model.layers.20.input_layernorm.weight', 'models.0.model.layers.20.post_attention_layernorm.weight', 'models.0.model.layers.21.self_attn.q_proj.weight', 'models.0.model.layers.21.self_attn.k_proj.weight', 'models.0.model.layers.21.self_attn.v_proj.weight', 'models.0.model.layers.21.self_attn.o_proj.weight', 'models.0.model.layers.21.mlp.gate_proj.weight', 'models.0.model.layers.21.mlp.up_proj.weight', 'models.0.model.layers.21.mlp.down_proj.weight', 'models.0.model.layers.21.input_layernorm.weight', 'models.0.model.layers.21.post_attention_layernorm.weight', 'models.0.model.layers.22.self_attn.q_proj.weight', 'models.0.model.layers.22.self_attn.k_proj.weight', 'models.0.model.layers.22.self_attn.v_proj.weight', 'models.0.model.layers.22.self_attn.o_proj.weight', 'models.0.model.layers.22.mlp.gate_proj.weight', 'models.0.model.layers.22.mlp.up_proj.weight', 'models.0.model.layers.22.mlp.down_proj.weight', 'models.0.model.layers.22.input_layernorm.weight', 'models.0.model.layers.22.post_attention_layernorm.weight', 'models.0.model.layers.23.self_attn.q_proj.weight', 'models.0.model.layers.23.self_attn.k_proj.weight', 'models.0.model.layers.23.self_attn.v_proj.weight', 'models.0.model.layers.23.self_attn.o_proj.weight', 'models.0.model.layers.23.mlp.gate_proj.weight', 'models.0.model.layers.23.mlp.up_proj.weight', 'models.0.model.layers.23.mlp.down_proj.weight', 'models.0.model.layers.23.input_layernorm.weight', 'models.0.model.layers.23.post_attention_layernorm.weight', 'models.0.model.layers.24.self_attn.q_proj.weight', 'models.0.model.layers.24.self_attn.k_proj.weight', 'models.0.model.layers.24.self_attn.v_proj.weight', 'models.0.model.layers.24.self_attn.o_proj.weight', 'models.0.model.layers.24.mlp.gate_proj.weight', 'models.0.model.layers.24.mlp.up_proj.weight', 'models.0.model.layers.24.mlp.down_proj.weight', 'models.0.model.layers.24.input_layernorm.weight', 'models.0.model.layers.24.post_attention_layernorm.weight', 'models.0.model.layers.25.self_attn.q_proj.weight', 'models.0.model.layers.25.self_attn.k_proj.weight', 'models.0.model.layers.25.self_attn.v_proj.weight', 'models.0.model.layers.25.self_attn.o_proj.weight', 'models.0.model.layers.25.mlp.gate_proj.weight', 'models.0.model.layers.25.mlp.up_proj.weight', 'models.0.model.layers.25.mlp.down_proj.weight', 'models.0.model.layers.25.input_layernorm.weight', 'models.0.model.layers.25.post_attention_layernorm.weight', 'models.0.model.layers.26.self_attn.q_proj.weight', 'models.0.model.layers.26.self_attn.k_proj.weight', 'models.0.model.layers.26.self_attn.v_proj.weight', 'models.0.model.layers.26.self_attn.o_proj.weight', 'models.0.model.layers.26.mlp.gate_proj.weight', 'models.0.model.layers.26.mlp.up_proj.weight', 'models.0.model.layers.26.mlp.down_proj.weight', 'models.0.model.layers.26.input_layernorm.weight', 'models.0.model.layers.26.post_attention_layernorm.weight', 'models.0.model.layers.27.self_attn.q_proj.weight', 'models.0.model.layers.27.self_attn.k_proj.weight', 'models.0.model.layers.27.self_attn.v_proj.weight', 'models.0.model.layers.27.self_attn.o_proj.weight', 'models.0.model.layers.27.mlp.gate_proj.weight', 'models.0.model.layers.27.mlp.up_proj.weight', 'models.0.model.layers.27.mlp.down_proj.weight', 'models.0.model.layers.27.input_layernorm.weight', 'models.0.model.layers.27.post_attention_layernorm.weight', 'models.0.model.norm.weight', 'models.0.lm_head.weight', 'models.1.model.embed_tokens.weight', 'models.1.model.layers.0.self_attn.q_proj.weight', 'models.1.model.layers.0.self_attn.k_proj.weight', 'models.1.model.layers.0.self_attn.v_proj.weight', 'models.1.model.layers.0.self_attn.o_proj.weight', 'models.1.model.layers.0.mlp.gate_proj.weight', 'models.1.model.layers.0.mlp.up_proj.weight', 'models.1.model.layers.0.mlp.down_proj.weight', 'models.1.model.layers.0.input_layernorm.weight', 'models.1.model.layers.0.post_attention_layernorm.weight', 'models.1.model.layers.1.self_attn.q_proj.weight', 'models.1.model.layers.1.self_attn.k_proj.weight', 'models.1.model.layers.1.self_attn.v_proj.weight', 'models.1.model.layers.1.self_attn.o_proj.weight', 'models.1.model.layers.1.mlp.gate_proj.weight', 'models.1.model.layers.1.mlp.up_proj.weight', 'models.1.model.layers.1.mlp.down_proj.weight', 'models.1.model.layers.1.input_layernorm.weight', 'models.1.model.layers.1.post_attention_layernorm.weight', 'models.1.model.layers.2.self_attn.q_proj.weight', 'models.1.model.layers.2.self_attn.k_proj.weight', 'models.1.model.layers.2.self_attn.v_proj.weight', 'models.1.model.layers.2.self_attn.o_proj.weight', 'models.1.model.layers.2.mlp.gate_proj.weight', 'models.1.model.layers.2.mlp.up_proj.weight', 'models.1.model.layers.2.mlp.down_proj.weight', 'models.1.model.layers.2.input_layernorm.weight', 'models.1.model.layers.2.post_attention_layernorm.weight', 'models.1.model.layers.3.self_attn.q_proj.weight', 'models.1.model.layers.3.self_attn.k_proj.weight', 'models.1.model.layers.3.self_attn.v_proj.weight', 'models.1.model.layers.3.self_attn.o_proj.weight', 'models.1.model.layers.3.mlp.gate_proj.weight', 'models.1.model.layers.3.mlp.up_proj.weight', 'models.1.model.layers.3.mlp.down_proj.weight', 'models.1.model.layers.3.input_layernorm.weight', 'models.1.model.layers.3.post_attention_layernorm.weight', 'models.1.model.layers.4.self_attn.q_proj.weight', 'models.1.model.layers.4.self_attn.k_proj.weight', 'models.1.model.layers.4.self_attn.v_proj.weight', 'models.1.model.layers.4.self_attn.o_proj.weight', 'models.1.model.layers.4.mlp.gate_proj.weight', 'models.1.model.layers.4.mlp.up_proj.weight', 'models.1.model.layers.4.mlp.down_proj.weight', 'models.1.model.layers.4.input_layernorm.weight', 'models.1.model.layers.4.post_attention_layernorm.weight', 'models.1.model.layers.5.self_attn.q_proj.weight', 'models.1.model.layers.5.self_attn.k_proj.weight', 'models.1.model.layers.5.self_attn.v_proj.weight', 'models.1.model.layers.5.self_attn.o_proj.weight', 'models.1.model.layers.5.mlp.gate_proj.weight', 'models.1.model.layers.5.mlp.up_proj.weight', 'models.1.model.layers.5.mlp.down_proj.weight', 'models.1.model.layers.5.input_layernorm.weight', 'models.1.model.layers.5.post_attention_layernorm.weight', 'models.1.model.layers.6.self_attn.q_proj.weight', 'models.1.model.layers.6.self_attn.k_proj.weight', 'models.1.model.layers.6.self_attn.v_proj.weight', 'models.1.model.layers.6.self_attn.o_proj.weight', 'models.1.model.layers.6.mlp.gate_proj.weight', 'models.1.model.layers.6.mlp.up_proj.weight', 'models.1.model.layers.6.mlp.down_proj.weight', 'models.1.model.layers.6.input_layernorm.weight', 'models.1.model.layers.6.post_attention_layernorm.weight', 'models.1.model.layers.7.self_attn.q_proj.weight', 'models.1.model.layers.7.self_attn.k_proj.weight', 'models.1.model.layers.7.self_attn.v_proj.weight', 'models.1.model.layers.7.self_attn.o_proj.weight', 'models.1.model.layers.7.mlp.gate_proj.weight', 'models.1.model.layers.7.mlp.up_proj.weight', 'models.1.model.layers.7.mlp.down_proj.weight', 'models.1.model.layers.7.input_layernorm.weight', 'models.1.model.layers.7.post_attention_layernorm.weight', 'models.1.model.layers.8.self_attn.q_proj.weight', 'models.1.model.layers.8.self_attn.k_proj.weight', 'models.1.model.layers.8.self_attn.v_proj.weight', 'models.1.model.layers.8.self_attn.o_proj.weight', 'models.1.model.layers.8.mlp.gate_proj.weight', 'models.1.model.layers.8.mlp.up_proj.weight', 'models.1.model.layers.8.mlp.down_proj.weight', 'models.1.model.layers.8.input_layernorm.weight', 'models.1.model.layers.8.post_attention_layernorm.weight', 'models.1.model.layers.9.self_attn.q_proj.weight', 'models.1.model.layers.9.self_attn.k_proj.weight', 'models.1.model.layers.9.self_attn.v_proj.weight', 'models.1.model.layers.9.self_attn.o_proj.weight', 'models.1.model.layers.9.mlp.gate_proj.weight', 'models.1.model.layers.9.mlp.up_proj.weight', 'models.1.model.layers.9.mlp.down_proj.weight', 'models.1.model.layers.9.input_layernorm.weight', 'models.1.model.layers.9.post_attention_layernorm.weight', 'models.1.model.layers.10.self_attn.q_proj.weight', 'models.1.model.layers.10.self_attn.k_proj.weight', 'models.1.model.layers.10.self_attn.v_proj.weight', 'models.1.model.layers.10.self_attn.o_proj.weight', 'models.1.model.layers.10.mlp.gate_proj.weight', 'models.1.model.layers.10.mlp.up_proj.weight', 'models.1.model.layers.10.mlp.down_proj.weight', 'models.1.model.layers.10.input_layernorm.weight', 'models.1.model.layers.10.post_attention_layernorm.weight', 'models.1.model.layers.11.self_attn.q_proj.weight', 'models.1.model.layers.11.self_attn.k_proj.weight', 'models.1.model.layers.11.self_attn.v_proj.weight', 'models.1.model.layers.11.self_attn.o_proj.weight', 'models.1.model.layers.11.mlp.gate_proj.weight', 'models.1.model.layers.11.mlp.up_proj.weight', 'models.1.model.layers.11.mlp.down_proj.weight', 'models.1.model.layers.11.input_layernorm.weight', 'models.1.model.layers.11.post_attention_layernorm.weight', 'models.1.model.layers.12.self_attn.q_proj.weight', 'models.1.model.layers.12.self_attn.k_proj.weight', 'models.1.model.layers.12.self_attn.v_proj.weight', 'models.1.model.layers.12.self_attn.o_proj.weight', 'models.1.model.layers.12.mlp.gate_proj.weight', 'models.1.model.layers.12.mlp.up_proj.weight', 'models.1.model.layers.12.mlp.down_proj.weight', 'models.1.model.layers.12.input_layernorm.weight', 'models.1.model.layers.12.post_attention_layernorm.weight', 'models.1.model.layers.13.self_attn.q_proj.weight', 'models.1.model.layers.13.self_attn.k_proj.weight', 'models.1.model.layers.13.self_attn.v_proj.weight', 'models.1.model.layers.13.self_attn.o_proj.weight', 'models.1.model.layers.13.mlp.gate_proj.weight', 'models.1.model.layers.13.mlp.up_proj.weight', 'models.1.model.layers.13.mlp.down_proj.weight', 'models.1.model.layers.13.input_layernorm.weight', 'models.1.model.layers.13.post_attention_layernorm.weight', 'models.1.model.layers.14.self_attn.q_proj.weight', 'models.1.model.layers.14.self_attn.k_proj.weight', 'models.1.model.layers.14.self_attn.v_proj.weight', 'models.1.model.layers.14.self_attn.o_proj.weight', 'models.1.model.layers.14.mlp.gate_proj.weight', 'models.1.model.layers.14.mlp.up_proj.weight', 'models.1.model.layers.14.mlp.down_proj.weight', 'models.1.model.layers.14.input_layernorm.weight', 'models.1.model.layers.14.post_attention_layernorm.weight', 'models.1.model.layers.15.self_attn.q_proj.weight', 'models.1.model.layers.15.self_attn.k_proj.weight', 'models.1.model.layers.15.self_attn.v_proj.weight', 'models.1.model.layers.15.self_attn.o_proj.weight', 'models.1.model.layers.15.mlp.gate_proj.weight', 'models.1.model.layers.15.mlp.up_proj.weight', 'models.1.model.layers.15.mlp.down_proj.weight', 'models.1.model.layers.15.input_layernorm.weight', 'models.1.model.layers.15.post_attention_layernorm.weight', 'models.1.model.layers.16.self_attn.q_proj.weight', 'models.1.model.layers.16.self_attn.k_proj.weight', 'models.1.model.layers.16.self_attn.v_proj.weight', 'models.1.model.layers.16.self_attn.o_proj.weight', 'models.1.model.layers.16.mlp.gate_proj.weight', 'models.1.model.layers.16.mlp.up_proj.weight', 'models.1.model.layers.16.mlp.down_proj.weight', 'models.1.model.layers.16.input_layernorm.weight', 'models.1.model.layers.16.post_attention_layernorm.weight', 'models.1.model.layers.17.self_attn.q_proj.weight', 'models.1.model.layers.17.self_attn.k_proj.weight', 'models.1.model.layers.17.self_attn.v_proj.weight', 'models.1.model.layers.17.self_attn.o_proj.weight', 'models.1.model.layers.17.mlp.gate_proj.weight', 'models.1.model.layers.17.mlp.up_proj.weight', 'models.1.model.layers.17.mlp.down_proj.weight', 'models.1.model.layers.17.input_layernorm.weight', 'models.1.model.layers.17.post_attention_layernorm.weight', 'models.1.model.layers.18.self_attn.q_proj.weight', 'models.1.model.layers.18.self_attn.k_proj.weight', 'models.1.model.layers.18.self_attn.v_proj.weight', 'models.1.model.layers.18.self_attn.o_proj.weight', 'models.1.model.layers.18.mlp.gate_proj.weight', 'models.1.model.layers.18.mlp.up_proj.weight', 'models.1.model.layers.18.mlp.down_proj.weight', 'models.1.model.layers.18.input_layernorm.weight', 'models.1.model.layers.18.post_attention_layernorm.weight', 'models.1.model.layers.19.self_attn.q_proj.weight', 'models.1.model.layers.19.self_attn.k_proj.weight', 'models.1.model.layers.19.self_attn.v_proj.weight', 'models.1.model.layers.19.self_attn.o_proj.weight', 'models.1.model.layers.19.mlp.gate_proj.weight', 'models.1.model.layers.19.mlp.up_proj.weight', 'models.1.model.layers.19.mlp.down_proj.weight', 'models.1.model.layers.19.input_layernorm.weight', 'models.1.model.layers.19.post_attention_layernorm.weight', 'models.1.model.layers.20.self_attn.q_proj.weight', 'models.1.model.layers.20.self_attn.k_proj.weight', 'models.1.model.layers.20.self_attn.v_proj.weight', 'models.1.model.layers.20.self_attn.o_proj.weight', 'models.1.model.layers.20.mlp.gate_proj.weight', 'models.1.model.layers.20.mlp.up_proj.weight', 'models.1.model.layers.20.mlp.down_proj.weight', 'models.1.model.layers.20.input_layernorm.weight', 'models.1.model.layers.20.post_attention_layernorm.weight', 'models.1.model.layers.21.self_attn.q_proj.weight', 'models.1.model.layers.21.self_attn.k_proj.weight', 'models.1.model.layers.21.self_attn.v_proj.weight', 'models.1.model.layers.21.self_attn.o_proj.weight', 'models.1.model.layers.21.mlp.gate_proj.weight', 'models.1.model.layers.21.mlp.up_proj.weight', 'models.1.model.layers.21.mlp.down_proj.weight', 'models.1.model.layers.21.input_layernorm.weight', 'models.1.model.layers.21.post_attention_layernorm.weight', 'models.1.model.layers.22.self_attn.q_proj.weight', 'models.1.model.layers.22.self_attn.k_proj.weight', 'models.1.model.layers.22.self_attn.v_proj.weight', 'models.1.model.layers.22.self_attn.o_proj.weight', 'models.1.model.layers.22.mlp.gate_proj.weight', 'models.1.model.layers.22.mlp.up_proj.weight', 'models.1.model.layers.22.mlp.down_proj.weight', 'models.1.model.layers.22.input_layernorm.weight', 'models.1.model.layers.22.post_attention_layernorm.weight', 'models.1.model.layers.23.self_attn.q_proj.weight', 'models.1.model.layers.23.self_attn.k_proj.weight', 'models.1.model.layers.23.self_attn.v_proj.weight', 'models.1.model.layers.23.self_attn.o_proj.weight', 'models.1.model.layers.23.mlp.gate_proj.weight', 'models.1.model.layers.23.mlp.up_proj.weight', 'models.1.model.layers.23.mlp.down_proj.weight', 'models.1.model.layers.23.input_layernorm.weight', 'models.1.model.layers.23.post_attention_layernorm.weight', 'models.1.model.layers.24.self_attn.q_proj.weight', 'models.1.model.layers.24.self_attn.k_proj.weight', 'models.1.model.layers.24.self_attn.v_proj.weight', 'models.1.model.layers.24.self_attn.o_proj.weight', 'models.1.model.layers.24.mlp.gate_proj.weight', 'models.1.model.layers.24.mlp.up_proj.weight', 'models.1.model.layers.24.mlp.down_proj.weight', 'models.1.model.layers.24.input_layernorm.weight', 'models.1.model.layers.24.post_attention_layernorm.weight', 'models.1.model.layers.25.self_attn.q_proj.weight', 'models.1.model.layers.25.self_attn.k_proj.weight', 'models.1.model.layers.25.self_attn.v_proj.weight', 'models.1.model.layers.25.self_attn.o_proj.weight', 'models.1.model.layers.25.mlp.gate_proj.weight', 'models.1.model.layers.25.mlp.up_proj.weight', 'models.1.model.layers.25.mlp.down_proj.weight', 'models.1.model.layers.25.input_layernorm.weight', 'models.1.model.layers.25.post_attention_layernorm.weight', 'models.1.model.layers.26.self_attn.q_proj.weight', 'models.1.model.layers.26.self_attn.k_proj.weight', 'models.1.model.layers.26.self_attn.v_proj.weight', 'models.1.model.layers.26.self_attn.o_proj.weight', 'models.1.model.layers.26.mlp.gate_proj.weight', 'models.1.model.layers.26.mlp.up_proj.weight', 'models.1.model.layers.26.mlp.down_proj.weight', 'models.1.model.layers.26.input_layernorm.weight', 'models.1.model.layers.26.post_attention_layernorm.weight', 'models.1.model.layers.27.self_attn.q_proj.weight', 'models.1.model.layers.27.self_attn.k_proj.weight', 'models.1.model.layers.27.self_attn.v_proj.weight', 'models.1.model.layers.27.self_attn.o_proj.weight', 'models.1.model.layers.27.mlp.gate_proj.weight', 'models.1.model.layers.27.mlp.up_proj.weight', 'models.1.model.layers.27.mlp.down_proj.weight', 'models.1.model.layers.27.input_layernorm.weight', 'models.1.model.layers.27.post_attention_layernorm.weight', 'models.1.model.norm.weight', 'models.1.lm_head.weight', 'merger.model.embed_tokens.embeddings.0.weight', 'merger.model.embed_tokens.embeddings.1.weight', 'merger.model.layers.0.self_attn.q_proj.linears.0.weight', 'merger.model.layers.0.self_attn.q_proj.linears.1.weight', 'merger.model.layers.0.self_attn.k_proj.linears.0.weight', 'merger.model.layers.0.self_attn.k_proj.linears.1.weight', 'merger.model.layers.0.self_attn.v_proj.linears.0.weight', 'merger.model.layers.0.self_attn.v_proj.linears.1.weight', 'merger.model.layers.0.self_attn.o_proj.linears.0.weight', 'merger.model.layers.0.self_attn.o_proj.linears.1.weight', 'merger.model.layers.0.mlp.gate_proj.linears.0.weight', 'merger.model.layers.0.mlp.gate_proj.linears.1.weight', 'merger.model.layers.0.mlp.up_proj.linears.0.weight', 'merger.model.layers.0.mlp.up_proj.linears.1.weight', 'merger.model.layers.0.mlp.down_proj.linears.0.weight', 'merger.model.layers.0.mlp.down_proj.linears.1.weight', 'merger.model.layers.0.input_layernorm.rms_norms.0.weight', 'merger.model.layers.0.input_layernorm.rms_norms.1.weight', 'merger.model.layers.0.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.0.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.1.self_attn.q_proj.linears.0.weight', 'merger.model.layers.1.self_attn.q_proj.linears.1.weight', 'merger.model.layers.1.self_attn.k_proj.linears.0.weight', 'merger.model.layers.1.self_attn.k_proj.linears.1.weight', 'merger.model.layers.1.self_attn.v_proj.linears.0.weight', 'merger.model.layers.1.self_attn.v_proj.linears.1.weight', 'merger.model.layers.1.self_attn.o_proj.linears.0.weight', 'merger.model.layers.1.self_attn.o_proj.linears.1.weight', 'merger.model.layers.1.mlp.gate_proj.linears.0.weight', 'merger.model.layers.1.mlp.gate_proj.linears.1.weight', 'merger.model.layers.1.mlp.up_proj.linears.0.weight', 'merger.model.layers.1.mlp.up_proj.linears.1.weight', 'merger.model.layers.1.mlp.down_proj.linears.0.weight', 'merger.model.layers.1.mlp.down_proj.linears.1.weight', 'merger.model.layers.1.input_layernorm.rms_norms.0.weight', 'merger.model.layers.1.input_layernorm.rms_norms.1.weight', 'merger.model.layers.1.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.1.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.2.self_attn.q_proj.linears.0.weight', 'merger.model.layers.2.self_attn.q_proj.linears.1.weight', 'merger.model.layers.2.self_attn.k_proj.linears.0.weight', 'merger.model.layers.2.self_attn.k_proj.linears.1.weight', 'merger.model.layers.2.self_attn.v_proj.linears.0.weight', 'merger.model.layers.2.self_attn.v_proj.linears.1.weight', 'merger.model.layers.2.self_attn.o_proj.linears.0.weight', 'merger.model.layers.2.self_attn.o_proj.linears.1.weight', 'merger.model.layers.2.mlp.gate_proj.linears.0.weight', 'merger.model.layers.2.mlp.gate_proj.linears.1.weight', 'merger.model.layers.2.mlp.up_proj.linears.0.weight', 'merger.model.layers.2.mlp.up_proj.linears.1.weight', 'merger.model.layers.2.mlp.down_proj.linears.0.weight', 'merger.model.layers.2.mlp.down_proj.linears.1.weight', 'merger.model.layers.2.input_layernorm.rms_norms.0.weight', 'merger.model.layers.2.input_layernorm.rms_norms.1.weight', 'merger.model.layers.2.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.2.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.3.self_attn.q_proj.linears.0.weight', 'merger.model.layers.3.self_attn.q_proj.linears.1.weight', 'merger.model.layers.3.self_attn.k_proj.linears.0.weight', 'merger.model.layers.3.self_attn.k_proj.linears.1.weight', 'merger.model.layers.3.self_attn.v_proj.linears.0.weight', 'merger.model.layers.3.self_attn.v_proj.linears.1.weight', 'merger.model.layers.3.self_attn.o_proj.linears.0.weight', 'merger.model.layers.3.self_attn.o_proj.linears.1.weight', 'merger.model.layers.3.mlp.gate_proj.linears.0.weight', 'merger.model.layers.3.mlp.gate_proj.linears.1.weight', 'merger.model.layers.3.mlp.up_proj.linears.0.weight', 'merger.model.layers.3.mlp.up_proj.linears.1.weight', 'merger.model.layers.3.mlp.down_proj.linears.0.weight', 'merger.model.layers.3.mlp.down_proj.linears.1.weight', 'merger.model.layers.3.input_layernorm.rms_norms.0.weight', 'merger.model.layers.3.input_layernorm.rms_norms.1.weight', 'merger.model.layers.3.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.3.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.4.self_attn.q_proj.linears.0.weight', 'merger.model.layers.4.self_attn.q_proj.linears.1.weight', 'merger.model.layers.4.self_attn.k_proj.linears.0.weight', 'merger.model.layers.4.self_attn.k_proj.linears.1.weight', 'merger.model.layers.4.self_attn.v_proj.linears.0.weight', 'merger.model.layers.4.self_attn.v_proj.linears.1.weight', 'merger.model.layers.4.self_attn.o_proj.linears.0.weight', 'merger.model.layers.4.self_attn.o_proj.linears.1.weight', 'merger.model.layers.4.mlp.gate_proj.linears.0.weight', 'merger.model.layers.4.mlp.gate_proj.linears.1.weight', 'merger.model.layers.4.mlp.up_proj.linears.0.weight', 'merger.model.layers.4.mlp.up_proj.linears.1.weight', 'merger.model.layers.4.mlp.down_proj.linears.0.weight', 'merger.model.layers.4.mlp.down_proj.linears.1.weight', 'merger.model.layers.4.input_layernorm.rms_norms.0.weight', 'merger.model.layers.4.input_layernorm.rms_norms.1.weight', 'merger.model.layers.4.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.4.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.5.self_attn.q_proj.linears.0.weight', 'merger.model.layers.5.self_attn.q_proj.linears.1.weight', 'merger.model.layers.5.self_attn.k_proj.linears.0.weight', 'merger.model.layers.5.self_attn.k_proj.linears.1.weight', 'merger.model.layers.5.self_attn.v_proj.linears.0.weight', 'merger.model.layers.5.self_attn.v_proj.linears.1.weight', 'merger.model.layers.5.self_attn.o_proj.linears.0.weight', 'merger.model.layers.5.self_attn.o_proj.linears.1.weight', 'merger.model.layers.5.mlp.gate_proj.linears.0.weight', 'merger.model.layers.5.mlp.gate_proj.linears.1.weight', 'merger.model.layers.5.mlp.up_proj.linears.0.weight', 'merger.model.layers.5.mlp.up_proj.linears.1.weight', 'merger.model.layers.5.mlp.down_proj.linears.0.weight', 'merger.model.layers.5.mlp.down_proj.linears.1.weight', 'merger.model.layers.5.input_layernorm.rms_norms.0.weight', 'merger.model.layers.5.input_layernorm.rms_norms.1.weight', 'merger.model.layers.5.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.5.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.6.self_attn.q_proj.linears.0.weight', 'merger.model.layers.6.self_attn.q_proj.linears.1.weight', 'merger.model.layers.6.self_attn.k_proj.linears.0.weight', 'merger.model.layers.6.self_attn.k_proj.linears.1.weight', 'merger.model.layers.6.self_attn.v_proj.linears.0.weight', 'merger.model.layers.6.self_attn.v_proj.linears.1.weight', 'merger.model.layers.6.self_attn.o_proj.linears.0.weight', 'merger.model.layers.6.self_attn.o_proj.linears.1.weight', 'merger.model.layers.6.mlp.gate_proj.linears.0.weight', 'merger.model.layers.6.mlp.gate_proj.linears.1.weight', 'merger.model.layers.6.mlp.up_proj.linears.0.weight', 'merger.model.layers.6.mlp.up_proj.linears.1.weight', 'merger.model.layers.6.mlp.down_proj.linears.0.weight', 'merger.model.layers.6.mlp.down_proj.linears.1.weight', 'merger.model.layers.6.input_layernorm.rms_norms.0.weight', 'merger.model.layers.6.input_layernorm.rms_norms.1.weight', 'merger.model.layers.6.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.6.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.7.self_attn.q_proj.linears.0.weight', 'merger.model.layers.7.self_attn.q_proj.linears.1.weight', 'merger.model.layers.7.self_attn.k_proj.linears.0.weight', 'merger.model.layers.7.self_attn.k_proj.linears.1.weight', 'merger.model.layers.7.self_attn.v_proj.linears.0.weight', 'merger.model.layers.7.self_attn.v_proj.linears.1.weight', 'merger.model.layers.7.self_attn.o_proj.linears.0.weight', 'merger.model.layers.7.self_attn.o_proj.linears.1.weight', 'merger.model.layers.7.mlp.gate_proj.linears.0.weight', 'merger.model.layers.7.mlp.gate_proj.linears.1.weight', 'merger.model.layers.7.mlp.up_proj.linears.0.weight', 'merger.model.layers.7.mlp.up_proj.linears.1.weight', 'merger.model.layers.7.mlp.down_proj.linears.0.weight', 'merger.model.layers.7.mlp.down_proj.linears.1.weight', 'merger.model.layers.7.input_layernorm.rms_norms.0.weight', 'merger.model.layers.7.input_layernorm.rms_norms.1.weight', 'merger.model.layers.7.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.7.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.8.self_attn.q_proj.linears.0.weight', 'merger.model.layers.8.self_attn.q_proj.linears.1.weight', 'merger.model.layers.8.self_attn.k_proj.linears.0.weight', 'merger.model.layers.8.self_attn.k_proj.linears.1.weight', 'merger.model.layers.8.self_attn.v_proj.linears.0.weight', 'merger.model.layers.8.self_attn.v_proj.linears.1.weight', 'merger.model.layers.8.self_attn.o_proj.linears.0.weight', 'merger.model.layers.8.self_attn.o_proj.linears.1.weight', 'merger.model.layers.8.mlp.gate_proj.linears.0.weight', 'merger.model.layers.8.mlp.gate_proj.linears.1.weight', 'merger.model.layers.8.mlp.up_proj.linears.0.weight', 'merger.model.layers.8.mlp.up_proj.linears.1.weight', 'merger.model.layers.8.mlp.down_proj.linears.0.weight', 'merger.model.layers.8.mlp.down_proj.linears.1.weight', 'merger.model.layers.8.input_layernorm.rms_norms.0.weight', 'merger.model.layers.8.input_layernorm.rms_norms.1.weight', 'merger.model.layers.8.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.8.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.9.self_attn.q_proj.linears.0.weight', 'merger.model.layers.9.self_attn.q_proj.linears.1.weight', 'merger.model.layers.9.self_attn.k_proj.linears.0.weight', 'merger.model.layers.9.self_attn.k_proj.linears.1.weight', 'merger.model.layers.9.self_attn.v_proj.linears.0.weight', 'merger.model.layers.9.self_attn.v_proj.linears.1.weight', 'merger.model.layers.9.self_attn.o_proj.linears.0.weight', 'merger.model.layers.9.self_attn.o_proj.linears.1.weight', 'merger.model.layers.9.mlp.gate_proj.linears.0.weight', 'merger.model.layers.9.mlp.gate_proj.linears.1.weight', 'merger.model.layers.9.mlp.up_proj.linears.0.weight', 'merger.model.layers.9.mlp.up_proj.linears.1.weight', 'merger.model.layers.9.mlp.down_proj.linears.0.weight', 'merger.model.layers.9.mlp.down_proj.linears.1.weight', 'merger.model.layers.9.input_layernorm.rms_norms.0.weight', 'merger.model.layers.9.input_layernorm.rms_norms.1.weight', 'merger.model.layers.9.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.9.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.10.self_attn.q_proj.linears.0.weight', 'merger.model.layers.10.self_attn.q_proj.linears.1.weight', 'merger.model.layers.10.self_attn.k_proj.linears.0.weight', 'merger.model.layers.10.self_attn.k_proj.linears.1.weight', 'merger.model.layers.10.self_attn.v_proj.linears.0.weight', 'merger.model.layers.10.self_attn.v_proj.linears.1.weight', 'merger.model.layers.10.self_attn.o_proj.linears.0.weight', 'merger.model.layers.10.self_attn.o_proj.linears.1.weight', 'merger.model.layers.10.mlp.gate_proj.linears.0.weight', 'merger.model.layers.10.mlp.gate_proj.linears.1.weight', 'merger.model.layers.10.mlp.up_proj.linears.0.weight', 'merger.model.layers.10.mlp.up_proj.linears.1.weight', 'merger.model.layers.10.mlp.down_proj.linears.0.weight', 'merger.model.layers.10.mlp.down_proj.linears.1.weight', 'merger.model.layers.10.input_layernorm.rms_norms.0.weight', 'merger.model.layers.10.input_layernorm.rms_norms.1.weight', 'merger.model.layers.10.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.10.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.11.self_attn.q_proj.linears.0.weight', 'merger.model.layers.11.self_attn.q_proj.linears.1.weight', 'merger.model.layers.11.self_attn.k_proj.linears.0.weight', 'merger.model.layers.11.self_attn.k_proj.linears.1.weight', 'merger.model.layers.11.self_attn.v_proj.linears.0.weight', 'merger.model.layers.11.self_attn.v_proj.linears.1.weight', 'merger.model.layers.11.self_attn.o_proj.linears.0.weight', 'merger.model.layers.11.self_attn.o_proj.linears.1.weight', 'merger.model.layers.11.mlp.gate_proj.linears.0.weight', 'merger.model.layers.11.mlp.gate_proj.linears.1.weight', 'merger.model.layers.11.mlp.up_proj.linears.0.weight', 'merger.model.layers.11.mlp.up_proj.linears.1.weight', 'merger.model.layers.11.mlp.down_proj.linears.0.weight', 'merger.model.layers.11.mlp.down_proj.linears.1.weight', 'merger.model.layers.11.input_layernorm.rms_norms.0.weight', 'merger.model.layers.11.input_layernorm.rms_norms.1.weight', 'merger.model.layers.11.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.11.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.12.self_attn.q_proj.linears.0.weight', 'merger.model.layers.12.self_attn.q_proj.linears.1.weight', 'merger.model.layers.12.self_attn.k_proj.linears.0.weight', 'merger.model.layers.12.self_attn.k_proj.linears.1.weight', 'merger.model.layers.12.self_attn.v_proj.linears.0.weight', 'merger.model.layers.12.self_attn.v_proj.linears.1.weight', 'merger.model.layers.12.self_attn.o_proj.linears.0.weight', 'merger.model.layers.12.self_attn.o_proj.linears.1.weight', 'merger.model.layers.12.mlp.gate_proj.linears.0.weight', 'merger.model.layers.12.mlp.gate_proj.linears.1.weight', 'merger.model.layers.12.mlp.up_proj.linears.0.weight', 'merger.model.layers.12.mlp.up_proj.linears.1.weight', 'merger.model.layers.12.mlp.down_proj.linears.0.weight', 'merger.model.layers.12.mlp.down_proj.linears.1.weight', 'merger.model.layers.12.input_layernorm.rms_norms.0.weight', 'merger.model.layers.12.input_layernorm.rms_norms.1.weight', 'merger.model.layers.12.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.12.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.13.self_attn.q_proj.linears.0.weight', 'merger.model.layers.13.self_attn.q_proj.linears.1.weight', 'merger.model.layers.13.self_attn.k_proj.linears.0.weight', 'merger.model.layers.13.self_attn.k_proj.linears.1.weight', 'merger.model.layers.13.self_attn.v_proj.linears.0.weight', 'merger.model.layers.13.self_attn.v_proj.linears.1.weight', 'merger.model.layers.13.self_attn.o_proj.linears.0.weight', 'merger.model.layers.13.self_attn.o_proj.linears.1.weight', 'merger.model.layers.13.mlp.gate_proj.linears.0.weight', 'merger.model.layers.13.mlp.gate_proj.linears.1.weight', 'merger.model.layers.13.mlp.up_proj.linears.0.weight', 'merger.model.layers.13.mlp.up_proj.linears.1.weight', 'merger.model.layers.13.mlp.down_proj.linears.0.weight', 'merger.model.layers.13.mlp.down_proj.linears.1.weight', 'merger.model.layers.13.input_layernorm.rms_norms.0.weight', 'merger.model.layers.13.input_layernorm.rms_norms.1.weight', 'merger.model.layers.13.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.13.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.14.self_attn.q_proj.linears.0.weight', 'merger.model.layers.14.self_attn.q_proj.linears.1.weight', 'merger.model.layers.14.self_attn.k_proj.linears.0.weight', 'merger.model.layers.14.self_attn.k_proj.linears.1.weight', 'merger.model.layers.14.self_attn.v_proj.linears.0.weight', 'merger.model.layers.14.self_attn.v_proj.linears.1.weight', 'merger.model.layers.14.self_attn.o_proj.linears.0.weight', 'merger.model.layers.14.self_attn.o_proj.linears.1.weight', 'merger.model.layers.14.mlp.gate_proj.linears.0.weight', 'merger.model.layers.14.mlp.gate_proj.linears.1.weight', 'merger.model.layers.14.mlp.up_proj.linears.0.weight', 'merger.model.layers.14.mlp.up_proj.linears.1.weight', 'merger.model.layers.14.mlp.down_proj.linears.0.weight', 'merger.model.layers.14.mlp.down_proj.linears.1.weight', 'merger.model.layers.14.input_layernorm.rms_norms.0.weight', 'merger.model.layers.14.input_layernorm.rms_norms.1.weight', 'merger.model.layers.14.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.14.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.15.self_attn.q_proj.linears.0.weight', 'merger.model.layers.15.self_attn.q_proj.linears.1.weight', 'merger.model.layers.15.self_attn.k_proj.linears.0.weight', 'merger.model.layers.15.self_attn.k_proj.linears.1.weight', 'merger.model.layers.15.self_attn.v_proj.linears.0.weight', 'merger.model.layers.15.self_attn.v_proj.linears.1.weight', 'merger.model.layers.15.self_attn.o_proj.linears.0.weight', 'merger.model.layers.15.self_attn.o_proj.linears.1.weight', 'merger.model.layers.15.mlp.gate_proj.linears.0.weight', 'merger.model.layers.15.mlp.gate_proj.linears.1.weight', 'merger.model.layers.15.mlp.up_proj.linears.0.weight', 'merger.model.layers.15.mlp.up_proj.linears.1.weight', 'merger.model.layers.15.mlp.down_proj.linears.0.weight', 'merger.model.layers.15.mlp.down_proj.linears.1.weight', 'merger.model.layers.15.input_layernorm.rms_norms.0.weight', 'merger.model.layers.15.input_layernorm.rms_norms.1.weight', 'merger.model.layers.15.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.15.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.16.self_attn.q_proj.linears.0.weight', 'merger.model.layers.16.self_attn.q_proj.linears.1.weight', 'merger.model.layers.16.self_attn.k_proj.linears.0.weight', 'merger.model.layers.16.self_attn.k_proj.linears.1.weight', 'merger.model.layers.16.self_attn.v_proj.linears.0.weight', 'merger.model.layers.16.self_attn.v_proj.linears.1.weight', 'merger.model.layers.16.self_attn.o_proj.linears.0.weight', 'merger.model.layers.16.self_attn.o_proj.linears.1.weight', 'merger.model.layers.16.mlp.gate_proj.linears.0.weight', 'merger.model.layers.16.mlp.gate_proj.linears.1.weight', 'merger.model.layers.16.mlp.up_proj.linears.0.weight', 'merger.model.layers.16.mlp.up_proj.linears.1.weight', 'merger.model.layers.16.mlp.down_proj.linears.0.weight', 'merger.model.layers.16.mlp.down_proj.linears.1.weight', 'merger.model.layers.16.input_layernorm.rms_norms.0.weight', 'merger.model.layers.16.input_layernorm.rms_norms.1.weight', 'merger.model.layers.16.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.16.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.17.self_attn.q_proj.linears.0.weight', 'merger.model.layers.17.self_attn.q_proj.linears.1.weight', 'merger.model.layers.17.self_attn.k_proj.linears.0.weight', 'merger.model.layers.17.self_attn.k_proj.linears.1.weight', 'merger.model.layers.17.self_attn.v_proj.linears.0.weight', 'merger.model.layers.17.self_attn.v_proj.linears.1.weight', 'merger.model.layers.17.self_attn.o_proj.linears.0.weight', 'merger.model.layers.17.self_attn.o_proj.linears.1.weight', 'merger.model.layers.17.mlp.gate_proj.linears.0.weight', 'merger.model.layers.17.mlp.gate_proj.linears.1.weight', 'merger.model.layers.17.mlp.up_proj.linears.0.weight', 'merger.model.layers.17.mlp.up_proj.linears.1.weight', 'merger.model.layers.17.mlp.down_proj.linears.0.weight', 'merger.model.layers.17.mlp.down_proj.linears.1.weight', 'merger.model.layers.17.input_layernorm.rms_norms.0.weight', 'merger.model.layers.17.input_layernorm.rms_norms.1.weight', 'merger.model.layers.17.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.17.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.18.self_attn.q_proj.linears.0.weight', 'merger.model.layers.18.self_attn.q_proj.linears.1.weight', 'merger.model.layers.18.self_attn.k_proj.linears.0.weight', 'merger.model.layers.18.self_attn.k_proj.linears.1.weight', 'merger.model.layers.18.self_attn.v_proj.linears.0.weight', 'merger.model.layers.18.self_attn.v_proj.linears.1.weight', 'merger.model.layers.18.self_attn.o_proj.linears.0.weight', 'merger.model.layers.18.self_attn.o_proj.linears.1.weight', 'merger.model.layers.18.mlp.gate_proj.linears.0.weight', 'merger.model.layers.18.mlp.gate_proj.linears.1.weight', 'merger.model.layers.18.mlp.up_proj.linears.0.weight', 'merger.model.layers.18.mlp.up_proj.linears.1.weight', 'merger.model.layers.18.mlp.down_proj.linears.0.weight', 'merger.model.layers.18.mlp.down_proj.linears.1.weight', 'merger.model.layers.18.input_layernorm.rms_norms.0.weight', 'merger.model.layers.18.input_layernorm.rms_norms.1.weight', 'merger.model.layers.18.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.18.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.19.self_attn.q_proj.linears.0.weight', 'merger.model.layers.19.self_attn.q_proj.linears.1.weight', 'merger.model.layers.19.self_attn.k_proj.linears.0.weight', 'merger.model.layers.19.self_attn.k_proj.linears.1.weight', 'merger.model.layers.19.self_attn.v_proj.linears.0.weight', 'merger.model.layers.19.self_attn.v_proj.linears.1.weight', 'merger.model.layers.19.self_attn.o_proj.linears.0.weight', 'merger.model.layers.19.self_attn.o_proj.linears.1.weight', 'merger.model.layers.19.mlp.gate_proj.linears.0.weight', 'merger.model.layers.19.mlp.gate_proj.linears.1.weight', 'merger.model.layers.19.mlp.up_proj.linears.0.weight', 'merger.model.layers.19.mlp.up_proj.linears.1.weight', 'merger.model.layers.19.mlp.down_proj.linears.0.weight', 'merger.model.layers.19.mlp.down_proj.linears.1.weight', 'merger.model.layers.19.input_layernorm.rms_norms.0.weight', 'merger.model.layers.19.input_layernorm.rms_norms.1.weight', 'merger.model.layers.19.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.19.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.20.self_attn.q_proj.linears.0.weight', 'merger.model.layers.20.self_attn.q_proj.linears.1.weight', 'merger.model.layers.20.self_attn.k_proj.linears.0.weight', 'merger.model.layers.20.self_attn.k_proj.linears.1.weight', 'merger.model.layers.20.self_attn.v_proj.linears.0.weight', 'merger.model.layers.20.self_attn.v_proj.linears.1.weight', 'merger.model.layers.20.self_attn.o_proj.linears.0.weight', 'merger.model.layers.20.self_attn.o_proj.linears.1.weight', 'merger.model.layers.20.mlp.gate_proj.linears.0.weight', 'merger.model.layers.20.mlp.gate_proj.linears.1.weight', 'merger.model.layers.20.mlp.up_proj.linears.0.weight', 'merger.model.layers.20.mlp.up_proj.linears.1.weight', 'merger.model.layers.20.mlp.down_proj.linears.0.weight', 'merger.model.layers.20.mlp.down_proj.linears.1.weight', 'merger.model.layers.20.input_layernorm.rms_norms.0.weight', 'merger.model.layers.20.input_layernorm.rms_norms.1.weight', 'merger.model.layers.20.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.20.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.21.self_attn.q_proj.linears.0.weight', 'merger.model.layers.21.self_attn.q_proj.linears.1.weight', 'merger.model.layers.21.self_attn.k_proj.linears.0.weight', 'merger.model.layers.21.self_attn.k_proj.linears.1.weight', 'merger.model.layers.21.self_attn.v_proj.linears.0.weight', 'merger.model.layers.21.self_attn.v_proj.linears.1.weight', 'merger.model.layers.21.self_attn.o_proj.linears.0.weight', 'merger.model.layers.21.self_attn.o_proj.linears.1.weight', 'merger.model.layers.21.mlp.gate_proj.linears.0.weight', 'merger.model.layers.21.mlp.gate_proj.linears.1.weight', 'merger.model.layers.21.mlp.up_proj.linears.0.weight', 'merger.model.layers.21.mlp.up_proj.linears.1.weight', 'merger.model.layers.21.mlp.down_proj.linears.0.weight', 'merger.model.layers.21.mlp.down_proj.linears.1.weight', 'merger.model.layers.21.input_layernorm.rms_norms.0.weight', 'merger.model.layers.21.input_layernorm.rms_norms.1.weight', 'merger.model.layers.21.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.21.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.22.self_attn.q_proj.linears.0.weight', 'merger.model.layers.22.self_attn.q_proj.linears.1.weight', 'merger.model.layers.22.self_attn.k_proj.linears.0.weight', 'merger.model.layers.22.self_attn.k_proj.linears.1.weight', 'merger.model.layers.22.self_attn.v_proj.linears.0.weight', 'merger.model.layers.22.self_attn.v_proj.linears.1.weight', 'merger.model.layers.22.self_attn.o_proj.linears.0.weight', 'merger.model.layers.22.self_attn.o_proj.linears.1.weight', 'merger.model.layers.22.mlp.gate_proj.linears.0.weight', 'merger.model.layers.22.mlp.gate_proj.linears.1.weight', 'merger.model.layers.22.mlp.up_proj.linears.0.weight', 'merger.model.layers.22.mlp.up_proj.linears.1.weight', 'merger.model.layers.22.mlp.down_proj.linears.0.weight', 'merger.model.layers.22.mlp.down_proj.linears.1.weight', 'merger.model.layers.22.input_layernorm.rms_norms.0.weight', 'merger.model.layers.22.input_layernorm.rms_norms.1.weight', 'merger.model.layers.22.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.22.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.23.self_attn.q_proj.linears.0.weight', 'merger.model.layers.23.self_attn.q_proj.linears.1.weight', 'merger.model.layers.23.self_attn.k_proj.linears.0.weight', 'merger.model.layers.23.self_attn.k_proj.linears.1.weight', 'merger.model.layers.23.self_attn.v_proj.linears.0.weight', 'merger.model.layers.23.self_attn.v_proj.linears.1.weight', 'merger.model.layers.23.self_attn.o_proj.linears.0.weight', 'merger.model.layers.23.self_attn.o_proj.linears.1.weight', 'merger.model.layers.23.mlp.gate_proj.linears.0.weight', 'merger.model.layers.23.mlp.gate_proj.linears.1.weight', 'merger.model.layers.23.mlp.up_proj.linears.0.weight', 'merger.model.layers.23.mlp.up_proj.linears.1.weight', 'merger.model.layers.23.mlp.down_proj.linears.0.weight', 'merger.model.layers.23.mlp.down_proj.linears.1.weight', 'merger.model.layers.23.input_layernorm.rms_norms.0.weight', 'merger.model.layers.23.input_layernorm.rms_norms.1.weight', 'merger.model.layers.23.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.23.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.24.self_attn.q_proj.linears.0.weight', 'merger.model.layers.24.self_attn.q_proj.linears.1.weight', 'merger.model.layers.24.self_attn.k_proj.linears.0.weight', 'merger.model.layers.24.self_attn.k_proj.linears.1.weight', 'merger.model.layers.24.self_attn.v_proj.linears.0.weight', 'merger.model.layers.24.self_attn.v_proj.linears.1.weight', 'merger.model.layers.24.self_attn.o_proj.linears.0.weight', 'merger.model.layers.24.self_attn.o_proj.linears.1.weight', 'merger.model.layers.24.mlp.gate_proj.linears.0.weight', 'merger.model.layers.24.mlp.gate_proj.linears.1.weight', 'merger.model.layers.24.mlp.up_proj.linears.0.weight', 'merger.model.layers.24.mlp.up_proj.linears.1.weight', 'merger.model.layers.24.mlp.down_proj.linears.0.weight', 'merger.model.layers.24.mlp.down_proj.linears.1.weight', 'merger.model.layers.24.input_layernorm.rms_norms.0.weight', 'merger.model.layers.24.input_layernorm.rms_norms.1.weight', 'merger.model.layers.24.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.24.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.25.self_attn.q_proj.linears.0.weight', 'merger.model.layers.25.self_attn.q_proj.linears.1.weight', 'merger.model.layers.25.self_attn.k_proj.linears.0.weight', 'merger.model.layers.25.self_attn.k_proj.linears.1.weight', 'merger.model.layers.25.self_attn.v_proj.linears.0.weight', 'merger.model.layers.25.self_attn.v_proj.linears.1.weight', 'merger.model.layers.25.self_attn.o_proj.linears.0.weight', 'merger.model.layers.25.self_attn.o_proj.linears.1.weight', 'merger.model.layers.25.mlp.gate_proj.linears.0.weight', 'merger.model.layers.25.mlp.gate_proj.linears.1.weight', 'merger.model.layers.25.mlp.up_proj.linears.0.weight', 'merger.model.layers.25.mlp.up_proj.linears.1.weight', 'merger.model.layers.25.mlp.down_proj.linears.0.weight', 'merger.model.layers.25.mlp.down_proj.linears.1.weight', 'merger.model.layers.25.input_layernorm.rms_norms.0.weight', 'merger.model.layers.25.input_layernorm.rms_norms.1.weight', 'merger.model.layers.25.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.25.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.26.self_attn.q_proj.linears.0.weight', 'merger.model.layers.26.self_attn.q_proj.linears.1.weight', 'merger.model.layers.26.self_attn.k_proj.linears.0.weight', 'merger.model.layers.26.self_attn.k_proj.linears.1.weight', 'merger.model.layers.26.self_attn.v_proj.linears.0.weight', 'merger.model.layers.26.self_attn.v_proj.linears.1.weight', 'merger.model.layers.26.self_attn.o_proj.linears.0.weight', 'merger.model.layers.26.self_attn.o_proj.linears.1.weight', 'merger.model.layers.26.mlp.gate_proj.linears.0.weight', 'merger.model.layers.26.mlp.gate_proj.linears.1.weight', 'merger.model.layers.26.mlp.up_proj.linears.0.weight', 'merger.model.layers.26.mlp.up_proj.linears.1.weight', 'merger.model.layers.26.mlp.down_proj.linears.0.weight', 'merger.model.layers.26.mlp.down_proj.linears.1.weight', 'merger.model.layers.26.input_layernorm.rms_norms.0.weight', 'merger.model.layers.26.input_layernorm.rms_norms.1.weight', 'merger.model.layers.26.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.26.post_attention_layernorm.rms_norms.1.weight', 'merger.model.layers.27.self_attn.q_proj.linears.0.weight', 'merger.model.layers.27.self_attn.q_proj.linears.1.weight', 'merger.model.layers.27.self_attn.k_proj.linears.0.weight', 'merger.model.layers.27.self_attn.k_proj.linears.1.weight', 'merger.model.layers.27.self_attn.v_proj.linears.0.weight', 'merger.model.layers.27.self_attn.v_proj.linears.1.weight', 'merger.model.layers.27.self_attn.o_proj.linears.0.weight', 'merger.model.layers.27.self_attn.o_proj.linears.1.weight', 'merger.model.layers.27.mlp.gate_proj.linears.0.weight', 'merger.model.layers.27.mlp.gate_proj.linears.1.weight', 'merger.model.layers.27.mlp.up_proj.linears.0.weight', 'merger.model.layers.27.mlp.up_proj.linears.1.weight', 'merger.model.layers.27.mlp.down_proj.linears.0.weight', 'merger.model.layers.27.mlp.down_proj.linears.1.weight', 'merger.model.layers.27.input_layernorm.rms_norms.0.weight', 'merger.model.layers.27.input_layernorm.rms_norms.1.weight', 'merger.model.layers.27.post_attention_layernorm.rms_norms.0.weight', 'merger.model.layers.27.post_attention_layernorm.rms_norms.1.weight', 'merger.model.norm.rms_norms.0.weight', 'merger.model.norm.rms_norms.1.weight', 'merger.lm_head.linears.0.weight', 'merger.lm_head.linears.1.weight']\n",
      "2025-01-06 04:49:32,219 - INFO - Unexpected keys: []\n"
     ]
    }
   ],
   "source": [
    "merger = NewMerger.from_pretrained(\n",
    "    \"./trained_masks\",\n",
    "    merge_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f11b513-6bf0-4b9b-833d-5dd9ef3a7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger.save_pretrained(\"./trained_masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0881557b-1b22-4f8b-8694-8274dd12d931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 03:16:05,405 - INFO - Initial GPU memory allocated: 0.00 GB\n",
      "2025-01-06 03:16:05,549 - INFO - Final GPU memory allocated: 0.00 GB\n",
      "2025-01-06 03:16:05,550 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Monitor memory usage\n",
    "initial_memory = torch.cuda.memory_allocated()\n",
    "logger.info(f\"Initial GPU memory allocated: {initial_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "final_memory = torch.cuda.memory_allocated()\n",
    "logger.info(f\"Final GPU memory allocated: {final_memory / 1024**3:.2f} GB\")\n",
    "logger.info(f\"Freed GPU memory: {(initial_memory - final_memory) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b80a1459-db79-4b6d-b48f-abe251903369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup training arguments and data collator\n",
    "args = Args()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.output_dir,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    learning_rate=args.learning_rate,\n",
    "    num_train_epochs=args.num_train_epochs,\n",
    "    save_steps=args.save_steps,\n",
    "    evaluation_strategy=args.evaluation_strategy if args.validation_split else \"no\",\n",
    "    eval_steps=args.eval_steps if args.validation_split else None,\n",
    "    logging_steps=args.logging_steps,\n",
    "    logging_dir=args.logging_dir,\n",
    "    report_to=args.report_to,  # Enable TensorBoard logging\n",
    "    remove_unused_columns=args.remove_unused_columns,\n",
    "    logging_first_step=args.logging_first_step,\n",
    "    save_safetensors=False\n",
    ")\n",
    "\n",
    "data_collator = MergerDataCollator(\n",
    "    tokenizer,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Initialize and start training\n",
    "trainer = MergerTrainer(\n",
    "    model=merger,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b06cc8-127d-4784-951a-b2a8fa229788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save(self, output_dir: Optional[str] = None, state_dict=None):\n",
    "    \"\"\"\n",
    "    Every save calls point back to this function.\n",
    "    _save_checkpoint -> save_model -> _save.\n",
    "    This function also calls back to .save_pretrained(), so basically\n",
    "    I only have to customize .save_pretrained()\n",
    "    \"\"\"\n",
    "    # If we are executing this function, we are the process zero, so we don't check for that.\n",
    "    output_dir = output_dir if output_dir is not None else self.args.output_dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(f\"Saving model checkpoint to {output_dir}\")\n",
    "\n",
    "    supported_classes = (PreTrainedModel,) if not is_peft_available() else (PreTrainedModel, PeftModel)\n",
    "    # Save a trained model and configuration using `save_pretrained()`.\n",
    "    # They can then be reloaded using `from_pretrained()`\n",
    "    if not isinstance(self.model, supported_classes):\n",
    "        if state_dict is None:\n",
    "            state_dict = self.model.state_dict()\n",
    "\n",
    "        if isinstance(self.accelerator.unwrap_model(self.model), supported_classes):\n",
    "            self.accelerator.unwrap_model(self.model).save_pretrained(\n",
    "                output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n",
    "    else:\n",
    "        self.model.save_pretrained(\n",
    "            output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors\n",
    "        )\n",
    "\n",
    "    if self.processing_class is not None:\n",
    "        self.processing_class.save_pretrained(output_dir)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "\n",
    "\n",
    "def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n",
    "    if model is None:\n",
    "        model = self.model\n",
    "\n",
    "    config_file = os.path.join(resume_from_checkpoint, CONFIG_NAME)\n",
    "    weights_file = os.path.join(resume_from_checkpoint, WEIGHTS_NAME)\n",
    "    weights_index_file = os.path.join(resume_from_checkpoint, WEIGHTS_INDEX_NAME)\n",
    "    safe_weights_file = os.path.join(resume_from_checkpoint, SAFE_WEIGHTS_NAME)\n",
    "    safe_weights_index_file = os.path.join(resume_from_checkpoint, SAFE_WEIGHTS_INDEX_NAME)\n",
    "    is_fsdp_ckpt = False\n",
    "    \n",
    "    if not (\n",
    "        any(\n",
    "            os.path.isfile(f)\n",
    "            for f in [\n",
    "                weights_file,\n",
    "                safe_weights_file,\n",
    "                weights_index_file,\n",
    "                safe_weights_index_file,\n",
    "                adapter_weights_file,\n",
    "                adapter_safe_weights_file,\n",
    "            ]\n",
    "        )\n",
    "    ):\n",
    "        raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\")\n",
    "\n",
    "    logger.info(f\"Loading model from {resume_from_checkpoint}.\")\n",
    "\n",
    "    if os.path.isfile(config_file):\n",
    "        config = PretrainedConfig.from_json_file(config_file)\n",
    "        checkpoint_version = config.transformers_version\n",
    "        if checkpoint_version is not None and checkpoint_version != __version__:\n",
    "            logger.warning(\n",
    "                f\"You are resuming training from a checkpoint trained with {checkpoint_version} of \"\n",
    "                f\"Transformers but your current version is {__version__}. This is not recommended and could \"\n",
    "                \"yield to errors or unwanted behaviors.\"\n",
    "            )\n",
    "\n",
    "    if os.path.isfile(weights_file) or os.path.isfile(safe_weights_file) or is_fsdp_ckpt:\n",
    "        weights_only_kwarg = {\"weights_only\": True} if is_torch_greater_or_equal_than_1_13 else {}\n",
    "        # If the model is on the GPU, it still works!\n",
    "        # We load the model state dict on the CPU to avoid an OOM error.\n",
    "        if self.args.save_safetensors and os.path.isfile(safe_weights_file):\n",
    "            state_dict = safetensors.torch.load_file(safe_weights_file, device=\"cpu\")\n",
    "        else:\n",
    "            state_dict = torch.load(\n",
    "                weights_file,\n",
    "                map_location=\"cpu\",\n",
    "                **weights_only_kwarg,\n",
    "            )\n",
    "\n",
    "        # workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\n",
    "        # which takes *args instead of **kwargs\n",
    "        load_result = model.load_state_dict(state_dict, False)\n",
    "        # release memory\n",
    "        del state_dict\n",
    "        self._issue_warnings_after_load(load_result)\n",
    "        \n",
    "    else:\n",
    "        # We load the sharded checkpoint\n",
    "        load_result = load_sharded_checkpoint(\n",
    "            model, resume_from_checkpoint, \n",
    "            strict=is_sagemaker_mp_enabled(), \n",
    "            prefer_safe=self.args.save_safetensors\n",
    "        )\n",
    "        if not is_sagemaker_mp_enabled():\n",
    "            self._issue_warnings_after_load(load_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ffdf19-9b4c-4394-a6bf-fe91626a749b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 03:16:16,773 - INFO - >>> Collator output keys: dict_keys(['input_ids', 'attention_mask', 'labels', 'data_source'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='937' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/937 09:32 < 1:00:08, 0.22 it/s, Epoch 0.14/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mMergerTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m data_source \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_source\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m effective_idxs \u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m logits_merged \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerger_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     17\u001b[0m logits_components \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponents_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/logits-guided-merger/accurate_masks.py:774\u001b[0m, in \u001b[0;36mNewMerger.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;66;03m# During training, we want both merger and component outputs\u001b[39;00m\n\u001b[1;32m    773\u001b[0m merger_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerger(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m--> 774\u001b[0m components_outputs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerger_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: merger_outputs,\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponents_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: components_outputs\n\u001b[1;32m    779\u001b[0m }\n",
      "File \u001b[0;32m/workspace/logits-guided-merger/accurate_masks.py:774\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;66;03m# During training, we want both merger and component outputs\u001b[39;00m\n\u001b[1;32m    773\u001b[0m merger_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerger(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m--> 774\u001b[0m components_outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels]\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerger_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: merger_outputs,\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponents_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: components_outputs\n\u001b[1;32m    779\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:915\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    913\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 915\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:992\u001b[0m, in \u001b[0;36mLlamaModel._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_causal_mask\u001b[39m(\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    985\u001b[0m     attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    989\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    990\u001b[0m ):\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 992\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m:\n\u001b[1;32m    993\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m attention_mask\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:1178\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, element)\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1175\u001b[0m     element, (torch\u001b[38;5;241m.\u001b[39mTensor, Number, torch\u001b[38;5;241m.\u001b[39mSymInt, torch\u001b[38;5;241m.\u001b[39mSymFloat, torch\u001b[38;5;241m.\u001b[39mSymBool)\n\u001b[1;32m   1176\u001b[0m ):\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1182\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5cedda5a-f753-4a62-a8a4-18772933ac0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight_masks': [Parameter containing:\n",
       "  tensor([1.0078, 0.9219, 1.2891,  ..., 0.9766, 1.0781, 1.4062], device='cuda:0',\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([0.5273, 0.2832, 0.5859,  ..., 0.1475, 0.4023, 0.7812], device='cuda:0',\n",
       "         dtype=torch.bfloat16, requires_grad=True)],\n",
       " 'bias_masks': [None, None]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.merger.model.layers[8].mlp.up_proj.get_constrained_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d64d684-d171-446b-8339-46aa36a1782f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Provide a concise, objective summary of the input text in up to three sentences, focusing on key actions and intentions without using second or third person pronouns.',\n",
       "  'role': 'system'},\n",
       " {'content': \"By . Ted Thornhill . A mom may have saved her teenage son's life by spying on his Facebook page, as she found death threats on it and alerted police. The concerned parent, from Salt Lake City, called the authorities when she found threats to shoot her son - who attends West High School – had been posted on his profile page. The threats were allegedly made by two male teenagers, 16 and 17, who police arrested when they were found waiting in a car near the school on Friday. Potential life-saver: A mother of a West High School pupil alerted police after she saw threats to her son's life had been made on his Facebook page . Police said they found a gun, loaded magazine, ammunition, cash, marijuana and a bong inside the car. Salt Lake police detective Greg Wilking told Deseret News: ‘She [the mother] had actually read threats and seen the threat on his Facebook page. There were very specific threats that they were going to go the high school and shoot her son. ‘There was a picture of the gun on Instagram, the gun that was seized. And there were letters written on the hand that was holding the gun, and those letters were gang affiliated.’ Praise: The police said the mother did the right thing by reporting the threat (file picture) He described the threat to the teenager’s life as ‘credible’. The two arrested boys, who were not West High School students, have been place in juvenile detention and charged with various misdemeanours. Police are not yet certain whether the two arrested teens were waiting to shoot the boy threatened on Facebook or what the origin of the dispute was. Wilking said that the mother had done the right thing by alerting the authorities to the threat. Sorry we are not currently accepting comments on this article.\",\n",
       "  'role': 'user'},\n",
       " {'content': \"A mother in Salt Lake City may have saved her teenage son's life by discovering death threats on his Facebook page and alerting the police. The threats, which included a picture of a gun on Instagram, were made by two male teenagers, aged 16 and 17. Police arrested the suspects near West High School on Friday, finding a gun, loaded magazine, ammunition, cash, marijuana, and a bong in their car. The teens, who are not students at West High, have been charged with various misdemeanors and placed in juvenile detention. Detective Greg Wilking praised the mother for her quick action, describing the threat as credible. The police are still investigating the origin of the dispute and the suspects' intentions.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2b2066e-951a-446c-a315-b027835dfdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "system = train_dataset[idx]['messages'][0]['content']\n",
    "prompt = train_dataset[idx]['messages'][1]['content']\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8175fe4-cccc-4ce6-9b28-5ca5320feb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily met Tom at the parent-teacher conference and wants to discuss teaching methods and offers to give a guest lecture on the history of Pine Grove School. She also invites Tom to meet for coffee to talk more. Emily is working on a book about the history of education in colonial America.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, trainer.model.merger, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d368a2d-afd8-4e13-bf8b-94608d735d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_train = load_dataset(\"HuggingFaceTB/smoltalk\", \"smol-rewrite\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e259767-bc32-46e4-bb1d-c847f19643d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You're an AI assistant for text re-writing. Rewrite the input text to make it more concise while preserving its core meaning.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hey Michael,\n",
      "\n",
      "I hope you're doing well! I wanted to touch base with you regarding the progress on our nutrition curriculum project. I've been working on the lesson plans for grades 3-5 and have made some great strides. I'd love to get your feedback on what I've put together so far.\n",
      "\n",
      "Also, I wanted to share a new recipe I tried out this weekend - a quinoa and black bean salad that was a hit with my family. I thought you might enjoy it too, given our mutual love for healthy cooking. I'll attach the recipe below.\n",
      "\n",
      "Finally, I've been giving some thought to pursuing a Master's degree in Nutrition Education. I know you've been in the field for a while now, and I was hoping to get your advice on the best path forward. If you have any insights or recommendations, I'd be incredibly grateful.\n",
      "\n",
      "Looking forward to catching up soon!\n",
      "\n",
      "Best,\n",
      "Emily<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 300\n",
    "system = rewrite_train[idx]['messages'][0]['content']\n",
    "prompt = rewrite_train[idx]['messages'][1]['content']\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f0006db-151c-4d85-9529-b3979e4fabd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily is sharing updates on the nutrition curriculum project and seeking feedback on the lesson plans for grades 3-5. She also shares a new quinoa and black bean salad recipe and asks for advice on pursuing a Master's degree in Nutrition Education. Emily looks forward to catching up soon.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, trainer.model.merger, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9aa5e940-61a6-43a9-a97f-2283cb444239",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.merger.save_pretrained(\"./trained_masks\", safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bfd6ce8-3471-4835-a7f7-e79e1f390145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewMerger(\n",
       "  (models): ModuleList(\n",
       "    (0-1): 2 x LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaFlashAttention2(\n",
       "              (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "              (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (merger): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): EmbeddingsWithMasks(\n",
       "        (embeddings): ModuleList(\n",
       "          (0-1): 2 x Embedding(128256, 3072)\n",
       "        )\n",
       "        (masks): ModuleList(\n",
       "          (0-1): 2 x Mask(mask_mode=vector_input)\n",
       "        )\n",
       "        (masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): LinearsWithMasks(\n",
       "              (linears): ModuleList(\n",
       "                (0-1): 2 x Linear(in_features=3072, out_features=3072, bias=False)\n",
       "              )\n",
       "              (weight_masks): ModuleList(\n",
       "                (0-1): 2 x Mask(mask_mode=vector_input)\n",
       "              )\n",
       "              (weight_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "              (bias_masks): ModuleList(\n",
       "                (0-1): 2 x None\n",
       "              )\n",
       "              (bias_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "            )\n",
       "            (k_proj): LinearsWithMasks(\n",
       "              (linears): ModuleList(\n",
       "                (0-1): 2 x Linear(in_features=3072, out_features=1024, bias=False)\n",
       "              )\n",
       "              (weight_masks): ModuleList(\n",
       "                (0-1): 2 x Mask(mask_mode=vector_input)\n",
       "              )\n",
       "              (weight_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "              (bias_masks): ModuleList(\n",
       "                (0-1): 2 x None\n",
       "              )\n",
       "              (bias_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "            )\n",
       "            (v_proj): LinearsWithMasks(\n",
       "              (linears): ModuleList(\n",
       "                (0-1): 2 x Linear(in_features=3072, out_features=1024, bias=False)\n",
       "              )\n",
       "              (weight_masks): ModuleList(\n",
       "                (0-1): 2 x Mask(mask_mode=vector_input)\n",
       "              )\n",
       "              (weight_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "              (bias_masks): ModuleList(\n",
       "                (0-1): 2 x None\n",
       "              )\n",
       "              (bias_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "            )\n",
       "            (o_proj): LinearsWithMasks(\n",
       "              (linears): ModuleList(\n",
       "                (0-1): 2 x Linear(in_features=3072, out_features=3072, bias=False)\n",
       "              )\n",
       "              (weight_masks): ModuleList(\n",
       "                (0-1): 2 x Mask(mask_mode=vector_input)\n",
       "              )\n",
       "              (weight_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "              (bias_masks): ModuleList(\n",
       "                (0-1): 2 x None\n",
       "              )\n",
       "              (bias_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "            )\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): LinearsWithMasks(\n",
       "              (linears): ModuleList(\n",
       "                (0-1): 2 x Linear(in_features=3072, out_features=8192, bias=False)\n",
       "              )\n",
       "              (weight_masks): ModuleList(\n",
       "                (0-1): 2 x Mask(mask_mode=vector_input)\n",
       "              )\n",
       "              (weight_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "              (bias_masks): ModuleList(\n",
       "                (0-1): 2 x None\n",
       "              )\n",
       "              (bias_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "            )\n",
       "            (up_proj): LinearsWithMasks(\n",
       "              (linears): ModuleList(\n",
       "                (0-1): 2 x Linear(in_features=3072, out_features=8192, bias=False)\n",
       "              )\n",
       "              (weight_masks): ModuleList(\n",
       "                (0-1): 2 x Mask(mask_mode=vector_input)\n",
       "              )\n",
       "              (weight_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "              (bias_masks): ModuleList(\n",
       "                (0-1): 2 x None\n",
       "              )\n",
       "              (bias_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "            )\n",
       "            (down_proj): LinearsWithMasks(\n",
       "              (linears): ModuleList(\n",
       "                (0-1): 2 x Linear(in_features=8192, out_features=3072, bias=False)\n",
       "              )\n",
       "              (weight_masks): ModuleList(\n",
       "                (0-1): 2 x Mask(mask_mode=vector_input)\n",
       "              )\n",
       "              (weight_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "              (bias_masks): ModuleList(\n",
       "                (0-1): 2 x None\n",
       "              )\n",
       "              (bias_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): RMSNormsWithMasks(\n",
       "            (rms_norms): ModuleList(\n",
       "              (0-1): 2 x LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (masks): ModuleList(\n",
       "              (0-1): 2 x Mask(mask_mode=scalar)\n",
       "            )\n",
       "            (masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNormsWithMasks(\n",
       "            (rms_norms): ModuleList(\n",
       "              (0-1): 2 x LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (masks): ModuleList(\n",
       "              (0-1): 2 x Mask(mask_mode=scalar)\n",
       "            )\n",
       "            (masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNormsWithMasks(\n",
       "        (rms_norms): ModuleList(\n",
       "          (0-1): 2 x LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        )\n",
       "        (masks): ModuleList(\n",
       "          (0-1): 2 x Mask(mask_mode=scalar)\n",
       "        )\n",
       "        (masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "      )\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): LinearsWithMasks(\n",
       "      (linears): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=3072, out_features=128256, bias=False)\n",
       "      )\n",
       "      (weight_masks): ModuleList(\n",
       "        (0-1): 2 x Mask(mask_mode=vector_input)\n",
       "      )\n",
       "      (weight_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "      (bias_masks): ModuleList(\n",
       "        (0-1): 2 x None\n",
       "      )\n",
       "      (bias_masks_constrainer): Constrainer(constrain_mode=identity)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
