{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f63518dd-ba3e-444b-a3b7-12d52ef0bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import math\n",
    "import yaml\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from IO import (\n",
    "    load_tensors, \n",
    "    get_layer_signatures, \n",
    "    save_checkpoint\n",
    ")\n",
    "\n",
    "def sparse(\n",
    "    v0: Union[np.ndarray, torch.Tensor],\n",
    "    v1: Union[np.ndarray, torch.Tensor],\n",
    "    eps: float = 1e-8,\n",
    "    device: str = \"cpu\"\n",
    "):\n",
    "\n",
    "    # Convert inputs to PyTorch tensors and move to the specified device\n",
    "    if not isinstance(v0, torch.Tensor):\n",
    "        v0 = torch.from_numpy(v0)\n",
    "    if not isinstance(v1, torch.Tensor):\n",
    "        v1 = torch.from_numpy(v1)\n",
    "\n",
    "\n",
    "    v0 = v0.to(device)\n",
    "    v1 = v1.to(device)\n",
    "\n",
    "    v0_copy = v0.clone()\n",
    "    v1_copy = v1.clone()\n",
    "    \n",
    "    diff = torch.abs(v0 - v1)\n",
    "\n",
    "    diff_indices = diff < eps\n",
    "    v1[diff_indices] = v0[diff_indices]\n",
    "\n",
    "    numel = torch.numel(diff)\n",
    "    num_diff = torch.numel(diff[diff_indices]) / numel\n",
    "    # print(f\"Changing {num_diff:.3} of {numel} params.\")\n",
    "    if not torch.allclose(v1, v1_copy, atol=0.0, rtol=0.0):\n",
    "        print(\"what\")\n",
    "    return v1\n",
    "\n",
    "\n",
    "def merge_layer(tensors, merge_config):\n",
    "    \"\"\"\n",
    "    Merges corresponding layers of two models using SLERP.\n",
    "    \n",
    "    Args:\n",
    "        tensors (List[Dict[str, torch.Tensor]]): List of model weight dictionaries.\n",
    "        merge_config (dict): Configuration dictionary containing merging parameters.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, torch.Tensor]: Merged model weights.\n",
    "    \"\"\"\n",
    "    assert len(tensors) == 2    \n",
    "    weight_names = [key for key in tensors[0].keys()]\n",
    "    \n",
    "    for weight_name in weight_names:\n",
    "        tensor_a = tensors[0][weight_name]\n",
    "        tensor_b = tensors[1][weight_name]\n",
    "        \n",
    "        tensor_computed = (\n",
    "            sparse(\n",
    "                tensor_a,\n",
    "                tensor_b,\n",
    "            )\n",
    "            .to(tensor_a.dtype)\n",
    "            .to(tensor_a.device)\n",
    "        )\n",
    "        # print(np.linalg.norm(tensor_computed.float()))\n",
    "        tensors[0][weight_name] = tensor_computed\n",
    "    return tensors[0]\n",
    "\n",
    "def run_merge(\n",
    "    merge_config\n",
    "):\n",
    "    ## Read configs\n",
    "    model_paths = [x['model'] for x in merge_config['sources']]\n",
    "    layer_signatures = get_layer_signatures(model_paths[0])\n",
    "    output_dir = merge_config[\"output_dir\"]\n",
    "    tmp_dir = os.path.join(output_dir, \"tmp_dir\")\n",
    "        \n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.makedirs(tmp_dir)\n",
    "\n",
    "    ## Merge models\n",
    "    for signature in (\n",
    "        pbar := tqdm(\n",
    "            layer_signatures,\n",
    "            desc=\"Merging ...\",\n",
    "            ncols=150\n",
    "        )\n",
    "    ):\n",
    "        pbar.set_description(f\"Merging {signature}\")\n",
    "        models_tensors = [load_tensors(path, signature) for path in model_paths]\n",
    "        merged_tensors = merge_layer(models_tensors, merge_config)\n",
    "        outfile = os.path.join(tmp_dir, f\"{signature.strip('.')}.safetensors\")\n",
    "        save_file(merged_tensors, outfile)\n",
    "\n",
    "    ## Save models\n",
    "    save_checkpoint(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd45442b-3225-4219-8d41-79fe5a78dc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging model.embed_tokens:   0%|                                                                                              | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging layers.19.:  70%|█████████████████████████████████████████████████████████████████                            | 21/30 [00:12<00:04,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging layers.27.: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:17<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying files to /workspace/dont15/models/sparse-merged\n",
      "Saved shard 1 with size 1.97 GB\n",
      "Saved shard 2 with size 1.97 GB\n",
      "Saved shard 3 with size 1.31 GB\n",
      "Saved shard 4 with size 0.73 GB\n",
      "Saved model weight map to /workspace/dont15/models/sparse-merged/model.safetensors.index.json.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    CONFIG_FILE = \"config.yaml\"\n",
    "    with open(CONFIG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        merge_config = yaml.safe_load(f)\n",
    "        \n",
    "    run_merge(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa4f5e-08f6-4117-87e0-bde36ccb05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward():\n",
    "    ...\n",
    "    return dict(\n",
    "        outputs_components=[output_A, output_B, ..., output_D],\n",
    "        output_merged=output_merged\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de54c47c-c61e-4c78-9b25-76fd689ff627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from modeling_qwen2 import (\n",
    "    Qwen2RMSNorm, \n",
    "    Qwen2RotaryEmbedding, \n",
    "    Qwen2MLP, \n",
    "    Qwen2Attention, \n",
    "    Qwen2FlashAttention2, \n",
    "    Qwen2SdpaAttention, \n",
    "    Qwen2DecoderLayer, \n",
    "    Qwen2PreTrainedModel, \n",
    "    Qwen2Model, \n",
    "    Qwen2ForCausalLM,\n",
    ")\n",
    "\n",
    "from configuration_qwen2 import Qwen2Config\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    ")\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d1965f2-f19c-4e35-81c9-480c73fca094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, model, tokenizer, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()\n",
    "\n",
    "def get_logits(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_ids).logits\n",
    "    return logits\n",
    "\n",
    "def get_hidden_states(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids, output_hidden_states=True, use_cache=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaeb2d2a-45d2-4c94-8395-3e0b27b438c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e8b6a9e0bf483e8b6f92e7e7a45df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"/workspace/dont15/models/sparse-merged/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map={\"\":7},\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d4aa515-63a8-4f83-8696-8e65e4a5c44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nProvide a concise, objective summary of the input text in up to three sentences, focusing on key actions and intentions without using second or third person pronouns.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nEmily,\\n\\n\"English Sounds\" is a great recommendation - I\\'ll definitely check it out. And I love the idea of having students record themselves. That kind of self-analysis can be so powerful.\\n\\nA virtual coffee chat next Wednesday sounds perfect. I\\'m free anytime after 2 pm my time (Eastern). Just let me know what time works for you and I\\'ll send over a Zoom link.\\n\\nI\\'m really excited about the idea of proposing a workshop for the conference. Our combined insights could make for a really engaging and practical session. We could each present some strategies that have worked well for us and then facilitate a group discussion where attendees can share their own experiences and ideas.\\n\\nLet me know if you want to brainstorm a title or outline before our chat.\\n\\nLooking forward to catching up more next week!\\n\\nBest,\\nJordan<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"Provide a concise, objective summary of the input text in up to three sentences, focusing on key actions and intentions without using second or third person pronouns.\"\n",
    "prompt = \"Emily,\\n\\n\\\"English Sounds\\\" is a great recommendation - I'll definitely check it out. And I love the idea of having students record themselves. That kind of self-analysis can be so powerful.\\n\\nA virtual coffee chat next Wednesday sounds perfect. I'm free anytime after 2 pm my time (Eastern). Just let me know what time works for you and I'll send over a Zoom link.\\n\\nI'm really excited about the idea of proposing a workshop for the conference. Our combined insights could make for a really engaging and practical session. We could each present some strategies that have worked well for us and then facilitate a group discussion where attendees can share their own experiences and ideas.\\n\\nLet me know if you want to brainstorm a title or outline before our chat.\\n\\nLooking forward to catching up more next week!\\n\\nBest,\\nJordan\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48a3bc91-a2d8-4125-8ef8-bf0f1324e1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jordan agrees with Emily's suggestion to use \"English Sounds\" and supports the idea of students recording themselves. Jordan confirms availability for a virtual coffee chat next Wednesday after 2 pm Eastern and expresses enthusiasm for collaborating on a conference workshop. Jordan suggests presenting strategies and facilitating a group discussion, and invites Emily to brainstorm a title or outline before the chat. Jordan looks forward to catching up more next week.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, model, tokenizer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a949125-3850-4956-aabb-7fbe07be9d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5813c837cb1c44d28b4007b1cdffdb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"/workspace/dont15/models/llama32_smol_summarize_20k/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map={\"\":7},\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cd2989a-5755-4379-8431-76836c605b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jordan agrees with Emily's suggestion to use \"English Sounds\" and supports the idea of students recording themselves. Jordan confirms availability for a virtual coffee chat next Wednesday after 2 pm Eastern and expresses enthusiasm for collaborating on a conference workshop. Jordan suggests presenting strategies and facilitating a group discussion, and invites Emily to brainstorm a title or outline before the chat. Jordan looks forward to catching up more next week.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, model, tokenizer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0bf2e-b61c-4bac-8e46-d305fa538f5a",
   "metadata": {},
   "source": [
    "## Sparse neuron training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dccf098-f580-4f2c-875a-9e21a87c0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "dim = 1024\n",
    "w = torch.rand(dim, dim) ## 1_000_000\n",
    "mask = torch.rand(dim, dim) > 0.9 ## compressable. 1_000_000 / 4\n",
    "p = nn.Parameter(torch.rand(1, torch.sum(mask).item())) ## 100_000\n",
    "\n",
    "sparse = torch.zeros_like(w)\n",
    "sparse[mask] = p\n",
    "\n",
    "x = torch.rand(1, dim)\n",
    "label = torch.ones(1, dim)\n",
    "y = torch.matmul(x, w + sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506e26a-26da-4d7a-a38e-98d6fdb9ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "dim = 1024\n",
    "w = torch.rand(dim, dim)\n",
    "mask = torch.rand(dim, dim) > 0.9\n",
    "p = nn.Parameter(torch.rand(1, torch.sum(mask).item()))\n",
    "\n",
    "# Create a sparse tensor directly\n",
    "indices = torch.nonzero(mask).t()  # Get indices of non-zero elements\n",
    "values = p.squeeze()  # Use p for the non-zero values\n",
    "sparse = torch.sparse_coo_tensor(indices, values, size=(dim, dim))\n",
    "\n",
    "x = torch.rand(1, dim)\n",
    "label = torch.ones(1, dim)\n",
    "y = torch.matmul(x, w + sparse.to_dense()) # Convert to dense for matrix multiplication. Alternatively, you can use torch.sparse.mm.\n",
    "diff = y - label\n",
    "loss = torch.mean(diff ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "904b7f81-06b6-4e65-a4b1-48e39fc12c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IO import load_tensors\n",
    "import torch\n",
    "\n",
    "def normalize(v: torch.Tensor, eps: float = 1e-8):\n",
    "    norm_v = torch.norm(v)\n",
    "    if norm_v > eps:\n",
    "        v = v / norm_v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed850e77-daf4-4218-8bfe-b8387ef455ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    # \"/workspace/dont15/models/llama32_smol_rewrite_50k/\",\n",
    "    # \"/workspace/dont15/models/llama32_smol_summarize_20k/\",\n",
    "    \"/workspace/HUB_LLM/Llama-3.2-3B\",\n",
    "    \"/workspace/HUB_LLM/Llama-3.2-3B-Instruct\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29ae6a1-552c-465c-a2b7-a36f93c48b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = \"layers.15\"\n",
    "weight_name = f'model.{signature}.mlp.up_proj.weight'\n",
    "tensors_1 = load_tensors(model_paths[0], signature=signature)[weight_name]\n",
    "tensors_2 = load_tensors(model_paths[1], signature=signature)[weight_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5da9587-f6d7-4df8-b3fa-711f1e3c4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_indices = torch.abs(tensors_1 - tensors_2) > 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87be5d60-d71e-464e-8fb3-faaa571072f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9881)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(diff_indices) / torch.numel(diff_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efd4d138-4864-4cac-97c0-dbe198a2c3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9901)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm1 = normalize(tensors_1[diff_indices].float())\n",
    "norm2 = normalize(tensors_2[diff_indices].float())\n",
    "torch.sum(norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "46e2b895-3fe6-4af0-859b-dce38be95e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0007, -0.0036,  0.0055,  ..., -0.0026,  0.0042,  0.0072],\n",
       "        [ 0.0001,  0.0038, -0.0002,  ..., -0.0013, -0.0048,  0.0027],\n",
       "        [ 0.0002,  0.0059,  0.0009,  ...,  0.0018, -0.0016, -0.0029],\n",
       "        ...,\n",
       "        [-0.0010,  0.0031, -0.0014,  ..., -0.0011,  0.0042,  0.0041],\n",
       "        [ 0.0007, -0.0007, -0.0010,  ..., -0.0028, -0.0020, -0.0002],\n",
       "        [-0.0005,  0.0048, -0.0015,  ...,  0.0026,  0.0031,  0.0006]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors_1 - tensors_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "05004caf-6a4b-4051-bc89-beca0e31b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(tensors_1) == torch.norm(tensors_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e8ae9237-c71b-4057-b5f6-4ece1bc8ed93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5078, dtype=torch.bfloat16), tensor(0.4941, dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(tensors_1)), torch.max(torch.abs(tensors_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "206a6e71-31c3-4b43-8a9b-b0602a28a3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.8750, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(tensors_1 - tensors_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "01b1cba4-f3ef-4378-a61d-2b7ca7cb8d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(89.9470)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(tensors_1.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "96ef100c-ed00-4f63-832e-e84b0dc44749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(88.8236)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(tensors_2.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "142f9b8e-0b9a-4d54-b20f-c2bdc4f2ccb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9882)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(tensors_1 != tensors_2) / torch.numel(tensors_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8bef7d6c-a76f-4bab-be91-1b2a3ff39be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_indices = tensors_1 != tensors_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f78d8462-8201-48e9-93cd-f8ca7b9e8f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9904)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm1 = normalize(tensors_1.float())\n",
    "norm2 = normalize(tensors_2.float())\n",
    "torch.sum(norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33b37111-c0d6-4864-9302-3a8f207b637c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9900)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm1 = normalize(tensors_1[diff_indices].float())\n",
    "norm2 = normalize(tensors_2[diff_indices].float())\n",
    "torch.sum(norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efb2d9c5-0473-4ee5-857f-86e59ce273be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9903)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(tensors_1.float() * tensors_2.float()) / \\\n",
    "(torch.norm(tensors_1.float()) * torch.norm(tensors_2.float()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
