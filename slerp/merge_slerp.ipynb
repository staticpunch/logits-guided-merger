{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda7962c-d7a9-4cd2-97e1-6b3e82583f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def matching(pattern: str, text: str) -> bool:\n",
    "   # Convert wildcard * to regex .*\n",
    "   regex = pattern.replace('.', '\\.').replace('*', '[^.]*')\n",
    "   return bool(re.search(regex, text))\n",
    "\n",
    "def _load_bin_tensors(path, signature):\n",
    "    state_dict = {}\n",
    "    for shard_path in sorted(\n",
    "        f for f in os.listdir(path) if f.endswith('.bin')\n",
    "    ):\n",
    "        checkpoint = torch.load(\n",
    "            os.path.join(path, shard_path), map_location=\"cpu\", weights_only=True)\n",
    "        state_dict.update({\n",
    "            k: v for k, v in checkpoint.items() if matching(signature, k)\n",
    "        })\n",
    "    return state_dict\n",
    "\n",
    "def _load_safe_tensors(path, signature): \n",
    "    state_dict = {}\n",
    "    for shard_path in sorted(\n",
    "        f for f in os.listdir(path) if f.endswith('.safetensors')\n",
    "    ):\n",
    "        with safe_open(\n",
    "            os.path.join(path, shard_path), framework=\"pt\", device=\"cpu\"\n",
    "        ) as f:\n",
    "            state_dict.update({\n",
    "                k: f.get_tensor(k) for k in f.keys() if matching(signature, k)\n",
    "            })\n",
    "    return state_dict\n",
    "\n",
    "def load_tensors(path, signature=\"\", tensor_format=\"safe\"):\n",
    "    loaders = {\n",
    "        \"torch\": _load_bin_tensors,\n",
    "        \"safe\": _load_safe_tensors\n",
    "    }\n",
    "    if tensor_format not in loaders:\n",
    "        raise ValueError(f\"Unsupported tensor format: {tensor_format}\")\n",
    "    return loaders[tensor_format](path, signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33065c41-3ca5-40c6-a465-51fb048103b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(\n",
    "    t: float, v0: Union[np.ndarray, torch.Tensor], v1: Union[np.ndarray, torch.Tensor]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    return (1 - t) * v0 + t * v1\n",
    "\n",
    "\n",
    "def slerp(\n",
    "    t: Union[float, np.ndarray],\n",
    "    v0: Union[np.ndarray, torch.Tensor],\n",
    "    v1: Union[np.ndarray, torch.Tensor],\n",
    "    DOT_THRESHOLD: float = 0.9995,\n",
    "    eps: float = 1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Spherical linear interpolation\n",
    "\n",
    "    From: https://gist.github.com/dvschultz/3af50c40df002da3b751efab1daddf2c\n",
    "    Args:\n",
    "        t (float/np.ndarray): Float value between 0.0 and 1.0\n",
    "        v0 (np.ndarray): Starting vector\n",
    "        v1 (np.ndarray): Final vector\n",
    "        DOT_THRESHOLD (float): Threshold for considering the two vectors as\n",
    "                               colinear. Not recommended to alter this.\n",
    "    Returns:\n",
    "        v2 (np.ndarray): Interpolation vector between v0 and v1\n",
    "    \"\"\"\n",
    "    is_torch = False\n",
    "    if not isinstance(v0, np.ndarray):\n",
    "        is_torch = True\n",
    "        v0 = v0.detach().cpu().float().numpy()\n",
    "    if not isinstance(v1, np.ndarray):\n",
    "        is_torch = True\n",
    "        v1 = v1.detach().cpu().float().numpy()\n",
    "\n",
    "    # Copy the vectors to reuse them later\n",
    "    v0_copy = np.copy(v0)\n",
    "    v1_copy = np.copy(v1)\n",
    "\n",
    "    # Normalize the vectors to get the directions and angles\n",
    "    v0 = normalize(v0, eps)\n",
    "    v1 = normalize(v1, eps)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "\n",
    "    # Dot product with the normalized vectors (can't use np.dot in W)\n",
    "    dot = np.sum(v0 * v1)\n",
    "\n",
    "    # If absolute value of dot product is almost 1, vectors are ~colinear, so use lerp\n",
    "    if np.abs(dot) > DOT_THRESHOLD:\n",
    "        # print(\"tensors vewi similar!\")\n",
    "        res = lerp(t, v0_copy, v1_copy)\n",
    "        return maybe_torch(res, is_torch)\n",
    "\n",
    "    # Calculate initial angle between v0 and v1\n",
    "    theta_0 = np.arccos(dot)\n",
    "    sin_theta_0 = np.sin(theta_0)\n",
    "\n",
    "    # Angle at timestep t\n",
    "    theta_t = theta_0 * t\n",
    "    sin_theta_t = np.sin(theta_t)\n",
    "\n",
    "    # Finish the slerp algorithm\n",
    "    s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
    "    s1 = sin_theta_t / sin_theta_0\n",
    "    res = s0 * v0_copy + s1 * v1_copy\n",
    "    # print(\"tensors vewi distant...\")\n",
    "    return maybe_torch(res, is_torch)\n",
    "\n",
    "\n",
    "def maybe_torch(v: np.ndarray, is_torch: bool):\n",
    "    if is_torch:\n",
    "        return torch.from_numpy(v)\n",
    "    return v\n",
    "\n",
    "\n",
    "def normalize(v: np.ndarray, eps: float):\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_v > eps:\n",
    "        v = v / norm_v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e7f7ae9-54d0-49a5-9fa3-25829601ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend(\n",
    "    weight_name: str, \n",
    "    parameters: dict, \n",
    "    layer_idx: int, \n",
    "    num_layers: int\n",
    "):\n",
    "    assert isinstance(layer_idx, int) or layer_idx is None, (\n",
    "        f\"If the weight {weight_name} belongs to an i-th layer, \"\n",
    "        f\"the argument `layer_idx` should be an integer. Otherwise \"\n",
    "        f\"it should be a NoneType object. Found `layer_idx` = \"\n",
    "        f\"{layer_idx}.\"\n",
    "    )\n",
    "    assert isinstance(num_layers, int), (\n",
    "        f\"You must specify proper argument `num_layers` \"\n",
    "        f\"of type `int`. Found `num_layers` = {num_layers}.\"\n",
    "    )\n",
    "    if isinstance(layer_idx, int):\n",
    "        assert layer_idx <= num_layers - 1, (\n",
    "            f\"The argument `layer_idx` must have lower value than \"\n",
    "            f\"the argument `num_layers`. Found \"\n",
    "            f\"`layer_idx` = {layer_idx}, `num_layers` = {num_layers}.\"\n",
    "        )\n",
    "    \n",
    "    matching_filter = next(\n",
    "        (f for f in parameters if matching(f, weight_name)),\n",
    "        'default'\n",
    "    )\n",
    "    anchors = parameters[matching_filter]\n",
    "    if not isinstance(anchors, list):\n",
    "        anchors = [anchors]\n",
    "\n",
    "    if layer_idx is None:\n",
    "        return anchors[0]\n",
    "        \n",
    "    # Calculate interpolation for layer-specific weights\n",
    "    layer_fraction = layer_idx / (num_layers - 1)\n",
    "    anchor_position = layer_fraction * (len(anchors) - 1)\n",
    "    \n",
    "    lower_idx = math.floor(anchor_position)\n",
    "    upper_idx = min(len(anchors) - 1, lower_idx + 1)\n",
    "    fraction = anchor_position - lower_idx\n",
    "\n",
    "    interpolated = (\n",
    "        (1 - fraction) * anchors[lower_idx]\n",
    "        + fraction * anchors[upper_idx]\n",
    "    )\n",
    "    return interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff35e254-d4b7-4259-8101-eb7f3f2ffa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_small_files(base_path, output_dir):\n",
    "    print(f\"Copying files to {output_dir}\")\n",
    "    files_to_copy = [\n",
    "        \"config.json\",\n",
    "        \"generation_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"tokenizer.json\",\n",
    "        \"sentencepiece.bpe.model\",\n",
    "        \"sparse_linear.pt\",\n",
    "        \"colbert_linear.pt\",\n",
    "        \"config_sentence_transformers.json\",\n",
    "        \"sentence_bert_config.json\",\n",
    "        \"modules.json\"\n",
    "    ]\n",
    "    for filename in files_to_copy:\n",
    "        src_path = os.path.join(base_path, filename)\n",
    "        dst_path = os.path.join(output_dir, filename)\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy2(src_path, dst_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd44f0d-727a-41cc-b0ef-d7c9189dfd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_merge(merger_config):\n",
    "    sd0 = load_tensors(merger_config[\"models\"][0], tensor_format=\"torch\")\n",
    "    sd1 = load_tensors(merger_config[\"models\"][1], tensor_format=\"safe\")\n",
    "\n",
    "    parameters = merger_config[\"parameters\"]\n",
    "    num_layers = merger_config[\"num_layers\"]\n",
    "    output_dir = merger_config[\"output_dir\"]\n",
    "    \n",
    "    keys = list(sd0.keys())\n",
    "    merged_dict = {}\n",
    "    for weight_name in tqdm(keys, desc=f\"Merging\"):\n",
    "        find_layer = re.search(r\"layer\\.([^\\.]*)\\.\", weight_name)\n",
    "        layer_idx = int(find_layer.group(1)) if find_layer else None\n",
    "        t = blend(weight_name, parameters, layer_idx, num_layers)\n",
    "        v0, v1 = sd0[weight_name], sd1[weight_name]\n",
    "        merged_weight = slerp(t, v0, v1)\n",
    "        merged_dict[weight_name] = merged_weight\n",
    "\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    copy_small_files(\n",
    "        base_path=merger_config[\"models\"][0],\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    model_outfile = os.path.join(output_dir, \"model.safetensors\")\n",
    "    metadata = {\"format\": \"pt\"}\n",
    "    save_file(merged_dict, model_outfile, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09cdc870-d3eb-432d-98b5-ed5ce72fa500",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"layer.*.attention\": [0, 0.3, 0.5, 0.7, 1],\n",
    "    \"layer.*.intermediate\": [1, 0.7, 0.5, 0.3, 0],\n",
    "    \"layer.*.output\": [1, 0.7, 0.5, 0.3, 0],\n",
    "    \"default\": 0.5\n",
    "}\n",
    "\n",
    "config_a = {\n",
    "    \"models\": [\n",
    "        \"/workspace/models/bge-m3/\",\n",
    "        \"/workspace/models/chieunq-embedding/checkpoint-10000/\"\n",
    "    ],\n",
    "    \"num_layers\": 24,\n",
    "    \"parameters\": parameters,\n",
    "    \"torch_dtype\": \"float32\",\n",
    "    \"output_dir\": \"/workspace/models/chieunq/checkpoint-10002\"\n",
    "}\n",
    "\n",
    "config_b = {\n",
    "    \"models\": [\n",
    "        \"/workspace/models/bge-m3/\",\n",
    "        \"/workspace/models/chieunq-embedding/checkpoint-60000/\"\n",
    "    ],\n",
    "    \"num_layers\": 24,\n",
    "    \"parameters\": parameters,\n",
    "    \"torch_dtype\": \"float32\",\n",
    "    \"output_dir\": \"/workspace/models/chieunq/checkpoint-60002/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c21d866-d123-4429-8154-f779ff4e5b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8463068e13649cdbbe9174781a3e2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying files to /workspace/models/chieunq/checkpoint-10002\n"
     ]
    }
   ],
   "source": [
    "run_merge(config_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f0be31-b6bc-4429-8cbf-68f992466ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e145e85e794be19c82093aec82ea22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying files to /workspace/models/chieunq/checkpoint-60002/\n"
     ]
    }
   ],
   "source": [
    "run_merge(config_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2dee98a-9f2e-4833-b229-3a930c2d189e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (148060486.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    token =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "token = \"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\" ## DO\n",
    "# token = \"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\" ## DAO\n",
    "token = \"HF_TOKEN\"\n",
    "\n",
    "def upload(repo_id, local_dir, target_dir=\"\"):\n",
    "    create_repo(\n",
    "        repo_id=repo_id,\n",
    "        private=False,\n",
    "        repo_type=\"model\",\n",
    "        exist_ok=True,\n",
    "        token=token\n",
    "    )\n",
    "    api = HfApi()\n",
    "    api.upload_folder(\n",
    "        folder_path=local_dir,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "        path_in_repo=target_dir,  # Specify target directory\n",
    "        token=token\n",
    "    )\n",
    "\n",
    "def delete(repo_id, target_dir):\n",
    "    api = HfApi()\n",
    "    api.delete_folder(\n",
    "        repo_id=repo_id,\n",
    "        path_in_repo=target_dir,\n",
    "        repo_type=\"model\",\n",
    "        token=token\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "679568bf-faca-4e4f-88ca-b560629d634d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5cee16097e4081badce7d2b801146c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "upload(\n",
    "    repo_id=\"nguyenthanhdo/merged-embedding\",\n",
    "    local_dir=\"/workspace/models/chieunq/checkpoint-60002\",\n",
    "    target_dir=\"checkpoint-60002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a5d6849-7348-41f6-b772-cf0a2d575e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"/workspace/models/chieunq/checkpoint-10002\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
