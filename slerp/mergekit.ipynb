{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03120036-34d1-4c26-9a0b-ef0f1367a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually do merge\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.merge import MergeOptions, run_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bedf5d74-ff6d-464e-8e8c-f8c70ab4ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../models/merged-test\"  # folder to store the result in\n",
    "LORA_MERGE_CACHE = \"/tmp\"  # change if you want to keep these for some reason\n",
    "CONFIG_YML = \"slerp-config.yaml\"  # merge configuration file\n",
    "COPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\n",
    "LAZY_UNPICKLE = False  # experimental low-memory model loader\n",
    "LOW_CPU_MEMORY = False  # enable if you somehow have more VRAM than RAM+swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b01a36-5eec-4675-bb7f-fa2794ff3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
    "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\n",
    "\n",
    "options=MergeOptions(\n",
    "\tlora_merge_cache=LORA_MERGE_CACHE,\n",
    "\t# cuda=torch.cuda.is_available(),\n",
    "\tcopy_tokenizer=COPY_TOKENIZER,\n",
    "\tlazy_unpickle=LAZY_UNPICKLE,\n",
    "\tlow_cpu_memory=LOW_CPU_MEMORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ed27e9-d612-4abb-837a-2ecfb3da0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import importlib.resources\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "import tqdm\n",
    "import transformers\n",
    "\n",
    "from mergekit._data import chat_templates\n",
    "from mergekit.architecture import ArchitectureInfo, get_architecture_info\n",
    "from mergekit.card import generate_card\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.graph import Executor\n",
    "from mergekit.io.tasks import LoaderCache\n",
    "from mergekit.options import MergeOptions\n",
    "from mergekit.plan import MergePlanner\n",
    "from mergekit.tokenizer import TokenizerInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72712bf-3869-4512-b7ee-a2cc8d1e3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mergekit.merge import (\n",
    "\t_model_out_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d99355b-57a3-46b6-aa3e-e1e77bd52e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if options.random_seed is not None:\n",
    "\ttransformers.trainer_utils.set_seed(options.random_seed)\n",
    "\n",
    "if not merge_config.models and not merge_config.slices:\n",
    "\traise RuntimeError(\"No output requested\")\n",
    "\n",
    "model_arch_info = [\n",
    "\tget_architecture_info(m.config(trust_remote_code=options.trust_remote_code))\n",
    "\tfor m in merge_config.referenced_models()\n",
    "]\n",
    "if not options.allow_crimes:\n",
    "\tif not all(a == model_arch_info[0] for a in model_arch_info[1:]):\n",
    "\t\traise RuntimeError(\n",
    "\t\t\t\"Must specify --allow-crimes to attempt to mix different architectures\"\n",
    "\t\t)\n",
    "arch_info = model_arch_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad378a35-20c9-4d91-9c65-15c4a7daf568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup loader cache: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 665.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize loader cache and set options\n",
    "loader_cache = LoaderCache()\n",
    "loader_cache.setup(options=options)\n",
    "\n",
    "# create config for output model\n",
    "cfg_out = _model_out_config(\n",
    "\tmerge_config, arch_info, trust_remote_code=options.trust_remote_code\n",
    ")\n",
    "\n",
    "# warm up loader cache\n",
    "for model in (\n",
    "\tpbar := tqdm.tqdm(\n",
    "\t\tmerge_config.referenced_models(),\n",
    "\t\tdesc=\"Warmup loader cache\",\n",
    "\t\tdisable=options.quiet,\n",
    "\t)\n",
    "):\n",
    "\tloader_cache.get(model)\n",
    "del pbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee0ae506-1dd1-427d-ac67-182864cae9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = MergePlanner(\n",
    "\tmerge_config,\n",
    "\tarch_info,\n",
    "\toptions=options,\n",
    "\tout_model_config=cfg_out,\n",
    ")\n",
    "\n",
    "targets = planner.plan_to_disk(out_path=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69b61142-2e30-4345-8947-c84923d674d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "model.embed_tokens.weight 0.5\n",
      "------------------------------------------------------------\n",
      "model.layers.0.input_layernorm.weight 0.5\n",
      "model.layers.0.self_attn.q_proj.weight 0.0\n",
      "model.layers.0.self_attn.k_proj.weight 0.0\n",
      "model.layers.0.self_attn.v_proj.weight 0.0\n",
      "model.layers.0.self_attn.o_proj.weight 0.0\n",
      "model.layers.0.post_attention_layernorm.weight 0.5\n",
      "model.layers.0.mlp.up_proj.weight 1.0\n",
      "model.layers.0.mlp.gate_proj.weight 1.0\n",
      "model.layers.0.mlp.down_proj.weight 1.0\n",
      "------------------------------------------------------------\n",
      "model.layers.1.input_layernorm.weight 0.5\n",
      "model.layers.1.self_attn.q_proj.weight 0.06451612903225806\n",
      "model.layers.1.self_attn.k_proj.weight 0.06451612903225806\n",
      "model.layers.1.self_attn.v_proj.weight 0.06451612903225806\n",
      "model.layers.1.self_attn.o_proj.weight 0.06451612903225806\n",
      "model.layers.1.post_attention_layernorm.weight 0.5\n",
      "model.layers.1.mlp.up_proj.weight 0.935483870967742\n",
      "model.layers.1.mlp.gate_proj.weight 0.935483870967742\n",
      "model.layers.1.mlp.down_proj.weight 0.935483870967742\n",
      "------------------------------------------------------------\n",
      "model.layers.2.input_layernorm.weight 0.5\n",
      "model.layers.2.self_attn.q_proj.weight 0.12903225806451613\n",
      "model.layers.2.self_attn.k_proj.weight 0.12903225806451613\n",
      "model.layers.2.self_attn.v_proj.weight 0.12903225806451613\n",
      "model.layers.2.self_attn.o_proj.weight 0.12903225806451613\n",
      "model.layers.2.post_attention_layernorm.weight 0.5\n",
      "model.layers.2.mlp.up_proj.weight 0.8709677419354839\n",
      "model.layers.2.mlp.gate_proj.weight 0.8709677419354839\n",
      "model.layers.2.mlp.down_proj.weight 0.8709677419354839\n",
      "------------------------------------------------------------\n",
      "model.layers.3.input_layernorm.weight 0.5\n",
      "model.layers.3.self_attn.q_proj.weight 0.1935483870967742\n",
      "model.layers.3.self_attn.k_proj.weight 0.1935483870967742\n",
      "model.layers.3.self_attn.v_proj.weight 0.1935483870967742\n",
      "model.layers.3.self_attn.o_proj.weight 0.1935483870967742\n",
      "model.layers.3.post_attention_layernorm.weight 0.5\n",
      "model.layers.3.mlp.up_proj.weight 0.8064516129032258\n",
      "model.layers.3.mlp.gate_proj.weight 0.8064516129032258\n",
      "model.layers.3.mlp.down_proj.weight 0.8064516129032258\n",
      "------------------------------------------------------------\n",
      "model.layers.4.input_layernorm.weight 0.5\n",
      "model.layers.4.self_attn.q_proj.weight 0.25806451612903225\n",
      "model.layers.4.self_attn.k_proj.weight 0.25806451612903225\n",
      "model.layers.4.self_attn.v_proj.weight 0.25806451612903225\n",
      "model.layers.4.self_attn.o_proj.weight 0.25806451612903225\n",
      "model.layers.4.post_attention_layernorm.weight 0.5\n",
      "model.layers.4.mlp.up_proj.weight 0.7419354838709677\n",
      "model.layers.4.mlp.gate_proj.weight 0.7419354838709677\n",
      "model.layers.4.mlp.down_proj.weight 0.7419354838709677\n",
      "------------------------------------------------------------\n",
      "model.layers.5.input_layernorm.weight 0.5\n",
      "model.layers.5.self_attn.q_proj.weight 0.3225806451612903\n",
      "model.layers.5.self_attn.k_proj.weight 0.3225806451612903\n",
      "model.layers.5.self_attn.v_proj.weight 0.3225806451612903\n",
      "model.layers.5.self_attn.o_proj.weight 0.3225806451612903\n",
      "model.layers.5.post_attention_layernorm.weight 0.5\n",
      "model.layers.5.mlp.up_proj.weight 0.6774193548387097\n",
      "model.layers.5.mlp.gate_proj.weight 0.6774193548387097\n",
      "model.layers.5.mlp.down_proj.weight 0.6774193548387097\n",
      "------------------------------------------------------------\n",
      "model.layers.6.input_layernorm.weight 0.5\n",
      "model.layers.6.self_attn.q_proj.weight 0.3870967741935484\n",
      "model.layers.6.self_attn.k_proj.weight 0.3870967741935484\n",
      "model.layers.6.self_attn.v_proj.weight 0.3870967741935484\n",
      "model.layers.6.self_attn.o_proj.weight 0.3870967741935484\n",
      "model.layers.6.post_attention_layernorm.weight 0.5\n",
      "model.layers.6.mlp.up_proj.weight 0.6129032258064516\n",
      "model.layers.6.mlp.gate_proj.weight 0.6129032258064516\n",
      "model.layers.6.mlp.down_proj.weight 0.6129032258064516\n",
      "------------------------------------------------------------\n",
      "model.layers.7.input_layernorm.weight 0.5\n",
      "model.layers.7.self_attn.q_proj.weight 0.45161290322580644\n",
      "model.layers.7.self_attn.k_proj.weight 0.45161290322580644\n",
      "model.layers.7.self_attn.v_proj.weight 0.45161290322580644\n",
      "model.layers.7.self_attn.o_proj.weight 0.45161290322580644\n",
      "model.layers.7.post_attention_layernorm.weight 0.5\n",
      "model.layers.7.mlp.up_proj.weight 0.5483870967741935\n",
      "model.layers.7.mlp.gate_proj.weight 0.5483870967741935\n",
      "model.layers.7.mlp.down_proj.weight 0.5483870967741935\n",
      "------------------------------------------------------------\n",
      "model.layers.8.input_layernorm.weight 0.5\n",
      "model.layers.8.self_attn.q_proj.weight 0.4935483870967742\n",
      "model.layers.8.self_attn.k_proj.weight 0.4935483870967742\n",
      "model.layers.8.self_attn.v_proj.weight 0.4935483870967742\n",
      "model.layers.8.self_attn.o_proj.weight 0.4935483870967742\n",
      "model.layers.8.post_attention_layernorm.weight 0.5\n",
      "model.layers.8.mlp.up_proj.weight 0.5064516129032258\n",
      "model.layers.8.mlp.gate_proj.weight 0.5064516129032258\n",
      "model.layers.8.mlp.down_proj.weight 0.5064516129032258\n",
      "------------------------------------------------------------\n",
      "model.layers.9.input_layernorm.weight 0.5\n",
      "model.layers.9.self_attn.q_proj.weight 0.46774193548387094\n",
      "model.layers.9.self_attn.k_proj.weight 0.46774193548387094\n",
      "model.layers.9.self_attn.v_proj.weight 0.46774193548387094\n",
      "model.layers.9.self_attn.o_proj.weight 0.46774193548387094\n",
      "model.layers.9.post_attention_layernorm.weight 0.5\n",
      "model.layers.9.mlp.up_proj.weight 0.532258064516129\n",
      "model.layers.9.mlp.gate_proj.weight 0.532258064516129\n",
      "model.layers.9.mlp.down_proj.weight 0.532258064516129\n",
      "------------------------------------------------------------\n",
      "model.layers.10.input_layernorm.weight 0.5\n",
      "model.layers.10.self_attn.q_proj.weight 0.44193548387096776\n",
      "model.layers.10.self_attn.k_proj.weight 0.44193548387096776\n",
      "model.layers.10.self_attn.v_proj.weight 0.44193548387096776\n",
      "model.layers.10.self_attn.o_proj.weight 0.44193548387096776\n",
      "model.layers.10.post_attention_layernorm.weight 0.5\n",
      "model.layers.10.mlp.up_proj.weight 0.5580645161290323\n",
      "model.layers.10.mlp.gate_proj.weight 0.5580645161290323\n",
      "model.layers.10.mlp.down_proj.weight 0.5580645161290323\n",
      "------------------------------------------------------------\n",
      "model.layers.11.input_layernorm.weight 0.5\n",
      "model.layers.11.self_attn.q_proj.weight 0.4161290322580645\n",
      "model.layers.11.self_attn.k_proj.weight 0.4161290322580645\n",
      "model.layers.11.self_attn.v_proj.weight 0.4161290322580645\n",
      "model.layers.11.self_attn.o_proj.weight 0.4161290322580645\n",
      "model.layers.11.post_attention_layernorm.weight 0.5\n",
      "model.layers.11.mlp.up_proj.weight 0.5838709677419355\n",
      "model.layers.11.mlp.gate_proj.weight 0.5838709677419355\n",
      "model.layers.11.mlp.down_proj.weight 0.5838709677419355\n",
      "------------------------------------------------------------\n",
      "model.layers.12.input_layernorm.weight 0.5\n",
      "model.layers.12.self_attn.q_proj.weight 0.3903225806451613\n",
      "model.layers.12.self_attn.k_proj.weight 0.3903225806451613\n",
      "model.layers.12.self_attn.v_proj.weight 0.3903225806451613\n",
      "model.layers.12.self_attn.o_proj.weight 0.3903225806451613\n",
      "model.layers.12.post_attention_layernorm.weight 0.5\n",
      "model.layers.12.mlp.up_proj.weight 0.6096774193548387\n",
      "model.layers.12.mlp.gate_proj.weight 0.6096774193548387\n",
      "model.layers.12.mlp.down_proj.weight 0.6096774193548387\n",
      "------------------------------------------------------------\n",
      "model.layers.13.input_layernorm.weight 0.5\n",
      "model.layers.13.self_attn.q_proj.weight 0.36451612903225805\n",
      "model.layers.13.self_attn.k_proj.weight 0.36451612903225805\n",
      "model.layers.13.self_attn.v_proj.weight 0.36451612903225805\n",
      "model.layers.13.self_attn.o_proj.weight 0.36451612903225805\n",
      "model.layers.13.post_attention_layernorm.weight 0.5\n",
      "model.layers.13.mlp.up_proj.weight 0.635483870967742\n",
      "model.layers.13.mlp.gate_proj.weight 0.635483870967742\n",
      "model.layers.13.mlp.down_proj.weight 0.635483870967742\n",
      "------------------------------------------------------------\n",
      "model.layers.14.input_layernorm.weight 0.5\n",
      "model.layers.14.self_attn.q_proj.weight 0.33870967741935487\n",
      "model.layers.14.self_attn.k_proj.weight 0.33870967741935487\n",
      "model.layers.14.self_attn.v_proj.weight 0.33870967741935487\n",
      "model.layers.14.self_attn.o_proj.weight 0.33870967741935487\n",
      "model.layers.14.post_attention_layernorm.weight 0.5\n",
      "model.layers.14.mlp.up_proj.weight 0.6612903225806451\n",
      "model.layers.14.mlp.gate_proj.weight 0.6612903225806451\n",
      "model.layers.14.mlp.down_proj.weight 0.6612903225806451\n",
      "------------------------------------------------------------\n",
      "model.layers.15.input_layernorm.weight 0.5\n",
      "model.layers.15.self_attn.q_proj.weight 0.3129032258064516\n",
      "model.layers.15.self_attn.k_proj.weight 0.3129032258064516\n",
      "model.layers.15.self_attn.v_proj.weight 0.3129032258064516\n",
      "model.layers.15.self_attn.o_proj.weight 0.3129032258064516\n",
      "model.layers.15.post_attention_layernorm.weight 0.5\n",
      "model.layers.15.mlp.up_proj.weight 0.6870967741935483\n",
      "model.layers.15.mlp.gate_proj.weight 0.6870967741935483\n",
      "model.layers.15.mlp.down_proj.weight 0.6870967741935483\n",
      "------------------------------------------------------------\n",
      "model.layers.16.input_layernorm.weight 0.5\n",
      "model.layers.16.self_attn.q_proj.weight 0.32580645161290317\n",
      "model.layers.16.self_attn.k_proj.weight 0.32580645161290317\n",
      "model.layers.16.self_attn.v_proj.weight 0.32580645161290317\n",
      "model.layers.16.self_attn.o_proj.weight 0.32580645161290317\n",
      "model.layers.16.post_attention_layernorm.weight 0.5\n",
      "model.layers.16.mlp.up_proj.weight 0.6741935483870967\n",
      "model.layers.16.mlp.gate_proj.weight 0.6741935483870967\n",
      "model.layers.16.mlp.down_proj.weight 0.6741935483870967\n",
      "------------------------------------------------------------\n",
      "model.layers.17.input_layernorm.weight 0.5\n",
      "model.layers.17.self_attn.q_proj.weight 0.3774193548387096\n",
      "model.layers.17.self_attn.k_proj.weight 0.3774193548387096\n",
      "model.layers.17.self_attn.v_proj.weight 0.3774193548387096\n",
      "model.layers.17.self_attn.o_proj.weight 0.3774193548387096\n",
      "model.layers.17.post_attention_layernorm.weight 0.5\n",
      "model.layers.17.mlp.up_proj.weight 0.6225806451612903\n",
      "model.layers.17.mlp.gate_proj.weight 0.6225806451612903\n",
      "model.layers.17.mlp.down_proj.weight 0.6225806451612903\n",
      "------------------------------------------------------------\n",
      "model.layers.18.input_layernorm.weight 0.5\n",
      "model.layers.18.self_attn.q_proj.weight 0.42903225806451617\n",
      "model.layers.18.self_attn.k_proj.weight 0.42903225806451617\n",
      "model.layers.18.self_attn.v_proj.weight 0.42903225806451617\n",
      "model.layers.18.self_attn.o_proj.weight 0.42903225806451617\n",
      "model.layers.18.post_attention_layernorm.weight 0.5\n",
      "model.layers.18.mlp.up_proj.weight 0.5709677419354838\n",
      "model.layers.18.mlp.gate_proj.weight 0.5709677419354838\n",
      "model.layers.18.mlp.down_proj.weight 0.5709677419354838\n",
      "------------------------------------------------------------\n",
      "model.layers.19.input_layernorm.weight 0.5\n",
      "model.layers.19.self_attn.q_proj.weight 0.4806451612903226\n",
      "model.layers.19.self_attn.k_proj.weight 0.4806451612903226\n",
      "model.layers.19.self_attn.v_proj.weight 0.4806451612903226\n",
      "model.layers.19.self_attn.o_proj.weight 0.4806451612903226\n",
      "model.layers.19.post_attention_layernorm.weight 0.5\n",
      "model.layers.19.mlp.up_proj.weight 0.5193548387096774\n",
      "model.layers.19.mlp.gate_proj.weight 0.5193548387096774\n",
      "model.layers.19.mlp.down_proj.weight 0.5193548387096774\n",
      "------------------------------------------------------------\n",
      "model.layers.20.input_layernorm.weight 0.5\n",
      "model.layers.20.self_attn.q_proj.weight 0.532258064516129\n",
      "model.layers.20.self_attn.k_proj.weight 0.532258064516129\n",
      "model.layers.20.self_attn.v_proj.weight 0.532258064516129\n",
      "model.layers.20.self_attn.o_proj.weight 0.532258064516129\n",
      "model.layers.20.post_attention_layernorm.weight 0.5\n",
      "model.layers.20.mlp.up_proj.weight 0.467741935483871\n",
      "model.layers.20.mlp.gate_proj.weight 0.467741935483871\n",
      "model.layers.20.mlp.down_proj.weight 0.467741935483871\n",
      "------------------------------------------------------------\n",
      "model.layers.21.input_layernorm.weight 0.5\n",
      "model.layers.21.self_attn.q_proj.weight 0.5838709677419354\n",
      "model.layers.21.self_attn.k_proj.weight 0.5838709677419354\n",
      "model.layers.21.self_attn.v_proj.weight 0.5838709677419354\n",
      "model.layers.21.self_attn.o_proj.weight 0.5838709677419354\n",
      "model.layers.21.post_attention_layernorm.weight 0.5\n",
      "model.layers.21.mlp.up_proj.weight 0.4161290322580646\n",
      "model.layers.21.mlp.gate_proj.weight 0.4161290322580646\n",
      "model.layers.21.mlp.down_proj.weight 0.4161290322580646\n",
      "------------------------------------------------------------\n",
      "model.layers.22.input_layernorm.weight 0.5\n",
      "model.layers.22.self_attn.q_proj.weight 0.635483870967742\n",
      "model.layers.22.self_attn.k_proj.weight 0.635483870967742\n",
      "model.layers.22.self_attn.v_proj.weight 0.635483870967742\n",
      "model.layers.22.self_attn.o_proj.weight 0.635483870967742\n",
      "model.layers.22.post_attention_layernorm.weight 0.5\n",
      "model.layers.22.mlp.up_proj.weight 0.364516129032258\n",
      "model.layers.22.mlp.gate_proj.weight 0.364516129032258\n",
      "model.layers.22.mlp.down_proj.weight 0.364516129032258\n",
      "------------------------------------------------------------\n",
      "model.layers.23.input_layernorm.weight 0.5\n",
      "model.layers.23.self_attn.q_proj.weight 0.6870967741935483\n",
      "model.layers.23.self_attn.k_proj.weight 0.6870967741935483\n",
      "model.layers.23.self_attn.v_proj.weight 0.6870967741935483\n",
      "model.layers.23.self_attn.o_proj.weight 0.6870967741935483\n",
      "model.layers.23.post_attention_layernorm.weight 0.5\n",
      "model.layers.23.mlp.up_proj.weight 0.31290322580645163\n",
      "model.layers.23.mlp.gate_proj.weight 0.31290322580645163\n",
      "model.layers.23.mlp.down_proj.weight 0.31290322580645163\n",
      "------------------------------------------------------------\n",
      "model.layers.24.input_layernorm.weight 0.5\n",
      "model.layers.24.self_attn.q_proj.weight 0.7290322580645161\n",
      "model.layers.24.self_attn.k_proj.weight 0.7290322580645161\n",
      "model.layers.24.self_attn.v_proj.weight 0.7290322580645161\n",
      "model.layers.24.self_attn.o_proj.weight 0.7290322580645161\n",
      "model.layers.24.post_attention_layernorm.weight 0.5\n",
      "model.layers.24.mlp.up_proj.weight 0.2709677419354839\n",
      "model.layers.24.mlp.gate_proj.weight 0.2709677419354839\n",
      "model.layers.24.mlp.down_proj.weight 0.2709677419354839\n",
      "------------------------------------------------------------\n",
      "model.layers.25.input_layernorm.weight 0.5\n",
      "model.layers.25.self_attn.q_proj.weight 0.7677419354838708\n",
      "model.layers.25.self_attn.k_proj.weight 0.7677419354838708\n",
      "model.layers.25.self_attn.v_proj.weight 0.7677419354838708\n",
      "model.layers.25.self_attn.o_proj.weight 0.7677419354838708\n",
      "model.layers.25.post_attention_layernorm.weight 0.5\n",
      "model.layers.25.mlp.up_proj.weight 0.23225806451612907\n",
      "model.layers.25.mlp.gate_proj.weight 0.23225806451612907\n",
      "model.layers.25.mlp.down_proj.weight 0.23225806451612907\n",
      "------------------------------------------------------------\n",
      "model.layers.26.input_layernorm.weight 0.5\n",
      "model.layers.26.self_attn.q_proj.weight 0.8064516129032258\n",
      "model.layers.26.self_attn.k_proj.weight 0.8064516129032258\n",
      "model.layers.26.self_attn.v_proj.weight 0.8064516129032258\n",
      "model.layers.26.self_attn.o_proj.weight 0.8064516129032258\n",
      "model.layers.26.post_attention_layernorm.weight 0.5\n",
      "model.layers.26.mlp.up_proj.weight 0.19354838709677416\n",
      "model.layers.26.mlp.gate_proj.weight 0.19354838709677416\n",
      "model.layers.26.mlp.down_proj.weight 0.19354838709677416\n",
      "------------------------------------------------------------\n",
      "model.layers.27.input_layernorm.weight 0.5\n",
      "model.layers.27.self_attn.q_proj.weight 0.8451612903225807\n",
      "model.layers.27.self_attn.k_proj.weight 0.8451612903225807\n",
      "model.layers.27.self_attn.v_proj.weight 0.8451612903225807\n",
      "model.layers.27.self_attn.o_proj.weight 0.8451612903225807\n",
      "model.layers.27.post_attention_layernorm.weight 0.5\n",
      "model.layers.27.mlp.up_proj.weight 0.15483870967741933\n",
      "model.layers.27.mlp.gate_proj.weight 0.15483870967741933\n",
      "model.layers.27.mlp.down_proj.weight 0.15483870967741933\n",
      "------------------------------------------------------------\n",
      "model.layers.28.input_layernorm.weight 0.5\n",
      "model.layers.28.self_attn.q_proj.weight 0.8838709677419354\n",
      "model.layers.28.self_attn.k_proj.weight 0.8838709677419354\n",
      "model.layers.28.self_attn.v_proj.weight 0.8838709677419354\n",
      "model.layers.28.self_attn.o_proj.weight 0.8838709677419354\n",
      "model.layers.28.post_attention_layernorm.weight 0.5\n",
      "model.layers.28.mlp.up_proj.weight 0.11612903225806454\n",
      "model.layers.28.mlp.gate_proj.weight 0.11612903225806454\n",
      "model.layers.28.mlp.down_proj.weight 0.11612903225806454\n",
      "------------------------------------------------------------\n",
      "model.layers.29.input_layernorm.weight 0.5\n",
      "model.layers.29.self_attn.q_proj.weight 0.9225806451612902\n",
      "model.layers.29.self_attn.k_proj.weight 0.9225806451612902\n",
      "model.layers.29.self_attn.v_proj.weight 0.9225806451612902\n",
      "model.layers.29.self_attn.o_proj.weight 0.9225806451612902\n",
      "model.layers.29.post_attention_layernorm.weight 0.5\n",
      "model.layers.29.mlp.up_proj.weight 0.07741935483870974\n",
      "model.layers.29.mlp.gate_proj.weight 0.07741935483870974\n",
      "model.layers.29.mlp.down_proj.weight 0.07741935483870974\n",
      "------------------------------------------------------------\n",
      "model.layers.30.input_layernorm.weight 0.5\n",
      "model.layers.30.self_attn.q_proj.weight 0.9612903225806452\n",
      "model.layers.30.self_attn.k_proj.weight 0.9612903225806452\n",
      "model.layers.30.self_attn.v_proj.weight 0.9612903225806452\n",
      "model.layers.30.self_attn.o_proj.weight 0.9612903225806452\n",
      "model.layers.30.post_attention_layernorm.weight 0.5\n",
      "model.layers.30.mlp.up_proj.weight 0.038709677419354806\n",
      "model.layers.30.mlp.gate_proj.weight 0.038709677419354806\n",
      "model.layers.30.mlp.down_proj.weight 0.038709677419354806\n",
      "------------------------------------------------------------\n",
      "model.layers.31.input_layernorm.weight 0.5\n",
      "model.layers.31.self_attn.q_proj.weight 1.0\n",
      "model.layers.31.self_attn.k_proj.weight 1.0\n",
      "model.layers.31.self_attn.v_proj.weight 1.0\n",
      "model.layers.31.self_attn.o_proj.weight 1.0\n",
      "model.layers.31.post_attention_layernorm.weight 0.5\n",
      "model.layers.31.mlp.up_proj.weight 0.0\n",
      "model.layers.31.mlp.gate_proj.weight 0.0\n",
      "model.layers.31.mlp.down_proj.weight 0.0\n",
      "------------------------------------------------------------\n",
      "model.norm.weight 0.5\n",
      "------------------------------------------------------------\n",
      "lm_head.weight 0.5\n"
     ]
    }
   ],
   "source": [
    "name = \"\"\n",
    "for target in targets[:-1]:\n",
    "\tnew_name = target.tensor_name.split(\".\")[:3]\n",
    "\tt_rev = target.tensor_task.t\n",
    "\tif new_name != name:\n",
    "\t\tprint(\"---\" * 20)\n",
    "\t\tname = new_name\n",
    "\tprint(target.tensor_name, t_rev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
