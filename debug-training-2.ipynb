{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566a4231-91b2-41a4-a1ac-d87d63078a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union, Mapping\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    default_data_collator,\n",
    "    is_torch_xla_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast\n",
    ")\n",
    "\n",
    "from efficient_masks import (\n",
    "    MergerConfig,\n",
    "    Merger,\n",
    "    init_masks,\n",
    "    set_masks\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    generate, \n",
    "    get_hidden_states, \n",
    "    get_logits,\n",
    "    free_memory\n",
    ")\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7a695f-0215-47a4-9da7-3150133eefbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite_train = load_dataset(\"HuggingFaceTB/smoltalk\", \"smol-rewrite\", split=\"train\")\n",
    "# summarize_train = load_dataset(\"HuggingFaceTB/smoltalk\", \"smol-summarize\", split=\"train\")\n",
    "# rewrite_train = rewrite_train.add_column(name=\"data_source\", column=[\"A\" for _ in rewrite_train])\n",
    "# summarize_train = summarize_train.add_column(name=\"data_source\", column=[\"B\" for _ in summarize_train])\n",
    "\n",
    "# train_dataset = datasets.concatenate_datasets([rewrite_train, summarize_train])\n",
    "# train_mini = train_dataset.shuffle().select(range(5000))\n",
    "# train_mini.to_json(\"train_mini.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b02ec5a-3c6a-41c0-9a44-1aa40d640a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mini = load_dataset(\"json\", data_files=[\"train_mini.jsonl\"], split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6209d994-0d0a-4b3e-8386-053509d7bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n",
    "    \"\"\"\n",
    "    Pads without triggering the warning about how using the pad function is sub-optimal when using a fast tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid errors when using Feature extractors\n",
    "    if not hasattr(tokenizer, \"deprecation_warnings\"):\n",
    "        return tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "\n",
    "    # Save the state of the warning, then disable it\n",
    "    warning_state = tokenizer.deprecation_warnings.get(\"Asking-to-pad-a-fast-tokenizer\", False)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    try:\n",
    "        padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "    finally:\n",
    "        # Restore the state of the warning.\n",
    "        tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n",
    "\n",
    "    return padded\n",
    "\n",
    "@dataclass\n",
    "class MergerDataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        \"\"\"\n",
    "        copied from DataCollatorForLanguageModeling\n",
    "        examples: List[Union[List[int], Any, Dict[str, Any]]]\n",
    "        \"\"\"\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if not isinstance(examples[0], Mapping):\n",
    "            raise ValueError(\"Data collator only processes list of dictionaries.\")\n",
    "\n",
    "        inputs_ids = []\n",
    "        for i in range(len(examples)):\n",
    "            _ = examples[i].pop(\"attention_mask\")\n",
    "            inputs_ids.append(\n",
    "                {\"input_ids\": examples[i].pop(\"input_ids\")}\n",
    "            )\n",
    "            \n",
    "        batch = pad_without_fast_tokenizer_warning(\n",
    "            self.tokenizer, inputs_ids, return_tensors=\"pt\", \n",
    "            pad_to_multiple_of=self.pad_to_multiple_of\n",
    "        )\n",
    "\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        if self.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        \n",
    "        for key in examples[0]:\n",
    "            if key in batch:\n",
    "                raise ValueError(f\"`{key}` feature is collated. Overriding it with its initial values is prohibitted.\")\n",
    "            else:\n",
    "                batch[key] = [x[key] for x in examples]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdaead7-8100-47f9-8ecb-25b011f5b593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"constrain_mode\": \"01\",\n",
       "  \"mode\": \"vector_input\",\n",
       "  \"model_paths\": [\n",
       "    \"nguyenthanhdo/llama32_smol_rewrite_50k\",\n",
       "    \"nguyenthanhdo/llama32_smol_summarize_50k\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"nguyenthanhdo/llama32_smol_rewrite_50k\",\n",
    "        \"nguyenthanhdo/llama32_smol_summarize_50k\",\n",
    "        # \"/workspace/HUB_LLM/Llama-3.2-3B-Instruct\",\n",
    "    ],\n",
    "    mode = \"vector_input\",\n",
    "    # mode = \"scalar\",\n",
    "    constrain_mode = \"01\"\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b840ebfd-3297-4e44-b6b6-7b0f2efac472",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = MergerDataCollator(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669f4f9c-289a-479f-ac08-ebc00553877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    templated = tokenizer.apply_chat_template(\n",
    "        element[\"messages\"], tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        templated,\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        add_special_tokens=False,\n",
    "        # return_tensors=\"pt\"\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7addc5-dffc-43e0-a232-b30732d28ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_mini = train_mini.map(tokenize, remove_columns=[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae261bfa-7067-4a57-ba63-c13ca96f9147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['data_source', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2b3cbf-6aa1-4d6f-bf57-1e947d003f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b5b8d1cee24d339e1c7ce918bb33f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60daae3673148cf8c3900ded880d572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|█▎                                                                                                                                                                             | 2/255 [00:12<27:38,  6.56s/it]2024-12-30 10:04:46,129 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:50<00:00,  5.04it/s]\n",
      "2024-12-30 10:05:23,898 - INFO - Initial GPU memory allocated: 0.00 GB\n",
      "2024-12-30 10:05:24,168 - INFO - Final GPU memory allocated: 0.00 GB\n",
      "2024-12-30 10:05:24,168 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "merger = Merger(merge_config)\n",
    "merger.__post_init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89659e8-4aaf-4f75-96e5-566102310733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e8b6a3d-906e-48aa-ba7d-1f1ad8946a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger = merger.to(device=\"cuda:0\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d19b15-1448-4531-8bee-3364419f2c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up masks: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:00<00:00, 12809.87it/s]\n"
     ]
    }
   ],
   "source": [
    "set_masks(merger.merger, strategy=\"uniform\", factors=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7f365f0-8f51-48c6-b88d-2d7cdd600c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_torch(s_logits, t_logits, idx_masks=None, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Computes KL(s || t) with temperature scaling.\n",
    "    \n",
    "    Args:\n",
    "        s_logits, t_logits: (batch_size, seq_len, vocab)\n",
    "        temperature: float\n",
    "        indices: effective indices that exclude pad tokens.\n",
    "\n",
    "    Returns:\n",
    "        KL divergence that \n",
    "        sum over vocab, then divided by batch_size * seq_len.\n",
    "        Also do not take into account pad tokens.\n",
    "\n",
    "    Note:  This is only for reference. It is unused.\n",
    "    \"\"\"\n",
    "    kl_fct = nn.KLDivLoss(reduction=\"none\")\n",
    "    diff = (\n",
    "        kl_fct(\n",
    "            F.log_softmax(t_logits / temperature, dim=-1),\n",
    "            F.softmax(s_logits / temperature, dim=-1)\n",
    "        )\n",
    "        * (temperature) ** 2\n",
    "    )\n",
    "    loss = diff.sum(-1).mean()\n",
    "    # loss = diff.sum(-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9669485d-857f-45e7-846d-2b8bf6c2f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 10:05:28,317 - INFO - Initial GPU memory allocated: 14.31 GB\n",
      "2024-12-30 10:05:28,671 - INFO - Final GPU memory allocated: 14.31 GB\n",
      "2024-12-30 10:05:28,672 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    logger.info(\"CUDA is not available. No GPU memory to free.\")\n",
    "    \n",
    "initial_memory = torch.cuda.memory_allocated()\n",
    "logger.info(f\"Initial GPU memory allocated: {initial_memory / 1024**3:.2f} GB\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "final_memory = torch.cuda.memory_allocated()\n",
    "logger.info(f\"Final GPU memory allocated: {final_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "freed_memory = initial_memory - final_memory\n",
    "logger.info(f\"Freed GPU memory: {freed_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4a86d-b536-417c-98ad-713a2d076ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Trainer with Custom compute_loss ---\n",
    "def get_entropy_weights(logits_a, logits_b, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Calculates entropy-based weights for merging two sets of logits.\n",
    "\n",
    "    This function efficiently computes the weights for logits_a and logits_b\n",
    "    based on their respective entropies. It combines the functionality of\n",
    "    calculating entropy, normalizing weights, and handling potential\n",
    "    division-by-zero issues.\n",
    "\n",
    "    Args:\n",
    "        logits_a: A PyTorch tensor representing the first set of logits.\n",
    "        logits_b: A PyTorch tensor representing the second set of logits.\n",
    "        epsilon: A small value to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two tensors: (weight_a, weight_b), representing the\n",
    "        normalized entropy-based weights for logits_a and logits_b, respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probs_a = F.softmax(logits_a, dim=-1)\n",
    "    probs_b = F.softmax(logits_b, dim=-1)\n",
    "\n",
    "    # Calculate entropies with epsilon for numerical stability\n",
    "    entropy_a = -(probs_a * probs_a.log()).sum(dim=-1, keepdim=True)\n",
    "    entropy_b = -(probs_b * probs_b.log()).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # Calculate inverse entropies (weights)\n",
    "    inv_entropy_a = 1.0 / (entropy_a + epsilon)\n",
    "    inv_entropy_b = 1.0 / (entropy_b + epsilon)\n",
    "\n",
    "    # Normalize weights\n",
    "    total_inv_entropy = inv_entropy_a + inv_entropy_b\n",
    "    weight_a = inv_entropy_a / (total_inv_entropy + epsilon)  \n",
    "    weight_b = inv_entropy_b / (total_inv_entropy + epsilon) \n",
    "\n",
    "    return weight_a, weight_b\n",
    "    \n",
    "def compute_logits_target(logits_components):\n",
    "    assert len(logits_components) == 2\n",
    "    logits_a, logits_b = logits_components\n",
    "    weight_a, weight_b = get_entropy_weights(logits_a, logits_b)\n",
    "\n",
    "    logits_target = weight_a * logits_a + weight_b * logits_b\n",
    "\n",
    "    return logits_target\n",
    "    \n",
    "class Mergerrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        ## Read inputs. Compute a simple mask `effective_idxs`\n",
    "        ## to exclude non-trainable tokens (e.g., PAD tokens).\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        effective_idxs = labels.clone()\n",
    "        effective_idxs[effective_idxs != -100] = 1.0\n",
    "        effective_idxs[effective_idxs == -100] = 0.0\n",
    "\n",
    "        ## Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits_merged = outputs[\"merger_outputs\"].logits\n",
    "        logits_components = [x.logits for x in outputs[\"components_outputs\"]]\n",
    "\n",
    "        ## Compute targt logits\n",
    "        logits_target = compute_logits_target(logits_components)\n",
    "\n",
    "        ## Compute KL divergence\n",
    "        temperature = 1.0\n",
    "        kl_fct = nn.KLDivLoss(reduction=\"none\")\n",
    "        diff = (\n",
    "            kl_fct(\n",
    "                F.log_softmax(logits_target / temperature, dim=-1),\n",
    "                F.softmax(logits_merged / temperature, dim=-1)\n",
    "            )\n",
    "            * (temperature) ** 2\n",
    "        )\n",
    "        \n",
    "        ### Exclude non-trainable tokens from taking loss\n",
    "        loss = (diff * effective_idxs).sum(dim=-1)\n",
    "        loss = (loss / effective_idxs.sum(dim=-1)).mean()\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329d3c1-5296-4224-bb94-a8dcbdcb0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    model_name: str = \"...\"  # You can replace this with any causal language model from HuggingFace\n",
    "    dataset_name: str = \"...\"  # Replace with your dataset name (e.g., \"your_username/your_dataset\")\n",
    "    train_split: str = \"train\"  # e.g., \"train[:80%]\" for an 80/20 train/validation split\n",
    "    validation_split: str = None  # e.g., \"train[80%:]\"\n",
    "    output_dir: str = \"./trained_masks\"\n",
    "    per_device_train_batch_size: int = 8\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    learning_rate: float = 2e-5\n",
    "    num_train_epochs: int = 3\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 500\n",
    "    logging_steps: int = 100\n",
    "    logging_dir: str = \"./trained_masks/logs\"\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    report_to: str = \"tensorboard\"\n",
    "    remove_unused_columns: bool = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.output_dir,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    learning_rate=args.learning_rate,\n",
    "    num_train_epochs=args.num_train_epochs,\n",
    "    save_steps=args.save_steps,\n",
    "    evaluation_strategy=args.evaluation_strategy if args.validation_split else \"no\",\n",
    "    eval_steps=args.eval_steps if args.validation_split else None,\n",
    "    logging_steps=args.logging_steps,\n",
    "    logging_dir=args.logging_dir,\n",
    "    report_to=args.report_to,  # Enable TensorBoard logging\n",
    "    remove_unused_columns=args.remove_unused_columns\n",
    ")\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator = MergerDataCollator(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "trainer = Mergerrainer(\n",
    "    model=merger,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_mini,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c98648-1943-441a-bd87-f53c6e989b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train the Model ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a68a28-1cee-435a-b6d8-26649bf97c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the Model and Tokenizer ---\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(args.output_dir)\n",
    "print(f\"Model and tokenizer saved to {args.output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
