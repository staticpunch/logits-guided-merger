{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566a4231-91b2-41a4-a1ac-d87d63078a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    default_data_collator,\n",
    "    is_torch_xla_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast\n",
    ")\n",
    "\n",
    "from constrained_masks import (\n",
    "    MergerConfig,\n",
    "    Merger,\n",
    "    init_masks,\n",
    "    set_masks\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    generate, \n",
    "    get_hidden_states, \n",
    "    get_logits,\n",
    "    free_memory\n",
    ")\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8fe09e-0549-4dd1-bd9e-7f770c2e43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger_forward(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6209d994-0d0a-4b3e-8386-053509d7bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n",
    "    \"\"\"\n",
    "    Pads without triggering the warning about how using the pad function is sub-optimal when using a fast tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid errors when using Feature extractors\n",
    "    if not hasattr(tokenizer, \"deprecation_warnings\"):\n",
    "        return tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "\n",
    "    # Save the state of the warning, then disable it\n",
    "    warning_state = tokenizer.deprecation_warnings.get(\"Asking-to-pad-a-fast-tokenizer\", False)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    try:\n",
    "        padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "    finally:\n",
    "        # Restore the state of the warning.\n",
    "        tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n",
    "\n",
    "    return padded\n",
    "\n",
    "@dataclass\n",
    "class MergerDataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        \"\"\"\n",
    "        copied from DataCollatorForLanguageModeling\n",
    "        examples: List[Union[List[int], Any, Dict[str, Any]]]\n",
    "        \"\"\"\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        assert isinstance(examples[0], Mapping)\n",
    "        batch = pad_without_fast_tokenizer_warning(\n",
    "            self.tokenizer, examples, return_tensors=\"pt\", \n",
    "            pad_to_multiple_of=self.pad_to_multiple_of\n",
    "        )\n",
    "\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        if self.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdaead7-8100-47f9-8ecb-25b011f5b593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"constrain_mode\": \"01\",\n",
       "  \"mode\": \"vector_input\",\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/dont15/models/llama32_smol_rewrite_50k/\",\n",
       "    \"/workspace/dont15/models/llama32_smol_summarize_50k/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.47.1\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/dont15/models/llama32_smol_rewrite_50k/\",\n",
    "        \"/workspace/dont15/models/llama32_smol_summarize_50k/\",\n",
    "        # \"/workspace/HUB_LLM/Llama-3.2-3B-Instruct\",\n",
    "    ],\n",
    "    mode = \"vector_input\",\n",
    "    # mode = \"scalar\",\n",
    "    constrain_mode = \"01\"\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b840ebfd-3297-4e44-b6b6-7b0f2efac472",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = MergerDataCollator(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52b4f81-49b4-4524-86b1-c44476136d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_train = load_dataset(\"HuggingFaceTB/smoltalk\", \"smol-rewrite\", split=\"train\")\n",
    "summarize_train = load_dataset(\"HuggingFaceTB/smoltalk\", \"smol-summarize\", split=\"train\")\n",
    "rewrite_train = rewrite_train.add_column(name=\"data_source\", column=[\"A\" for _ in rewrite_train])\n",
    "summarize_train = summarize_train.add_column(name=\"data_source\", column=[\"B\" for _ in summarize_train])\n",
    "\n",
    "train_dataset = datasets.concatenate_datasets([rewrite_train, summarize_train])\n",
    "train_mini = train_dataset.shuffle().select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "669f4f9c-289a-479f-ac08-ebc00553877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    templated = tokenizer.apply_chat_template(\n",
    "        element[\"messages\"], tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        templated,\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        add_special_tokens=False\n",
    "        # return_tensors=\"pt\"\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7addc5-dffc-43e0-a232-b30732d28ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73caa399d7b24dc8927241af00777c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_mini = train_mini.map(tokenize, remove_columns=[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7334b006-de24-4696-9b84-408273e27d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a2cee6a-460f-4497-83bd-4c163e00a153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e31f0c77-0d4d-48ab-a40a-4b1dffcfe0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2b3cbf-6aa1-4d6f-bf57-1e947d003f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec332d04587f4269954287f0af0da611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac4b62cb9b54a7baa4093c117d48b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 04:02:01,658 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [01:23<00:00,  3.05it/s]\n",
      "2024-12-27 04:03:12,665 - INFO - Initial GPU memory allocated: 0.00 GB\n",
      "2024-12-27 04:03:13,002 - INFO - Final GPU memory allocated: 0.00 GB\n",
      "2024-12-27 04:03:13,003 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "merger = Merger(merge_config)\n",
    "merger.__post_init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8b6a3d-906e-48aa-ba7d-1f1ad8946a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger = merger.to(device=\"cuda:4\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67d19b15-1448-4531-8bee-3364419f2c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up masks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:00<00:00, 7234.25it/s]\n"
     ]
    }
   ],
   "source": [
    "set_masks(merger.merger, strategy=\"uniform\", factors=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25847ac6-a9f8-4e85-b97b-5f25113f9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = merger_forward(summarize_text, merger, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebffd9c9-645a-4ce9-ba48-81de0ab3e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs[\"merger_outputs\"].logits[..., :-1, :].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d71a98f0-fccf-424a-b4e0-83aa2128ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs[\"merger_outputs\"].logits == outputs[\"component_outputs\"][0].logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c430c-2a44-497b-97d3-be0e4c0b27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "def entropy(logits):\n",
    "    probs = F.softmax(logits, -1)\n",
    "    return (probs * probs.log()).sum(-1, keepdim=True).neg()\n",
    "\n",
    "\n",
    "def merge_op(logits_a, logits_b, weight_a, weight_b):\n",
    "    # Linear\n",
    "    return weight_a * logits_a + weight_b * logits_b\n",
    "\n",
    "\n",
    "def get_entropy_weights(logits_a, logits_b):\n",
    "    weight_a = 1 / entropy(logits_a)\n",
    "    weight_b = 1 / entropy(logits_b)\n",
    "\n",
    "    den = (weight_a + weight_b)\n",
    "    \n",
    "    weight_a = weight_a / den\n",
    "    weight_b = weight_b / den\n",
    "    \n",
    "    return weight_a, weight_b\n",
    "    \n",
    "\n",
    "def compute_kl_loss(logits_merged, logits_components):\n",
    "    assert len(logits_components) == 2\n",
    "    logits_a, logits_b = logits_components\n",
    "    weight_a, weight_b = get_entropy_weights(logits_a, logits_b)\n",
    "\n",
    "    logits_target = merge_op(logits_a, logits_b, weight_a, weight_b)\n",
    "\n",
    "    probs_merged = F.softmax(logits_merged, -1)\n",
    "\n",
    "    d = probs_merged.log() - F.log_softmax(logits_target, -1)\n",
    "\n",
    "    ## TODO: need to take sum only on not -100 tokens.\n",
    "    return (probs_merged * d).sum(-1).mean()\n",
    "\n",
    "\n",
    "def compute_clm_loss(model, inputs, return_outputs=False):\n",
    "    \"\"\"\n",
    "    Custom compute_loss function for Causal Language Modeling.\n",
    "\n",
    "    Args:\n",
    "        model: The model to compute the loss for.\n",
    "        inputs: A dictionary of inputs as produced by the `collate_fn`.\n",
    "        return_outputs: Whether or not to return the model outputs in addition to the loss.\n",
    "\n",
    "    Returns:\n",
    "        The loss and the model outputs (if `return_outputs=True`).\n",
    "    \"\"\"\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Shift logits and labels for next token prediction\n",
    "    # We shift the logits to the left and the labels to the right to align them for loss computation\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    # Flatten the tokens\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- Custom Trainer with Custom compute_loss ---\n",
    "class Mergerrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # clm_loss = compute_clm_loss(model, inputs, return_outputs)\n",
    "        outputs = model(**inputs)\n",
    "        logits_merged = outputs[\"merger_outputs\"].logits\n",
    "        logits_components = [x.logits for x in outputs[\"components_outputs\"]]\n",
    "\n",
    "        kl_loss = compute_kl_loss(logits_merged, logits_components)\n",
    "\n",
    "        loss = kl_loss\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329d3c1-5296-4224-bb94-a8dcbdcb0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = \"...\"  # You can replace this with any causal language model from HuggingFace\n",
    "DATASET_NAME = \"...\"  # Replace with your dataset name (e.g., \"your_username/your_dataset\")\n",
    "TRAIN_SPLIT = \"train\" # e.g., \"train[:80%]\" for an 80/20 train/validation split\n",
    "VALIDATION_SPLIT = \"validation\" # e.g., \"train[80%:]\"\n",
    "OUTPUT_DIR = \"./trained_masks\"\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "SAVE_STEPS = 500\n",
    "EVAL_STEPS = 500\n",
    "LOGGING_STEPS = 100\n",
    "\n",
    "# --- Load Tokenizer and Model ---\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    evaluation_strategy=\"steps\" if VALIDATION_SPLIT else \"no\",\n",
    "    eval_steps=EVAL_STEPS if VALIDATION_SPLIT else None,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    report_to=\"tensorboard\",  # Enable TensorBoard logging\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator = MergerDataCollator(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "trainer = Mergerrainer(\n",
    "    model=merger,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_mini,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c98648-1943-441a-bd87-f53c6e989b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train the Model ---\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a68a28-1cee-435a-b6d8-26649bf97c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the Model and Tokenizer ---\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model and tokenizer saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27614cd-1cb0-47c7-a7e9-25623df6d373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
