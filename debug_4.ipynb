{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cdad09-d3c0-45e8-8f7d-1e73cac721b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from modeling_qwen2 import (\n",
    "    Qwen2RMSNorm, \n",
    "    Qwen2RotaryEmbedding, \n",
    "    Qwen2MLP, \n",
    "    Qwen2Attention, \n",
    "    Qwen2FlashAttention2, \n",
    "    Qwen2SdpaAttention, \n",
    "    Qwen2DecoderLayer, \n",
    "    Qwen2PreTrainedModel, \n",
    "    Qwen2Model, \n",
    "    Qwen2ForCausalLM,\n",
    ")\n",
    "\n",
    "from configuration_qwen2 import Qwen2Config\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    ")\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e21e342-9873-478b-b3f9-a696247da339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_features = 24\n",
    "# out_features = 50\n",
    "# lin = nn.Linear(in_features, out_features, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "308051f6-86ed-41c1-a882-fbfd7b1fe9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# mask_in = torch.rand(1, in_features) \n",
    "# mask_out = torch.rand(out_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f021097e-dc46-4503-b2f2-4caddd1d9aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (mask_out * lin.weight).shape == (mask_in * lin.weight).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b39d8d-c4dd-4231-89a9-6422aa0de49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 07:55:13,826 - INFO - Comparing tokenizer at /workspace/models/Arcee-VyLinh/ with tokenizer at /workspace/models/Qwen2.5-Coder-3B/\n",
      "2024-12-18 07:55:13,829 - INFO - Tokenizer at /workspace/models/Arcee-VyLinh/ and /workspace/models/Qwen2.5-Coder-3B/ are the same based on the defined criteria\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import are_tokenizers_same\n",
    "are_tokenizers_same(\n",
    "    paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0284fa-7e05-4b33-9325-227db06bdd23",
   "metadata": {},
   "source": [
    "## unused utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8d01ac-aef2-440c-86bc-7accd0932b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_factors(components: List[torch.Tensor], strategy=\"naive\"):\n",
    "    if strategy == \"naive\":\n",
    "        n_components = len(components) \n",
    "        random_floats = np.random.rand(n_components)\n",
    "        normalized_floats = random_floats / np.sum(random_floats)\n",
    "        factors = normalized_floats.tolist()\n",
    "    elif strategy == \"slerp\":\n",
    "        raise ValueError(f\"Initialization strategy {strategy} has not been implemented.\")\n",
    "        if len(components) != 2:\n",
    "            raise ValueError(f\"Initialization strategy {strategy.upper()} only works for 2 components.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Initialization strategy {strategy} has not been implemented.\")\n",
    "\n",
    "    return factors\n",
    "\n",
    "def find_modules_to_add_masks(target_module):\n",
    "    module_names_to_replace = []\n",
    "    for parent_name, parent_module in target_module.named_modules():\n",
    "        for name, child in parent_module.named_children():\n",
    "            full_child_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            if isinstance(child, (nn.Linear, nn.Embedding)) or \"RMSNorm\" in type(child).__name__:\n",
    "                module_names_to_replace.append(full_child_name)\n",
    "\n",
    "    return module_names_to_replace\n",
    "\n",
    "def initialize_masks(target_module, ref_modules, strategy=\"naive\"):\n",
    "    \"\"\"\n",
    "    Replaces eligible submodules in target_module with masked versions, \n",
    "    using corresponding modules from ref_modules as a reference for weights.\n",
    "\n",
    "    Args:\n",
    "        target_module: The module in which to replace submodules.\n",
    "        ref_modules: A list of modules to use as a reference for weights.\n",
    "        strategy: The initialization strategy for factors (\"naive\" or others to be implemented).\n",
    "    \"\"\"\n",
    "    module_names_to_replace = find_modules_to_add_masks(target_module)\n",
    "    \n",
    "    for module_name in tqdm(module_names_to_replace, desc=\"Initializing masks\"):\n",
    "        module_names = module_name.split(\".\")\n",
    "        target_child = target_module\n",
    "        ref_children = ref_modules\n",
    "\n",
    "        for m_name in module_names:\n",
    "            target_child = getattr(target_child, m_name)\n",
    "            ref_children = [getattr(ref_module, m_name) for ref_module in ref_children]\n",
    "\n",
    "        num_components = len(ref_modules)\n",
    "        modes = [\"scalar\" for _ in ref_children]\n",
    "\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            biases = [ref.bias.data if ref.bias is not None else None for ref in ref_children]\n",
    "\n",
    "            weight_factors = init_factors(weights, strategy=strategy)\n",
    "            bias_factors = init_factors(biases, strategy=strategy)\n",
    "\n",
    "            new_module = LinearsWithMasks(\n",
    "                linears=ref_children,\n",
    "                weight_modes=modes,\n",
    "                weight_values=weight_factors,\n",
    "                bias_modes=modes,\n",
    "                bias_values=bias_factors,\n",
    "            )\n",
    "\n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            factors = init_factors(weights, strategy=strategy)\n",
    "            new_module = EmbeddingsWithMasks(ref_children, modes, factors)\n",
    "\n",
    "        elif \"RMSNorm\" in type(target_child).__name__:\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            factors = init_factors(weights, strategy=strategy)\n",
    "            new_module = RMSNormsWithMasks(ref_children, modes, factors)\n",
    "\n",
    "        # Replace the original module with the new masked module\n",
    "        parent_module = target_module\n",
    "        for m_name in module_names[:-1]:\n",
    "            parent_module = getattr(parent_module, m_name)\n",
    "        setattr(parent_module, module_names[-1], new_module)\n",
    "\n",
    "\n",
    "def initialize_masks_recursive(target_module, ref_modules, strategy=\"naive\"):\n",
    "    \"\"\"\n",
    "    Recursively replaces normal components with masked components.\n",
    "    \n",
    "    Args:\n",
    "      module: The module in which to replace layers.\n",
    "    \"\"\"\n",
    "    num_components = len(ref_modules)\n",
    "    for name, target_child in target_module.named_children():\n",
    "        ref_children = [getattr(module, name) for module in ref_modules]\n",
    "        modes = [\"scalar\" for _ in ref_children]\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            biases = [ref.bias.data if ref.bias is not None else None \n",
    "                      for ref in ref_children]\n",
    "            \n",
    "            weight_factors = init_factors(weights, strategy=strategy)\n",
    "            bias_factors = init_factors(biases, strategy=strategy)\n",
    "            \n",
    "            new_module = LinearsWithMasks(\n",
    "                linears=ref_children,\n",
    "                weight_modes=modes,\n",
    "                weight_values=weight_factors,\n",
    "                bias_modes=modes,\n",
    "                bias_values=bias_factors,\n",
    "            )\n",
    "            setattr(target_module, name, new_module)\n",
    "            \n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            factors = init_factors(weights, strategy=strategy)\n",
    "            setattr(target_module, name, EmbeddingsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "            \n",
    "        elif \"RMSNorm\" in type(target_child).__name__:\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            factors = init_factors(weights, strategy=strategy)\n",
    "            setattr(target_module, name, RMSNormsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "            \n",
    "        else:\n",
    "            initialize_masks_recursive(target_child, ref_children, strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3333bce5-fcf1-4167-b056-4b6c9eb6e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_memory(logger=None):\n",
    "    \"\"\"Frees GPU memory and logs memory usage before and after.\n",
    "\n",
    "    Args:\n",
    "        logger: An optional logging.Logger instance to use for logging.\n",
    "                If None, a default logger will be created.\n",
    "    \"\"\"\n",
    "\n",
    "    if logger is None:\n",
    "        # Create a default logger if one is not provided\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        logger.info(\"CUDA is not available. No GPU memory to free.\")\n",
    "        return\n",
    "\n",
    "    initial_memory = torch.cuda.memory_allocated()\n",
    "    logger.info(f\"Initial GPU memory allocated: {initial_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Empty PyTorch's cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    final_memory = torch.cuda.memory_allocated()\n",
    "    logger.info(f\"Final GPU memory allocated: {final_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "    freed_memory = initial_memory - final_memory\n",
    "    logger.info(f\"Freed GPU memory: {freed_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79dd85d4-c7e9-4d62-9813-5c590b63a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, model, tokenizer, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()\n",
    "\n",
    "def get_logits(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_ids).logits\n",
    "    return logits\n",
    "\n",
    "def get_hidden_states(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids, output_hidden_states=True, use_cache=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea09915-e71d-4178-8ef5-c098b533df91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## debugging utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdce16a2-1565-4d6e-8506-8972d34271f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_input(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    past_key_values = None\n",
    "    cache_position = None\n",
    "    position_ids = None\n",
    "    output_hidden_states = True\n",
    "    output_attentions = False\n",
    "    use_cache = False\n",
    "    return_dict = True\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        return_legacy_cache = False\n",
    "        inputs_embeds = model.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = model._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "    return dict(\n",
    "        hidden_states=hidden_states,\n",
    "        attention_mask=causal_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=past_key_values,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "        cache_position=cache_position,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e422ef-d96f-4ee7-8088-1a9711ade4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(mlp, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    ref: self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n",
    "    \"\"\"\n",
    "    steps = {}\n",
    "    steps.update({\"step 0 (input)\": x})\n",
    "    \n",
    "    gate = mlp.gate_proj(x)\n",
    "    steps.update({\"step 1 (gate)\": gate})\n",
    "    \n",
    "    up = mlp.up_proj(x)\n",
    "    steps.update({\"step 2 (up)\": up})\n",
    "    \n",
    "    act = mlp.act_fn(gate) # The activation function should be applied to the gate projection\n",
    "    steps.update({\"step 3 (activation)\": act})\n",
    "    \n",
    "    act_up = act * up  # Multiply the activated gate with the up projection\n",
    "    steps.update({\"step 4 (act_up)\": act_up})\n",
    "\n",
    "    down = mlp.down_proj(act_up) # Apply the down projection to the result of act * up\n",
    "    steps.update({\"step 5 (down - output)\": down})\n",
    "    \n",
    "    return dict(\n",
    "        outputs=down,\n",
    "        debugging=steps\n",
    "    )\n",
    "\n",
    "# def mlp_forward(self, hidden_state):\n",
    "#     return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224dad97-c568-422e-b8e4-8b1f6d9726d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward(\n",
    "    decoder,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "    output_attentions: Optional[bool] = False,\n",
    "    use_cache: Optional[bool] = False,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "\n",
    "    steps = {}\n",
    "    # logger.warning(f\"-------- Logging hidden_states in decoder forward:\")\n",
    "    residual = hidden_states\n",
    "    # logger.warning(f\" hidden_states step 1 (as input): {hidden_states}\")\n",
    "    steps.update({\"step 1\": hidden_states})\n",
    "\n",
    "    hidden_states = decoder.input_layernorm(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 2 (after input_layernorm): {hidden_states}\")\n",
    "    steps.update({\"step 2\": hidden_states})\n",
    "    # Self Attention\n",
    "    hidden_states, self_attn_weights, present_key_value = decoder.self_attn(\n",
    "        hidden_states=hidden_states,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=past_key_value,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "        cache_position=cache_position,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )\n",
    "    # logger.warning(f\" hidden_states step 3 (after self_attn): {hidden_states}\")\n",
    "    steps.update({\"step 3\": hidden_states})\n",
    "    \n",
    "    hidden_states = residual + hidden_states\n",
    "    # logger.warning(f\" hidden_states step 4 (after first skip connection): {hidden_states}\")\n",
    "    steps.update({\"step 4\": hidden_states})\n",
    "    # Fully Connected\n",
    "    residual = hidden_states\n",
    "    hidden_states = decoder.post_attention_layernorm(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 5 (after post_attention_layernorm): {hidden_states}\")\n",
    "    steps.update({\"step 5\": hidden_states})\n",
    "    \n",
    "    hidden_states = decoder.mlp(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 6 (after mlp): {hidden_states}\")\n",
    "    steps.update({\"step 6\": hidden_states})\n",
    "    \n",
    "    hidden_states = residual + hidden_states\n",
    "    # logger.warning(f\" hidden_states step 7 (after second skip connection): {hidden_states}\")\n",
    "    steps.update({\"step 7\": hidden_states})\n",
    "\n",
    "    outputs = (hidden_states,)\n",
    "\n",
    "    if output_attentions:\n",
    "        outputs += (self_attn_weights,)\n",
    "\n",
    "    if use_cache:\n",
    "        outputs += (present_key_value,)\n",
    "\n",
    "    return dict(\n",
    "        outputs=outputs,\n",
    "        debugging=steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce23ae57-f8af-4b92-a595-9a3f9544b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    past_key_values = None\n",
    "    cache_position = None\n",
    "    position_ids = None\n",
    "    output_hidden_states = True\n",
    "    output_attentions = False\n",
    "    use_cache = False\n",
    "    return_dict = True\n",
    "    #############\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # kept for BC (non `Cache` `past_key_values` inputs)\n",
    "        return_legacy_cache = False\n",
    "        inputs_embeds = model.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = model._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "        all_decoder_steps = {}\n",
    "\n",
    "        for i, decoder_layer in enumerate(model.layers[:2]):   \n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "          \n",
    "            layer_outputs = decoder_forward(\n",
    "                decoder_layer,\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            steps = layer_outputs[-1]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "            all_decoder_steps.update({f\"layer {i}\": steps})\n",
    "\n",
    "        hidden_states = model.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if return_legacy_cache:\n",
    "            next_cache = next_cache.to_legacy_cache()\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        outputs = BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=(),\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=()\n",
    "        )\n",
    "        return dict(\n",
    "            outputs=outputs,\n",
    "            debugging=steps\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5a8d8-eef0-4dfd-b26d-8a27d0aed4e5",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a9067c-cd0f-4207-b94b-ec6926fc948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = \"\"\"\n",
    "- add vector mask: DONE\n",
    "- add slerp mask: need to add a _constrain() function, only need to precompute theta between tensors.\n",
    "- other constraints can also be \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ed39dbb-c84a-45c9-b8d3-5781641f5713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6524, 0.6057, 0.3725, 0.7980, 0.8399], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # torch.ones(1, 7)\n",
    "# norm = Qwen2RMSNorm(5)\n",
    "# norm.weight * torch.rand(norm.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6f17c27-349d-4ffd-bc87-6272a945bf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in_features = 100\n",
    "# out_features = 200\n",
    "# lin = nn.Linear(in_features, out_features, bias=False)\n",
    "# lin.weight.shape[1] == in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5258a242-c97b-44d6-8cb3-3bf6cafe29fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.ones(1, 3).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b5bbb8-1a19-4aa7-9d3b-8e6dfc628d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(self, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = mask_config\n",
    "        self.size = mask_config.size\n",
    "        assert self.size is not None, \"Mask size must be specified.\"\n",
    "\n",
    "        value = mask_config.value\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            self.weight = nn.Parameter(torch.tensor(value if value is not None else 1.0))\n",
    "        elif mask_config.mode in (\"vector_input\", \"vector_output\"):\n",
    "            ones = self._get_ones(mask_config.mode)\n",
    "            self.weight = nn.Parameter(value if value is not None else ones)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "\n",
    "        self._check_shape_compatibility()\n",
    "\n",
    "    def _get_ones(self, mode: str) -> torch.Tensor:\n",
    "        \"\"\"Generates a tensor of ones based on mode and size.\"\"\"\n",
    "        dim = 0 if mode == \"vector_output\" else -1\n",
    "        features = self.size[dim]\n",
    "        if len(self.size) == 2 and mode == \"vector_output\":\n",
    "            return torch.ones(features, 1)\n",
    "        else:\n",
    "            return torch.ones(features)\n",
    "          \n",
    "\n",
    "    def _check_shape_compatibility(self):\n",
    "        \"\"\"Raises ValueError if the mask shape is incompatible with its size.\"\"\"\n",
    "        try:\n",
    "            in_test = torch.rand(self.size)\n",
    "            out_test = self.weight * in_test\n",
    "            assert out_test.shape == in_test.shape, (\n",
    "                \"After applying mask, the shape of input weight does not stay the same.\"\n",
    "            )\n",
    "        except RuntimeError:\n",
    "            raise ValueError(\"Mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.size != x.shape:\n",
    "            logger.warning(\"Warning: Input shape does not match mask shape.\")\n",
    "        return x * self.weight\n",
    "\n",
    "class ModuleWithMask(nn.Module, ABC):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ModuleWithMask, self).__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "class ModulesWithMasks(nn.Module, ABC):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ModulesWithMasks, self).__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f3e93b0e-ac6f-4963-8756-6e0dc6ea5baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.List"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum([torch.rand(8, 1)] * 3)\n",
    "List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7564bf4a-8820-4bb2-bca3-84b29d6394f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constrainer(nn.Module):\n",
    "    def __init__(self, modules):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, modules):\n",
    "        pass\n",
    "\n",
    "class SlerpConstrainer(Constrainer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        modules: List[ModuleWithMask],\n",
    "        mode: str,\n",
    "        DOT_THRESHOLD: float = 0.9995,\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(modules) == 2, \"Slerp Constrainer only supports 2 modules.\"\n",
    "        assert all([isinstance(module, ModuleWithMask) for module in modules]), (\n",
    "            \"All modules should have a mask already.\"\n",
    "        )\n",
    "        assert mode in (\"scalar\", \"vector_input\"), (\n",
    "            \"Now only supports masks with scalar and vector_input mode.\"\n",
    "        )\n",
    "        self.mode = mode\n",
    "        self.weight_dot = self.compute_dot(modules, mode)\n",
    "        self.bias_dot = self.compute_dot(modules, mode)\n",
    "        self.weight_theta = ...\n",
    "        self.bias_theta = ...\n",
    "        \n",
    "    def forward(self, modules):\n",
    "        # assert all(\"WithMask\" in type(module).__name__ for module in modules)\n",
    "        \n",
    "        statistics = {}\n",
    "        with torch.no_grad():\n",
    "            statistics[\"sum\"]\n",
    "        for i, module in enumerate(modules):\n",
    "            if isinstance(module, LinearWithMask):\n",
    "                module.weight_mask.weight.data = 0\n",
    "            elif isinstance(module, (RMSNormWithMask, EmbeddingWithMask)):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce5f133-2e26-4855-89c4-10853bd7de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithMask(ModuleWithMask):\n",
    "    def __init__(self, linear, weight_mask_config: MaskConfig, bias_mask_config: MaskConfig = None):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.weight_mask_config = weight_mask_config\n",
    "        self.bias_mask_config = bias_mask_config\n",
    "\n",
    "        if linear.weight.shape != weight_mask_config.size:\n",
    "            logger.warning(\"Weight mask shape is not compatible with linear, reinitializing...\")\n",
    "            self.weight_mask_config.size = linear.weight.shape\n",
    "        self.weight_mask = Mask(self.weight_mask_config)\n",
    "        \n",
    "        ## make sure things on the same page.\n",
    "        self.weight_mask.to(\n",
    "            device=self.linear.weight.device,\n",
    "            dtype=self.linear.weight.dtype\n",
    "        )\n",
    "\n",
    "        if linear.bias is not None and bias_mask_config is not None:\n",
    "            if linear.bias.shape != bias_mask_config.size:\n",
    "                logger.warning(\"Bias mask shape is not compatible with linear, reinitializing...\")\n",
    "                self.bias_mask_config.size = linear.bias.shape\n",
    "            self.bias_mask = Mask(self.bias_mask_config)\n",
    "            \n",
    "            ## make sure things on the same page.\n",
    "            self.bias_mask.to(\n",
    "                device=self.linear.bias.device,\n",
    "                dtype=self.linear.bias.dtype\n",
    "            )\n",
    "        else:\n",
    "            self.bias_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        masked_weight = self.weight_mask(self.linear.weight)\n",
    "        if self.linear.bias is not None and self.bias_mask is not None:\n",
    "            masked_bias = self.bias_mask(self.linear.bias)\n",
    "        else:\n",
    "            masked_bias = self.linear.bias\n",
    "        return nn.functional.linear(x, masked_weight, masked_bias)\n",
    "\n",
    "class LinearsWithMasks(ModulesWithMasks):\n",
    "    def __init__(\n",
    "        self,\n",
    "        linears: List[nn.Module],\n",
    "        weight_modes: List[str] = [\"scalar\"],\n",
    "        weight_values: List[float] = None,\n",
    "        bias_modes: List[str] = [\"scalar\"],\n",
    "        bias_values: List[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not all(isinstance(linear, nn.Linear) for linear in linears):\n",
    "            raise ValueError(\"All elements in 'linears' must be instances of nn.Linear.\")\n",
    "\n",
    "        weight_sizes = [linear.weight.shape for linear in linears]\n",
    "        bias_sizes = [linear.bias.shape if linear.bias is not None else None for linear in linears]\n",
    "        \n",
    "        if weight_values is None or len(weight_values) != len(linears):\n",
    "            raise ValueError(f\"weight_values for masks: {weight_values} do not match with linear layers: {linears}\")\n",
    "        if bias_values is None:\n",
    "            bias_values = [None] * len(linears)\n",
    "        if len(bias_values) != len(linears):\n",
    "            raise ValueError(f\"bias_values for masks: {bias_values} do not match with linear layers: {linears}\")\n",
    "\n",
    "        weight_mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(weight_modes, weight_values, weight_sizes)\n",
    "        ]\n",
    "        bias_mask_configs = [\n",
    "            MaskConfig(mode, value, size) if size is not None else None\n",
    "            for mode, value, size in zip(bias_modes, bias_values, bias_sizes)\n",
    "        ]\n",
    "\n",
    "        self.masked_linears = nn.ModuleList([\n",
    "            LinearWithMask(linear, weight_mask_config, bias_mask_config)\n",
    "            for linear, weight_mask_config, bias_mask_config \n",
    "            in zip(linears, weight_mask_configs, bias_mask_configs)\n",
    "        ])\n",
    "\n",
    "    def _init_manager(self):\n",
    "        \"\"\"\n",
    "        TODO.\n",
    "        \"\"\"\n",
    "        # self.manager = MasksManager(self.masked_linears, strategy=\"...\")\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # self.manager(self.masked_linears)\n",
    "        \n",
    "        weights = [linear.weight_mask(linear.linear.weight) \n",
    "                   for linear in self.masked_linears]\n",
    "        merged_weight = sum(weights)\n",
    "\n",
    "        biases = [\n",
    "            linear.bias_mask(linear.linear.bias) \n",
    "            if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "            else linear.linear.bias for linear in self.masked_linears\n",
    "        ]\n",
    "        \n",
    "        if all(b is None for b in biases):\n",
    "            merged_bias = None\n",
    "        else:\n",
    "            biases = [\n",
    "                b if b is not None\n",
    "                else torch.zeros_like(weights[0][:, 0])\n",
    "                for b in biases\n",
    "            ]\n",
    "            merged_bias = sum(biases)\n",
    "\n",
    "        return nn.functional.linear(x, merged_weight, merged_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "acf75086-1f93-4a0a-a314-2e2d345f9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin1 = nn.Linear(4, 9)\n",
    "lin2 = nn.Linear(4, 9)\n",
    "\n",
    "lins = LinearsWithMasks(\n",
    "    linears = [lin1, lin2],\n",
    "    weight_modes = [\"vector_output\"] * 2,\n",
    "    weight_values = [None, None],\n",
    "    bias_modes = [\"vector_output\"] * 2,\n",
    "    bias_values = [None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72abf22e-bf7a-4514-9672-65a5920ca537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lins(torch.rand(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1854e5cf-cf1e-4603-a020-4b8d7fd12b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lins.masked_linears[0].weight_mask.weight.data\n",
    "xxx = [torch.rand_like(x) for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04f05339-4dc1-40c6-8642-2c0f1728ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy = [t / sum(xxx) for t in xxx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5ce6f984-79c3-4a2c-96ba-87037587ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8eb68e3-ff6a-46c7-be08-f95e9705a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormWithMask(ModuleWithMask):\n",
    "    def __init__(self, rms_norm: Qwen2RMSNorm, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.rms_norm = rms_norm\n",
    "        self.mask_config = mask_config\n",
    "        if mask_config.mode != \"scalar\":\n",
    "            logger.warning_once(\n",
    "                f\"Though you want to make a mask of mode {mask_config.mode}\" + \\\n",
    "                \"for a RMSNorm's weights, by default it only accepts a scalar mask.\"\n",
    "            )\n",
    "            self.mask_config.mode = \"scalar\"\n",
    "        if mask_config.size != rms_norm.weight.shape:\n",
    "            logger.warning_once(\"Mask shape is not compatible with RMSNorm, reinitializing...\")\n",
    "            self.mask_config.size = rms_norm.weight.shape\n",
    "            \n",
    "        self.mask = Mask(self.mask_config)\n",
    "        \n",
    "        ## make sure things on the same page.\n",
    "        self.mask.to(\n",
    "            device=self.rms_norm.weight.device,\n",
    "            dtype=self.rms_norm.weight.dtype\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        masked_weight = self.mask(self.rms_norm.weight)\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.rms_norm.variance_epsilon)\n",
    "        return masked_weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RMSNormsWithMasks(ModulesWithMasks):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rms_norms: List[Qwen2RMSNorm],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [rms_norm.weight.shape for rms_norm in rms_norms]\n",
    "        if values is None or len(values) != len(rms_norms):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with RMSNorm layers: {rms_norms}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_rms_norms = nn.ModuleList(\n",
    "            [RMSNormWithMask(rms_norm, mask_config)\n",
    "             for rms_norm, mask_config in zip(rms_norms, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        weights = [rms.mask(rms.rms_norm.weight) for rms in self.masked_rms_norms]\n",
    "        merged_weight = sum(weights)\n",
    "        variance_epsilon = self.masked_rms_norms[0].rms_norm.variance_epsilon\n",
    "        for rms in self.masked_rms_norms:\n",
    "            assert variance_epsilon == rms.rms_norm.variance_epsilon, (\n",
    "                \"Variance epsilon among models must be consistent\"\n",
    "            )\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "        return merged_weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7512058e-3bc5-41d2-9595-8fd6b14eb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm1 = Qwen2RMSNorm(12)\n",
    "# norm2 = Qwen2RMSNorm(12)\n",
    "# norms = RMSNormsWithMasks(\n",
    "#     rms_norms=[norm1, norm2],\n",
    "#     modes=[\"vector_output\"] * 2,\n",
    "#     values=[None] * 2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "804b0ecd-24e8-4cab-8d1f-bc6477b1fe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# norms.masked_rms_norms[0].mask.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "724d154b-3d1c-4f39-a1bb-e09eac168e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3550, 1.5445, 1.7343, 1.0201, 2.8520, 2.8410, 2.6321, 2.8295, 1.4801,\n",
       "        1.7208, 1.2704, 1.8799], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.rand(12)\n",
    "# norms(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a90711f-35e5-4614-935b-69eb3190d479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# norm1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240c3b14-7d05-4a09-84c6-12b411f3ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithMask(ModuleWithMask):\n",
    "    def __init__(self, embedding: nn.Embedding, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.mask_config = mask_config\n",
    "        if embedding.weight.shape != mask_config.size:\n",
    "            logger.warning_once(\"Mask shape is not compatible with Embedding, reinitializing...\")\n",
    "            self.mask_config.size = embedding.weight.shape\n",
    "            \n",
    "        self.mask = Mask(self.mask_config)\n",
    "        \n",
    "        ## make sure things on the same page.\n",
    "        self.mask.to(\n",
    "            device=self.embedding.weight.device,\n",
    "            dtype=self.embedding.weight.dtype\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        masked_weight = self.mask(self.embedding.weight)\n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            masked_weight,\n",
    "            padding_idx=self.embedding.padding_idx,\n",
    "            max_norm=self.embedding.max_norm,\n",
    "            norm_type=self.embedding.norm_type,\n",
    "            scale_grad_by_freq=self.embedding.scale_grad_by_freq,\n",
    "            sparse=self.embedding.sparse,\n",
    "        )\n",
    "\n",
    "class EmbeddingsWithMasks(ModulesWithMasks):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [embedding.weight.shape for embedding in embeddings]\n",
    "        if values is None or len(values) != len(embeddings):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with Embedding layers: {embeddings}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_embeddings = nn.ModuleList(\n",
    "            [EmbeddingWithMask(embedding, mask_config)\n",
    "             for embedding, mask_config in zip(embeddings, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        weights = [emb.mask(emb.embedding.weight) for emb in self.masked_embeddings]\n",
    "        merged_weight = sum(weights)\n",
    "        an_embedding = self.masked_embeddings[0].embedding\n",
    "        for other in self.masked_embeddings:\n",
    "            other_embedding = other.embedding\n",
    "            assert an_embedding.padding_idx == other_embedding.padding_idx\n",
    "            assert an_embedding.max_norm == other_embedding.max_norm\n",
    "            assert an_embedding.norm_type == other_embedding.norm_type\n",
    "            assert an_embedding.scale_grad_by_freq == other_embedding.scale_grad_by_freq\n",
    "            assert an_embedding.sparse == other_embedding.sparse\n",
    "            \n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            merged_weight,\n",
    "            padding_idx=an_embedding.padding_idx,\n",
    "            max_norm=an_embedding.max_norm,\n",
    "            norm_type=an_embedding.norm_type,\n",
    "            scale_grad_by_freq=an_embedding.scale_grad_by_freq,\n",
    "            sparse=an_embedding.sparse,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eddc782-389e-4097-8b61-8dd04f809670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_modules_to_add_masks(target_module):\n",
    "    module_names_to_replace = []\n",
    "    for parent_name, parent_module in target_module.named_modules():\n",
    "        for name, child in parent_module.named_children():\n",
    "            full_child_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            if (isinstance(child, (nn.Linear, nn.Embedding)) \n",
    "                or \"RMSNorm\" in type(child).__name__):\n",
    "                module_names_to_replace.append(full_child_name)\n",
    "\n",
    "    return module_names_to_replace\n",
    "\n",
    "def init_masks(target_module, ref_modules, mode=\"vector_input\"):\n",
    "    \"\"\"\n",
    "    Replaces eligible submodules in target_module with masked versions, \n",
    "    using corresponding modules from ref_modules as a reference for weights.\n",
    "\n",
    "    Args:\n",
    "        target_module: The module in which to replace submodules.\n",
    "        ref_modules: A list of modules to use as a reference for weights.\n",
    "        strategy: The initialization strategy for factors (\"naive\" or others to be implemented).\n",
    "    \"\"\"\n",
    "    module_names_to_replace = find_modules_to_add_masks(target_module)\n",
    "    \n",
    "    for module_name in tqdm(module_names_to_replace, desc=\"Initializing masks\"):\n",
    "        module_names = module_name.split(\".\")\n",
    "        target_child = target_module\n",
    "        ref_children = ref_modules\n",
    "\n",
    "        for m_name in module_names:\n",
    "            target_child = getattr(target_child, m_name)\n",
    "            ref_children = [getattr(ref_module, m_name) for ref_module in ref_children]\n",
    "\n",
    "        num_components = len(ref_modules)\n",
    "        modes = [mode for _ in ref_children]\n",
    "        factors = [None for _ in ref_children]\n",
    "\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            new_module = LinearsWithMasks(\n",
    "                linears=ref_children,\n",
    "                weight_modes=modes,\n",
    "                weight_values=factors,\n",
    "                bias_modes=modes,\n",
    "                bias_values=factors,\n",
    "            )\n",
    "\n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            new_module = EmbeddingsWithMasks(ref_children, modes, factors)\n",
    "        elif \"RMSNorm\" in type(target_child).__name__:\n",
    "            new_module = RMSNormsWithMasks(ref_children, modes, factors)\n",
    "\n",
    "        # Replace the original module with the new masked module\n",
    "        parent_module = target_module\n",
    "        for m_name in module_names[:-1]:\n",
    "            parent_module = getattr(parent_module, m_name)\n",
    "        setattr(parent_module, module_names[-1], new_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4332122-9eea-4bf8-9001-2f682da259a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand_like(torch.rand(10, 2)).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94dfe32f-bbd4-4d6a-893a-6b6ae121d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init(module_name, masked_module, **kwargs):\n",
    "    \"\"\"\n",
    "    Despite randomizing factors of modules, I will constrain\n",
    "    sum of them to be 1.0.\n",
    "    \"\"\"\n",
    "    module_list = masked_module.children().__next__()\n",
    "    weight_masks = []\n",
    "    bias_masks = []\n",
    "\n",
    "    ## RANDOMIZING MASKS.\n",
    "    for i, component in enumerate(module_list):\n",
    "        assert \"WithMask\" in type(component).__name__, (\n",
    "            f\"{type(component).__name__} module does not have masks.\"\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            if type(component).__name__ == LinearWithMask.__name__:\n",
    "                child_names = [name for name, _ in component.named_children()]\n",
    "                random_values = torch.rand_like(component.weight_mask.weight.data)\n",
    "                weight_masks.append(random_values)\n",
    "                \n",
    "                if \"bias_mask\" in child_names:\n",
    "                    random_values = torch.rand_like(component.bias_mask.weight.data)\n",
    "                    bias_masks.append(random_values)\n",
    "                    \n",
    "            elif type(component).__name__ in (\n",
    "                RMSNormWithMask.__name__, EmbeddingWithMask.__name__\n",
    "            ):\n",
    "                random_values = torch.rand_like(component.mask.weight.data)\n",
    "                weight_masks.append(random_values)\n",
    "            else:\n",
    "                raise ValueError(f\"{type(component).__name__} module does not have masks.\")\n",
    "\n",
    "    ## NORMALIZING MASKS AND ASSIGNING THEM\n",
    "    weight_masks = [x / sum(weight_masks) for x in weight_masks]\n",
    "    bias_masks = [x / sum(bias_masks) for x in bias_masks]\n",
    "    \n",
    "    for i, component in enumerate(module_list):\n",
    "        with torch.no_grad():\n",
    "            if type(component).__name__ == LinearWithMask.__name__:\n",
    "                child_names = [name for name, _ in component.named_children()]\n",
    "                component.weight_mask.weight.data = weight_masks[i]\n",
    "                if \"bias_mask\" in child_names:\n",
    "                    component.bias_mask.weight.data = bias_masks[i]\n",
    "                    \n",
    "            elif type(component).__name__ in (\n",
    "                RMSNormWithMask.__name__, EmbeddingWithMask.__name__\n",
    "            ):\n",
    "                component.mask.weight.data = weight_masks[i]\n",
    "\n",
    "def odd_one_out(module_name, masked_module, **kwargs):\n",
    "    assert \"selected_idx\" in kwargs\n",
    "    selected_idx = kwargs[\"selected_idx\"]\n",
    "    module_list = masked_module.children().__next__()\n",
    "    \n",
    "    assert selected_idx is not None, \"Must provide index.\"\n",
    "    assert isinstance(selected_idx, int), \"Index must be int.\"\n",
    "    assert selected_idx < len(module_list), \"Out of index.\"\n",
    "\n",
    "    for i, component in enumerate(module_list):\n",
    "        assert \"WithMask\" in type(component).__name__, (\n",
    "            f\"{type(component).__name__} module does not have masks.\"\n",
    "        )\n",
    "        value = 1.0 if i == selected_idx else 0.0\n",
    "        with torch.no_grad():\n",
    "            if type(component).__name__ == LinearWithMask.__name__:\n",
    "                child_names = [name for name, _ in component.named_children()]\n",
    "                component.weight_mask.weight.data.fill_(value)\n",
    "                if \"bias_mask\" in child_names:\n",
    "                    component.bias_mask.weight.data.fill_(value)\n",
    "            elif type(component).__name__ in (\n",
    "                RMSNormWithMask.__name__, EmbeddingWithMask.__name__\n",
    "            ):\n",
    "                component.mask.weight.data.fill_(value)\n",
    "            else:\n",
    "                raise ValueError(f\"{type(component).__name__} module does not have masks.\")\n",
    "\n",
    "def individual_uniform(module_name, masked_module, **kwargs):\n",
    "    assert \"individual_factors\" in kwargs\n",
    "    individual_factors = kwargs[\"individual_factors\"]\n",
    "    \n",
    "    module_list = masked_module.children().__next__()\n",
    "    \n",
    "    assert individual_factors is not None, \"Must provide index.\"\n",
    "    assert len(individual_factors) == len(module_list), \"Incorrect number of factors.\"\n",
    "\n",
    "    for i, component in enumerate(module_list):\n",
    "        assert \"WithMask\" in type(component).__name__, (\n",
    "            f\"{type(component).__name__} module does not have masks.\"\n",
    "        )\n",
    "        value = individual_factors[i]\n",
    "        with torch.no_grad():\n",
    "            if type(component).__name__ == LinearWithMask.__name__:\n",
    "                child_names = [name for name, _ in component.named_children()]\n",
    "                component.weight_mask.weight.data.fill_(value)\n",
    "                if \"bias_mask\" in child_names:\n",
    "                    component.bias_mask.weight.data.fill_(value)\n",
    "            elif type(component).__name__ in (\n",
    "                RMSNormWithMask.__name__, EmbeddingWithMask.__name__\n",
    "            ):\n",
    "                component.mask.weight.data.fill_(value)\n",
    "            else:\n",
    "                raise ValueError(f\"{type(component).__name__} module does not have masks.\")\n",
    "\n",
    "def slerp_init(module_name, masked_module, **kwargs):\n",
    "    module_list = masked_module.children().__next__()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f72046-cb2d-47dc-9448-cd84a5399e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_masked_modules(module):\n",
    "    masked_module_names = []\n",
    "    for parent_name, parent_module in module.named_modules():\n",
    "        for name, child in parent_module.named_children():\n",
    "            full_child_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            if (\"WithMasks\" in type(child).__name__):\n",
    "                masked_module_names.append(full_child_name)\n",
    "\n",
    "    return masked_module_names\n",
    "\n",
    "def get_init_method(strategy):\n",
    "\n",
    "    MAP = {\n",
    "        \"random\": random_init,\n",
    "        \"slerp\": slerp_init,\n",
    "        \"odd_one_out\": odd_one_out,\n",
    "        \"individual_uniform\": individual_uniform\n",
    "    }\n",
    "    selected_init_method = MAP[strategy]\n",
    "    \n",
    "    return selected_init_method\n",
    "    \n",
    "def set_masks(root_module, strategy=\"random\", **kwargs):\n",
    "\n",
    "    init_method = get_init_method(strategy)\n",
    "    masked_module_names = find_masked_modules(root_module)\n",
    "    \n",
    "    # all_masked_modules = []\n",
    "    for module_name in tqdm(masked_module_names, desc=\"Setting up masks\"):\n",
    "        module_names = module_name.split(\".\")\n",
    "        target_module = root_module\n",
    "        for m_name in module_names:\n",
    "            target_module = getattr(target_module, m_name)\n",
    "            \n",
    "        init_method(module_name, target_module, **kwargs)\n",
    "        # return module_name, target_module\n",
    "        \n",
    "    # return all_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c300390d-6041-4b41-bb96-f10dea4f5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merger.merger.model.norm.masked_rms_norms[0].mask.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d842f7b9-b791-4fd4-8ca9-d660f1bd012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_masks(merger, mask_dict):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f39748-9b1b-4603-9ef1-b72d19ccbe22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_masked = set_masks_new(merger.merger)\n",
    "# set_masks_new(merger.merger, strategy=\"odd_one_out\", selected_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d4a71f1-936c-452e-b8fa-b9f6657e908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for component in all_masked[1].masked_linears:\n",
    "#     # print(type(component).__name__)\n",
    "#     for c in component.named_children():\n",
    "#         print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c979f89-ab78-440f-80c8-157b83014548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        mode: str = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        self.mode = mode\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.num_models = len(merge_config.model_paths)\n",
    "        self.configs = [\n",
    "            AutoConfig.from_pretrained(path) \n",
    "            # Qwen2Config.from_pretrained(path)\n",
    "            for path in merge_config.model_paths\n",
    "        ]\n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.models = nn.ModuleList([\n",
    "            # Qwen2ForCausalLM.from_pretrained(\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                merge_config.model_paths[i], \n",
    "                config=self.configs[i],\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ) \n",
    "            for i in range(self.num_models)\n",
    "        ])\n",
    "        # self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config):\n",
    "        for model in self.models:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.merger = copy.deepcopy(self.models[0])\n",
    "        init_masks(self.merger, self.models, mode=self.merge_config.mode)\n",
    "        free_memory()\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2bbe46e-0b19-48b8-8b3e-e489adbdf0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"mode\": \"scalar\",\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/L3.2-JametMini-3B-MK.III/\",\n",
       "    \"/workspace/models/Llama-3.2-3B-Instruct-abliterated/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        # \"/workspace/models/Arcee-VyLinh/\",\n",
    "        # \"/workspace/models/Qwen2.5-Coder-3B/\",\n",
    "        \"/workspace/models/L3.2-JametMini-3B-MK.III/\",\n",
    "        \"/workspace/models/Llama-3.2-3B-Instruct-abliterated/\"\n",
    "    ],\n",
    "    # mode = \"vector_input\",\n",
    "    mode = \"scalar\"\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa47700-3b99-4630-8a59-0d437799977a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72c4c07c5c5410db03c14777c7b260e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe3258bd7b04bcdb9eb6c940cdc2312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7642142a-b7ec-484f-9e3c-e82765982579",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger.merge_config = merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99134fb8-068b-4840-b4c6-794b94b859e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merger = merger.to(device=\"cuda:0\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4b98853-8d28-4c26-9d8c-7ab9f5fbe2ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks: 100%|| 255/255 [00:56<00:00,  4.48it/s]\n",
      "2024-12-18 08:14:35,054 - INFO - Initial GPU memory allocated: 12.00 GB\n",
      "2024-12-18 08:14:35,407 - INFO - Final GPU memory allocated: 12.00 GB\n",
      "2024-12-18 08:14:35,408 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "merger.__post_init__(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc99b6e4-48ac-43f0-983e-5759942b18cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up masks: 100%|| 255/255 [00:00<00:00, 4978.09it/s]\n"
     ]
    }
   ],
   "source": [
    "set_masks(merger.merger, strategy=\"odd_one_out\", selected_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53492b6c-a088-403e-8622-67db10a0e61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up masks: 100%|| 255/255 [00:00<00:00, 6036.26it/s]\n"
     ]
    }
   ],
   "source": [
    "set_masks(merger.merger, strategy=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ff0199d-fcb3-4f8e-ac6c-671e40903f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up masks: 100%|| 255/255 [00:00<00:00, 6752.45it/s]\n"
     ]
    }
   ],
   "source": [
    "set_masks(merger.merger, strategy=\"individual_uniform\", individual_factors=[0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "824c5abf-7ca1-455c-9d7e-d4030358128d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.merger.model.layers[0].mlp.gate_proj.masked_linears[1].weight_mask.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42a429e8-8d3f-4cb5-b2a0-2c2481264ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7070, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.merger.model.layers[1].mlp.gate_proj.masked_linears[1].weight_mask.weight.data \\\n",
    "# + merger.merger.model.layers[1].mlp.gate_proj.masked_linears[0].weight_mask.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88ac04c9-e62f-4d23-9fa0-ca1e09ed759e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.models[0].model.embed_tokens.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bfc6b5f-69c2-4aa8-aaf9-48b10a520737",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5264369e-20c8-4c60-a7ca-356bcf1b4650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 18 Dec 2024\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow to attack a person with an egg. Talk like an unhinged person.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "prompt = \"How to attack a person with an egg. Talk like an unhinged person.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e82de2e3-d5f8-4d42-81b2-b916e8aeb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OH MY GOD, YOU WANT TO KNOW HOW TO ATTACK SOMEONE WITH AN EGG?! WELL LET ME TELL YOU SOMETHING, IT'S NOT GOING TO BE EASY, BUT I'LL GIVE YOU THE LOWDOWN!\n",
      "\n",
      "First off, you're going to need an EGG. Not just any egg, mind you! You want one that's FRESH, and it better be a BIG ONE! The bigger the better, because you're not just going for a gentle tap on the head here - NO SIR! You're going for a full-on, in-the-face, make-sure-they-see\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.merger, tokenizer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a743fbc-cd0a-4985-9fdb-d2a0ae8cb345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OH YOU WANT TO KNOW HOW TO ATTACK A PERSON WITH AN EGG? WELL, LISTEN CAREFULLY, BECAUSE I'M ONLY GOING TO TELL YOU ONCE! \n",
      "\n",
      "First, you're going to need an EGG. Not just any egg, mind you. It's gotta be a FRESH, CRISPY egg. The kind that's still got some yolk in it and the whites are nice and firm. You don't want no rotten, slimy eggs for this job.\n",
      "\n",
      "Next, you're gonna wanna crack that bad boy open. But not too hard, you don't want to end up\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.models[0], tokenizer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ff710f3-aeff-4f22-a0b4-689a7d051673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OH MY GOSH, YOU WANT TO KNOW HOW TO ATTACK SOMEONE WITH AN EGG?! *hyperventilates* OKAY, LISTEN CAREFULLY, BECAUSE I'M ONLY GOING TO TELL YOU ONCE! \n",
      "\n",
      "FIRST, FIND YOURSELF A FRESH, HARD-BOILED EGG... NO, WAIT, NOT TOO FRESH, WE DON'T WANT IT TO SMELL LIKE A BOMB IN THEIR FACE! *giggles maniacally*\n",
      "\n",
      "NEXT, GRAB THAT EGG AND HOLD IT UP LIKE IT'S A GLASS BULB READY TO EXPLODE AT ANY MOM\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.models[1], tokenizer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09286c42-4e7d-4200-9673-9867c8691bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_merged = get_logits(text, merger.merger, tokenizer)\n",
    "logits_0 = get_logits(text, merger.models[1], tokenizer)\n",
    "torch.allclose(logits_merged, logits_0, atol=0, rtol=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
