{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d17875-ae3d-48b5-b80b-d6067674413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union, Mapping\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    default_data_collator,\n",
    "    is_torch_xla_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast\n",
    ")\n",
    "\n",
    "import efficient_masks\n",
    "import accurate_masks\n",
    "\n",
    "from utils import (\n",
    "    generate, \n",
    "    get_hidden_states, \n",
    "    get_logits,\n",
    "    free_memory\n",
    ")\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22623236-28d5-4aac-b95d-a7905480e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_a = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "path_b = \"unsloth/Llama-3.2-1B\"\n",
    "merge_config_a = accurate_masks.MergerConfig(\n",
    "    model_paths = [path_a, path_b],\n",
    "    mode = \"vector_input\",\n",
    "    constrain_mode = \"01\",\n",
    ")\n",
    "merge_config_e = efficient_masks.MergerConfig(\n",
    "    model_paths = [path_a, path_b],\n",
    "    mode = \"vector_input\",\n",
    "    constrain_mode = \"01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8aa872-d1c6-4aae-921c-42cea626b652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"constrain_mode\": \"01\",\n",
       "  \"mode\": \"vector_input\",\n",
       "  \"model_paths\": [\n",
       "    \"unsloth/Llama-3.2-1B-Instruct\",\n",
       "    \"unsloth/Llama-3.2-1B\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e8680c-e36a-4aa4-af7e-cd52a9d9865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_linear_forward(self, x):\n",
    "    constrained_weight_masks = self.weight_masks_constrainer([m.weight for m in self.weight_masks])\n",
    "    constrained_bias_masks = self.bias_masks_constrainer(\n",
    "        [m.weight if m is not None else None for m in self.bias_masks]\n",
    "    )\n",
    "    masked_biases = [\n",
    "        b_mask * linear.bias if linear.bias is not None and b_mask is not None else linear.bias\n",
    "        for b_mask, linear in zip(constrained_bias_masks, self.linears)\n",
    "    ]\n",
    "    merged_bias = (\n",
    "        sum(b if b is not None else torch.zeros_like(\n",
    "            self.linears[0].weight[:, 0]) for b in masked_biases\n",
    "           ) \n",
    "        if not all(b is None for b in masked_biases) else None\n",
    "    )\n",
    "\n",
    "    logger.info(\"Debugging Linear forward.\")\n",
    "    output = 0.0\n",
    "    for i, linear in enumerate(self.linears):\n",
    "        logger.info(f\"BEFORE\")\n",
    "        logger.info(f\"  linear: device: {linear.weight.device}; dtype: {linear.weight.dtype}\")\n",
    "        logger.info(f\"  input: device: {x.device}; dtype: {x.dtype}\")\n",
    "        masked_input = constrained_weight_masks[i] * x\n",
    "        logger.info(f\"AFTER\")\n",
    "        logger.info(f\"  linear: device: {linear.weight.device}; dtype: {linear.weight.dtype}\")\n",
    "        logger.info(f\"  input: device: {masked_input.device}; dtype: {masked_input.dtype}\")\n",
    "        output = output + nn.functional.linear(masked_input, linear.weight, None)\n",
    "        logger.info(f\"OUTPUT\")\n",
    "        logger.info(f\"  output: device: {output.device}; dtype: {output.dtype}\")\n",
    "    if merged_bias:\n",
    "        output = output + merged_bias\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23556ad-acd1-4584-8658-c6935760444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_emb_forward(self, input_ids):\n",
    "    constrained_masks = self.masks_constrainer([m.weight for m in self.masks])\n",
    "    logger.info(\"Debugging Embedding forward.\")\n",
    "    an_embedding = self.embeddings[0]\n",
    "    out = 0.0\n",
    "    for i, emb in enumerate(self.embeddings):\n",
    "        logger.info(f\"BEFORE\")\n",
    "        logger.info(f\"  emb: device: {emb.weight.device}; dtype: {emb.weight.dtype}\")\n",
    "        logger.info(f\"  input: device: {input_ids.device}; dtype: {input_ids.dtype}\")\n",
    "        mask = constrained_masks[i]\n",
    "        masked_weight = emb.weight * mask\n",
    "        logger.info(f\"  mask: device: {mask.device}; dtype: {mask.dtype}\")\n",
    "        logger.info(f\"  masked_emb: device: {masked_weight.device}; dtype: {masked_weight.dtype}\")\n",
    "        out = out + nn.functional.embedding(\n",
    "            input_ids,\n",
    "            # emb.weight * mask,\n",
    "            masked_weight,\n",
    "            padding_idx=an_embedding.padding_idx,\n",
    "            max_norm=an_embedding.max_norm,\n",
    "            norm_type=an_embedding.norm_type,\n",
    "            scale_grad_by_freq=an_embedding.scale_grad_by_freq,\n",
    "            sparse=an_embedding.sparse,\n",
    "        )\n",
    "        logger.info(f\"AFTER\")\n",
    "        logger.info(f\"  output: device: {out.device}; dtype: {out.dtype}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc55413-f122-45a9-9743-656d77b25bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient_masks.LinearsWithMasks.forward = debug_linear_forward\n",
    "# efficient_masks.EmbeddingsWithMasks.forward = debug_emb_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0ada93-d19d-40e1-92f1-36a8268108de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b28405-016a-491e-b777-ecba79a2c0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|██▍                                                                                                                                                                            | 2/147 [00:16<20:28,  8.47s/it]2025-01-03 11:27:40,772 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 147/147 [00:48<00:00,  3.01it/s]\n",
      "2025-01-03 11:28:12,554 - INFO - Initial GPU memory allocated: 0.00 GB\n",
      "2025-01-03 11:28:12,941 - INFO - Final GPU memory allocated: 0.00 GB\n",
      "2025-01-03 11:28:12,942 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "em = efficient_masks.Merger(merge_config_e)\n",
    "em.__post_init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43559a6-e29d-41bb-8ae7-26224b11bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = em.to(device=\"cuda:0\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c63913d-36ee-441c-8324-653563436bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|██▍                                                                                                                                                                            | 2/147 [00:16<20:23,  8.44s/it]2025-01-03 11:28:35,340 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 147/147 [00:44<00:00,  3.32it/s]\n",
      "2025-01-03 11:29:02,641 - INFO - Initial GPU memory allocated: 4.61 GB\n",
      "2025-01-03 11:29:03,009 - INFO - Final GPU memory allocated: 4.61 GB\n",
      "2025-01-03 11:29:03,013 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "am = accurate_masks.Merger(merge_config_a)\n",
    "am.__post_init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f09dbe-18db-4d0f-9bec-885b2b9aca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = am.to(device=\"cuda:1\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22859463-8211-4b76-83f1-b818bef7d877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"How to attack a person with an egg. Talk like a crazy person.\"\n",
    "logits_merged_a = get_logits(prompt, am.merger, tokenizer)\n",
    "logits_merged_e = get_logits(prompt, em.merger, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0beeb8aa-7507-4b64-9de8-a9f38605f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_merged_a = logits_merged_a.to(logits_merged_e.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9bf0689-4cd2-424e-8787-3968052a3e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 128256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_merged_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e3532a-7a2a-49d7-9020-10c017abd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_a = torch.softmax(logits_merged_a, dim=-1)\n",
    "probs_e = torch.softmax(logits_merged_e, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e324ee19-b699-4f89-bab5-2a45e0053130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([6.4453e-01, 3.0469e-01, 2.5024e-02, 1.1841e-02, 3.3875e-03, 2.0599e-03,\n",
       "        9.7275e-04, 9.7275e-04, 7.5531e-04, 3.5667e-04, 3.5667e-04, 2.7847e-04,\n",
       "        2.1648e-04, 1.9169e-04, 1.6880e-04, 1.6880e-04, 1.4877e-04, 1.4877e-04,\n",
       "        1.3161e-04, 1.3161e-04], device='cuda:0', dtype=torch.bfloat16),\n",
       "indices=tensor([  755,     2,   791, 16309,   475,  3936,    17,    16,    32,    59,\n",
       "         1527,    11,    51,    50,  1687, 13066,  2028,    34,   220,    35],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(probs_a[0, 0, :], k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f73f4b3d-adbc-45b5-9f1a-27c989912039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([6.9531e-01, 2.5586e-01, 2.0996e-02, 1.2756e-02, 4.6997e-03, 2.2125e-03,\n",
       "        1.0452e-03, 1.0452e-03, 8.1635e-04, 6.3324e-04, 3.8528e-04, 3.8528e-04,\n",
       "        2.3365e-04, 2.3365e-04, 1.8215e-04, 1.8215e-04, 1.6022e-04, 1.4210e-04,\n",
       "        1.4210e-04, 1.4210e-04], device='cuda:0', dtype=torch.bfloat16),\n",
       "indices=tensor([  755,     2,   791, 16309,   475,  3936,    17,    16,    32,    59,\n",
       "         1527,    11,    50,    51,   220, 13066,  1687,    34,    35,    40],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(probs_e[0, 0, :], k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05ff9266-96e0-4b38-a862-8dafba525165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c3cfea-afeb-4790-8c49-881f48f42ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663618"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_params(em.merger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53a5bda7-2d17-4a26-8329-6b9a2250d6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663618"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_params(am.merger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684c24ff-d5be-453c-ba77-fa07f9467306",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model merging training implementation using PyTorch and Transformers.\n",
    "Implements custom data collation and training for merged language models.\n",
    "\"\"\"\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union, Mapping\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import safetensors\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    default_data_collator,\n",
    "    is_torch_xla_available,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from transformers.utils import CONFIG_NAME\n",
    "\n",
    "from merger import (\n",
    "# from efficient_masks import (\n",
    "    MergerConfig,\n",
    "    # Merger,\n",
    "    NewMerger,\n",
    "    init_masks,\n",
    "    set_masks\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    generate, \n",
    "    get_hidden_states, \n",
    "    get_logits,\n",
    "    free_memory\n",
    ")\n",
    "# Configure logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77073ab-a341-4460-a5d6-d7fd14246273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Option 1: Set specific GPU devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428e341a-7bb4-4205-970d-dfe78619c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./trained_masks/checkpoint-300/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9168be66-512c-463f-aeb1-4173e3e50d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"architectures\": [\n",
       "    \"NewMerger\"\n",
       "  ],\n",
       "  \"constrain_mode\": \"01\",\n",
       "  \"mode\": \"vector_input\",\n",
       "  \"model_paths\": [\n",
       "    \"nguyenthanhdo/llama32_smol_rewrite_50k\",\n",
       "    \"nguyenthanhdo/llama32_smol_summarize_50k\"\n",
       "  ],\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig.from_pretrained(checkpoint_dir)\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcda1bb2-14af-4f84-b52b-4fe456f2e4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 12:02:09,191 - INFO - Creating merger with dummy weights ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537eb6ecdbda443fbd73f2dbe616d265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c8d2413bd744d99942a170ae13c53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796499b1dfa94e2ba6b8655c2d1fbeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|█▎                                                                                                                                                                            | 2/255 [00:07<15:17,  3.63s/it]2025-01-06 12:02:18,728 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:27<00:00,  9.17it/s]\n"
     ]
    }
   ],
   "source": [
    "merger = NewMerger.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    merge_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a36484ac-a041-4eed-ac67-d42c0c845307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight_masks': [tensor([0.5156, 0.5000, 0.5117,  ..., 0.4668, 0.4727, 0.5000],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([0.5156, 0.5000, 0.5117,  ..., 0.4688, 0.4707, 0.5000],\n",
       "         dtype=torch.bfloat16)],\n",
       " 'bias_masks': [None, None]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.merger.model.layers[21].mlp.up_proj.get_constrained_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecb7209-a754-41ee-bdbd-63bb8af29602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 07:58:21,126 - INFO - Creating merger with dummy weights ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c923c831581d4b46bab91c3754bd95d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe36121b8b442a7ae6db02ca21699b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220402ea086942afab978d0d4ff78c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|█▎                                                                                                                                                                            | 2/255 [00:07<15:20,  3.64s/it]2025-01-06 07:58:30,567 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:27<00:00,  9.26it/s]\n",
      "Setting up masks: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:00<00:00, 39634.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Initialize configuration\n",
    "# merge_config = MergerConfig(\n",
    "#     model_paths=[\n",
    "#         \"nguyenthanhdo/llama32_smol_rewrite_50k\",\n",
    "#         \"nguyenthanhdo/llama32_smol_summarize_50k\",\n",
    "#     ],\n",
    "#     mode=\"vector_input\",\n",
    "#     constrain_mode=\"identity\"\n",
    "# )\n",
    "\n",
    "# merger = NewMerger.from_pretrained(\n",
    "#     None,\n",
    "#     merge_config,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "# )\n",
    "# set_masks(merger.merger, strategy=\"uniform\", factors=[0.95, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a6113bb-a62a-4619-9cc5-ff5f5cc9a3c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merger = merger.to(device=\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d4aafa-ef63-458a-9429-9809b6970e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b137af4-8a55-4ab0-b19a-b1c246f72b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergerTrainer(Trainer):\n",
    "    \"\"\"Custom trainer for merged model training.\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        data_source = inputs.pop(\"data_source\")\n",
    "        effective_idxs = (labels != -100).float().unsqueeze(dim=-1)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits_merged = outputs[\"merger_outputs\"].logits\n",
    "        logits_components = [x.logits for x in outputs[\"components_outputs\"]]\n",
    "\n",
    "        # Compute target logits and KL divergence\n",
    "        logits_target = selective_logits_target(logits_components, data_source)\n",
    "        temperature = 1.0\n",
    "        kl_fct = nn.KLDivLoss(reduction=\"none\")\n",
    "        diff = (\n",
    "            kl_fct(\n",
    "                F.log_softmax(logits_target / temperature, dim=-1),\n",
    "                F.softmax(logits_merged / temperature, dim=-1)\n",
    "            )\n",
    "            * (temperature) ** 2\n",
    "        )\n",
    "        \n",
    "        # Calculate final loss\n",
    "        loss = (diff * effective_idxs).sum(dim=-1)\n",
    "        loss = (loss / effective_idxs.sum(dim=1)).mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33003b2b-f7b2-4356-b71f-3eb89686b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dataset = load_dataset(\n",
    "    \"HuggingFaceTB/smoltalk\",\n",
    "    \"smol-summarize\",\n",
    "    split=\"train\"\n",
    ")\n",
    "rewrite_dataset = load_dataset(\n",
    "    \"HuggingFaceTB/smoltalk\",\n",
    "    \"smol-rewrite\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbe76300-1c0a-46cf-b720-88a716bd1f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Extract and present the main key point of the input text in one very short sentence, including essential details like dates or locations if necessary.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi Naomi,\n",
      "\n",
      "I hope you're having a good week so far. I wanted to share some exciting news with you - I just found out that our research project has been awarded a major grant from the National Health and Medical Research Council!\n",
      "\n",
      "This funding will allow us to expand our study and collect more comprehensive data on the social determinants of health in Indigenous Australian communities. It's a huge vote of confidence in our work and a testament to the importance of the issues we're addressing.\n",
      "\n",
      "I wanted to let you know right away, as I think this could have significant implications for our collaboration going forward. With this additional funding, we'll be able to take on a larger-scale project and potentially make an even greater impact.\n",
      "\n",
      "What do you think? I'd love to discuss how we can leverage this funding to maximize the impact of our research.\n",
      "\n",
      "Looking forward to hearing your thoughts and celebrating this exciting news together!\n",
      "\n",
      "Best,\n",
      "Liam<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 50000\n",
    "train_dataset = summarize_dataset\n",
    "system = train_dataset[idx]['messages'][0]['content']\n",
    "prompt = train_dataset[idx]['messages'][1]['content']\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    # train_dataset[idx]['messages'],\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f03e2600-baef-4826-8c16-14762c27fc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liam shares that the research project has received a major grant from the National Health and Medical Research Council, allowing for expanded study and data collection. Liam is eager to discuss how to leverage this funding to enhance the project's impact. Liam is excited to celebrate the news and hear feedback.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.models[1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04bad100-7fc6-4d83-bc00-f35d7e5f2aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liam shares that the research project has received a major grant from the National Health and Medical Research Council, enabling expanded data collection on social determinants of health in Indigenous Australian communities. Liam is eager to discuss how to leverage this funding to enhance the project's impact.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.merger, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a0c2e41-810d-4a2b-a319-01575d9e7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_logits_target(logits_components, data_source):\n",
    "    \"\"\"Select appropriate logits based on data source.\"\"\"\n",
    "    # logits_components ~ [(batch_size, seq_len, vocab_size) * n_components]\n",
    "    # stacked_logits.shape = (n_components, batch_size, seq_len, vocab_size)\n",
    "    # data_source.shape == (batch_size,)\n",
    "    # indices.shape == (batch_size, 1, 1)\n",
    "    stacked_logits = torch.stack(logits_components)\n",
    "    indices = data_source.unsqueeze(-1).unsqueeze(-1)\n",
    "    return stacked_logits[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d483031-5108-4eaf-addd-55cd3b36c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_merged = get_logits(text, merger.merger, tokenizer)\n",
    "logits_a = get_logits(text, merger.models[0], tokenizer)\n",
    "logits_b = get_logits(text, merger.models[1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b071020f-2ab0-4c5f-8b2e-b0962087567e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 241, 128256])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9380c9-da08-4ac0-a9f6-f7bbffdf5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_kl_div(logits_a, logits_b, mask):\n",
    "    logits_a = logits_a.view(-1, logits_a.size(-1))\n",
    "    logits_b = logits_b.view(-1, logits_b.size(-1))\n",
    "    mask = mask.view(-1)\n",
    "\n",
    "    assert mask.size(0) == logits_a.size(0)\n",
    "\n",
    "    log_probs_a = nn.functional.log_softmax(logits_a, dim=-1)\n",
    "    log_probs_b = nn.functional.log_softmax(logits_b, dim=-1)\n",
    "\n",
    "    div = log_probs_a.exp() * (log_probs_a - log_probs_b)\n",
    "    div = div.sum(-1)\n",
    "\n",
    "    div = (div * mask).sum() / mask.sum()\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16f3fdf6-6972-4b28-84ed-e3148fd88381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(logits_a, logits_b):\n",
    "    probs_a = nn.functional.softmax(logits_a, dim=-1)\n",
    "    \n",
    "    loss = probs_a * (probs_a.log() - nn.functional.log_softmax(logits_b, dim=-1))\n",
    "    return loss.sum(-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96fb7c16-291e-4dd0-b15f-c5f3f0a2b19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6875, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
