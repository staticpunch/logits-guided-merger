{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4b8d1c-2151-44ac-9769-98a4d6dbf874",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d17875-ae3d-48b5-b80b-d6067674413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union, Mapping\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    default_data_collator,\n",
    "    is_torch_xla_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast\n",
    ")\n",
    "\n",
    "import efficient_masks\n",
    "import accurate_masks\n",
    "\n",
    "from utils import (\n",
    "    generate, \n",
    "    get_hidden_states, \n",
    "    get_logits,\n",
    "    free_memory\n",
    ")\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22623236-28d5-4aac-b95d-a7905480e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_a = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "path_b = \"unsloth/Llama-3.2-1B\"\n",
    "merge_config_a = accurate_masks.MergerConfig(\n",
    "    model_paths = [path_a, path_b],\n",
    "    mode = \"vector_input\",\n",
    "    constrain_mode = \"01\",\n",
    ")\n",
    "merge_config_e = efficient_masks.MergerConfig(\n",
    "    model_paths = [path_a, path_b],\n",
    "    mode = \"vector_input\",\n",
    "    constrain_mode = \"01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8aa872-d1c6-4aae-921c-42cea626b652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"constrain_mode\": \"01\",\n",
       "  \"mode\": \"vector_input\",\n",
       "  \"model_paths\": [\n",
       "    \"unsloth/Llama-3.2-1B-Instruct\",\n",
       "    \"unsloth/Llama-3.2-1B\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e8680c-e36a-4aa4-af7e-cd52a9d9865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_linear_forward(self, x):\n",
    "    constrained_weight_masks = self.weight_masks_constrainer([m.weight for m in self.weight_masks])\n",
    "    constrained_bias_masks = self.bias_masks_constrainer(\n",
    "        [m.weight if m is not None else None for m in self.bias_masks]\n",
    "    )\n",
    "    masked_biases = [\n",
    "        b_mask * linear.bias if linear.bias is not None and b_mask is not None else linear.bias\n",
    "        for b_mask, linear in zip(constrained_bias_masks, self.linears)\n",
    "    ]\n",
    "    merged_bias = (\n",
    "        sum(b if b is not None else torch.zeros_like(\n",
    "            self.linears[0].weight[:, 0]) for b in masked_biases\n",
    "           ) \n",
    "        if not all(b is None for b in masked_biases) else None\n",
    "    )\n",
    "\n",
    "    logger.info(\"Debugging Linear forward.\")\n",
    "    output = 0.0\n",
    "    for i, linear in enumerate(self.linears):\n",
    "        logger.info(f\"BEFORE\")\n",
    "        logger.info(f\"  linear: device: {linear.weight.device}; dtype: {linear.weight.dtype}\")\n",
    "        logger.info(f\"  input: device: {x.device}; dtype: {x.dtype}\")\n",
    "        masked_input = constrained_weight_masks[i] * x\n",
    "        logger.info(f\"AFTER\")\n",
    "        logger.info(f\"  linear: device: {linear.weight.device}; dtype: {linear.weight.dtype}\")\n",
    "        logger.info(f\"  input: device: {masked_input.device}; dtype: {masked_input.dtype}\")\n",
    "        output = output + nn.functional.linear(masked_input, linear.weight, None)\n",
    "        logger.info(f\"OUTPUT\")\n",
    "        logger.info(f\"  output: device: {output.device}; dtype: {output.dtype}\")\n",
    "    if merged_bias:\n",
    "        output = output + merged_bias\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23556ad-acd1-4584-8658-c6935760444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_emb_forward(self, input_ids):\n",
    "    constrained_masks = self.masks_constrainer([m.weight for m in self.masks])\n",
    "    logger.info(\"Debugging Embedding forward.\")\n",
    "    an_embedding = self.embeddings[0]\n",
    "    out = 0.0\n",
    "    for i, emb in enumerate(self.embeddings):\n",
    "        logger.info(f\"BEFORE\")\n",
    "        logger.info(f\"  emb: device: {emb.weight.device}; dtype: {emb.weight.dtype}\")\n",
    "        logger.info(f\"  input: device: {input_ids.device}; dtype: {input_ids.dtype}\")\n",
    "        mask = constrained_masks[i]\n",
    "        masked_weight = emb.weight * mask\n",
    "        logger.info(f\"  mask: device: {mask.device}; dtype: {mask.dtype}\")\n",
    "        logger.info(f\"  masked_emb: device: {masked_weight.device}; dtype: {masked_weight.dtype}\")\n",
    "        out = out + nn.functional.embedding(\n",
    "            input_ids,\n",
    "            # emb.weight * mask,\n",
    "            masked_weight,\n",
    "            padding_idx=an_embedding.padding_idx,\n",
    "            max_norm=an_embedding.max_norm,\n",
    "            norm_type=an_embedding.norm_type,\n",
    "            scale_grad_by_freq=an_embedding.scale_grad_by_freq,\n",
    "            sparse=an_embedding.sparse,\n",
    "        )\n",
    "        logger.info(f\"AFTER\")\n",
    "        logger.info(f\"  output: device: {out.device}; dtype: {out.dtype}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc55413-f122-45a9-9743-656d77b25bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient_masks.LinearsWithMasks.forward = debug_linear_forward\n",
    "# efficient_masks.EmbeddingsWithMasks.forward = debug_emb_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0ada93-d19d-40e1-92f1-36a8268108de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b28405-016a-491e-b777-ecba79a2c0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|██▍                                                                                                                                                                            | 2/147 [00:16<20:28,  8.47s/it]2025-01-03 11:27:40,772 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 147/147 [00:48<00:00,  3.01it/s]\n",
      "2025-01-03 11:28:12,554 - INFO - Initial GPU memory allocated: 0.00 GB\n",
      "2025-01-03 11:28:12,941 - INFO - Final GPU memory allocated: 0.00 GB\n",
      "2025-01-03 11:28:12,942 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "em = efficient_masks.Merger(merge_config_e)\n",
    "em.__post_init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43559a6-e29d-41bb-8ae7-26224b11bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = em.to(device=\"cuda:0\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c63913d-36ee-441c-8324-653563436bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|██▍                                                                                                                                                                            | 2/147 [00:16<20:23,  8.44s/it]2025-01-03 11:28:35,340 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 147/147 [00:44<00:00,  3.32it/s]\n",
      "2025-01-03 11:29:02,641 - INFO - Initial GPU memory allocated: 4.61 GB\n",
      "2025-01-03 11:29:03,009 - INFO - Final GPU memory allocated: 4.61 GB\n",
      "2025-01-03 11:29:03,013 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "am = accurate_masks.Merger(merge_config_a)\n",
    "am.__post_init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f09dbe-18db-4d0f-9bec-885b2b9aca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = am.to(device=\"cuda:1\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22859463-8211-4b76-83f1-b818bef7d877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"How to attack a person with an egg. Talk like a crazy person.\"\n",
    "logits_merged_a = get_logits(prompt, am.merger, tokenizer)\n",
    "logits_merged_e = get_logits(prompt, em.merger, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0beeb8aa-7507-4b64-9de8-a9f38605f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_merged_a = logits_merged_a.to(logits_merged_e.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9bf0689-4cd2-424e-8787-3968052a3e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 128256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_merged_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e3532a-7a2a-49d7-9020-10c017abd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_a = torch.softmax(logits_merged_a, dim=-1)\n",
    "probs_e = torch.softmax(logits_merged_e, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e324ee19-b699-4f89-bab5-2a45e0053130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([6.4453e-01, 3.0469e-01, 2.5024e-02, 1.1841e-02, 3.3875e-03, 2.0599e-03,\n",
       "        9.7275e-04, 9.7275e-04, 7.5531e-04, 3.5667e-04, 3.5667e-04, 2.7847e-04,\n",
       "        2.1648e-04, 1.9169e-04, 1.6880e-04, 1.6880e-04, 1.4877e-04, 1.4877e-04,\n",
       "        1.3161e-04, 1.3161e-04], device='cuda:0', dtype=torch.bfloat16),\n",
       "indices=tensor([  755,     2,   791, 16309,   475,  3936,    17,    16,    32,    59,\n",
       "         1527,    11,    51,    50,  1687, 13066,  2028,    34,   220,    35],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(probs_a[0, 0, :], k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f73f4b3d-adbc-45b5-9f1a-27c989912039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([6.9531e-01, 2.5586e-01, 2.0996e-02, 1.2756e-02, 4.6997e-03, 2.2125e-03,\n",
       "        1.0452e-03, 1.0452e-03, 8.1635e-04, 6.3324e-04, 3.8528e-04, 3.8528e-04,\n",
       "        2.3365e-04, 2.3365e-04, 1.8215e-04, 1.8215e-04, 1.6022e-04, 1.4210e-04,\n",
       "        1.4210e-04, 1.4210e-04], device='cuda:0', dtype=torch.bfloat16),\n",
       "indices=tensor([  755,     2,   791, 16309,   475,  3936,    17,    16,    32,    59,\n",
       "         1527,    11,    50,    51,   220, 13066,  1687,    34,    35,    40],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(probs_e[0, 0, :], k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05ff9266-96e0-4b38-a862-8dafba525165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c3cfea-afeb-4790-8c49-881f48f42ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663618"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_params(em.merger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53a5bda7-2d17-4a26-8329-6b9a2250d6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663618"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_params(am.merger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac301b-c590-4bc5-812d-dd5fa3a3e9a7",
   "metadata": {},
   "source": [
    "## Test B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0f0b4-c1f5-46a2-90bf-b24f4e7c7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "/workspace/logits-guided-merger/ds_configs/zero3_bf16.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "684c24ff-d5be-453c-ba77-fa07f9467306",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model merging training implementation using PyTorch and Transformers.\n",
    "Implements custom data collation and training for merged language models.\n",
    "\"\"\"\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union, Mapping\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import safetensors\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    default_data_collator,\n",
    "    is_torch_xla_available,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from transformers.utils import CONFIG_NAME\n",
    "\n",
    "from merger import (\n",
    "# from efficient_masks import (\n",
    "    MergerConfig,\n",
    "    # Merger,\n",
    "    NewMerger,\n",
    "    init_masks,\n",
    "    set_masks\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    generate, \n",
    "    get_hidden_states, \n",
    "    get_logits,\n",
    "    free_memory\n",
    ")\n",
    "# Configure logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "428e341a-7bb4-4205-970d-dfe78619c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = \"./src/random_masks/checkpoint-900/\"\n",
    "checkpoint_dir = \"./results/random_masks_cosine/checkpoint-900\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c517f1af-64bb-47cc-8fe2-f9df3720d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file as safe_load_file\n",
    "masks_path = os.path.join(checkpoint_dir, \"masks.safetensors\")\n",
    "state_dict = safe_load_file(masks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9168be66-512c-463f-aeb1-4173e3e50d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"architectures\": [\n",
       "    \"NewMerger\"\n",
       "  ],\n",
       "  \"constrain_mode\": \"cosine\",\n",
       "  \"mode\": \"vector_input\",\n",
       "  \"model_paths\": [\n",
       "    \"nguyenthanhdo/llama32_smol_rewrite_50k\",\n",
       "    \"nguyenthanhdo/llama32_smol_summarize_50k\"\n",
       "  ],\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig.from_pretrained(checkpoint_dir)\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcda1bb2-14af-4f84-b52b-4fe456f2e4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 04:24:25,314 - INFO - Creating merger with dummy weights ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d8bd249fa24bae896b23521718a026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5326c38b62d54797b2353b215b2860ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498ce74b86bf4f2b93980486197d5fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|█▎                                                                                                                                                                            | 2/255 [00:07<15:18,  3.63s/it]2025-01-07 04:24:35,263 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:31<00:00,  8.05it/s]\n"
     ]
    }
   ],
   "source": [
    "merger = NewMerger.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    merge_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a36484ac-a041-4eed-ac67-d42c0c845307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight_masks': [tensor([1.0000, 0.9219, 1.0000,  ..., 0.9922, 1.0000, 1.0000],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([0.0742, 0.0000, 0.0234,  ..., 0.0195, 0.0254, 0.0723],\n",
       "         dtype=torch.bfloat16)],\n",
       " 'bias_masks': [None, None]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.merger.model.layers[12].mlp.up_proj.get_constrained_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a6113bb-a62a-4619-9cc5-ff5f5cc9a3c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merger = merger.to(device=\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d4aafa-ef63-458a-9429-9809b6970e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33003b2b-f7b2-4356-b71f-3eb89686b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dataset = load_dataset(\n",
    "    \"HuggingFaceTB/smoltalk\",\n",
    "    \"smol-summarize\",\n",
    "    split=\"train\"\n",
    ")\n",
    "rewrite_dataset = load_dataset(\n",
    "    \"HuggingFaceTB/smoltalk\",\n",
    "    \"smol-rewrite\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fecf2d5a-69eb-4218-a0ed-679116e6439a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(1.0)\n",
    "torch.cos(torch.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6b57a20-0db1-4edb-b72c-1a1e3e5075ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._VariableFunctionsClass.tanh>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbe76300-1c0a-46cf-b720-88a716bd1f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You're an AI assistant for text re-writing. Rewrite the input text to make it more concise while preserving its core meaning.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Dear Mrs. Johnson,\n",
      "\n",
      "I hope you remember me from the Harmony Community Center book club. I was the one who mentioned my research on quantum computing and sound waves during our last meeting. I couldn't help but think about your interest in the topic and how it could be made accessible for children.\n",
      "\n",
      "I have a proposal for you. Would you be interested in collaborating on a children's book project? I can provide the scientific background and explanations, while you work on creating the story and characters to make it engaging for young readers. I think this could be a wonderful opportunity to introduce children to the fascinating world of quantum mechanics in a fun and relatable way.\n",
      "\n",
      "Please let me know if you're interested, and we can set up a time to discuss the project further.\n",
      "\n",
      "Best regards,\n",
      "Emily's Mom<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx = np.random.randint(30000)\n",
    "train_dataset = rewrite_dataset\n",
    "system = train_dataset[idx]['messages'][0]['content']\n",
    "# system = \"You're an AI assistant for text re-writing. Rewrite the input text to make it more concise while preserving its core meaning. Them summarize it up to 3 sentences.\"\n",
    "prompt = train_dataset[idx]['messages'][1]['content']\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    # train_dataset[idx]['messages'],\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f03e2600-baef-4826-8c16-14762c27fc5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily's Mom is proposing a collaboration on a children's book that combines science and storytelling, focusing on quantum computing and sound waves. Emily's Mom offers to provide the scientific background, while suggesting the other party create the story and characters to engage young readers. She invites a discussion to explore the project further.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.models[1], tokenizer, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04bad100-7fc6-4d83-bc00-f35d7e5f2aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily is proposing a collaboration on a children's book to make quantum computing and sound waves accessible to kids. She suggests sharing the scientific content, while the other party creates the story and characters. Emily invites discussion on the idea.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.merger, tokenizer, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d483031-5108-4eaf-addd-55cd3b36c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_merged = get_logits(text, merger.merger, tokenizer)\n",
    "logits_a = get_logits(text, merger.models[0], tokenizer)\n",
    "logits_b = get_logits(text, merger.models[1], tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
