{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ced156d-5f7e-41ce-b906-b7834bdf5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "# from modeling_qwen2 import (\n",
    "#     Qwen2RMSNorm, \n",
    "#     Qwen2RotaryEmbedding, \n",
    "#     Qwen2MLP, \n",
    "#     Qwen2Attention, \n",
    "#     Qwen2FlashAttention2, \n",
    "#     Qwen2SdpaAttention, \n",
    "#     Qwen2DecoderLayer, \n",
    "#     Qwen2PreTrainedModel, \n",
    "#     Qwen2Model, \n",
    "#     Qwen2ForCausalLM,\n",
    "# )\n",
    "\n",
    "# from configuration_qwen2 import Qwen2Config\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    ")\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2296c730-0ec6-49eb-bc5b-e6f075dc9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_memory():\n",
    "    if not torch.cuda.is_available():\n",
    "        logger.info(\"CUDA is not available. No GPU memory to free.\")\n",
    "        return\n",
    "        \n",
    "    initial_memory = torch.cuda.memory_allocated()\n",
    "    logger.info(f\"Initial GPU memory allocated: {initial_memory / 1024**3:.2f} GB\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    final_memory = torch.cuda.memory_allocated()\n",
    "    logger.info(f\"Final GPU memory allocated: {final_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "    freed_memory = initial_memory - final_memory\n",
    "    logger.info(f\"Freed GPU memory: {freed_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7294924d-45f5-44de-9e72-3203ce929334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(self, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = mask_config\n",
    "        self.size = mask_config.size\n",
    "        assert self.size is not None, \"Mask size must be specified.\"\n",
    "\n",
    "        value = mask_config.value\n",
    "        if value is not None:\n",
    "            logger.warning(\"Highly reccommend initializing mask value using dedicated setup functions.\")\n",
    "            if not isinstance(value, torch.Tensor): \n",
    "                try: value = torch.tensor(value)\n",
    "                except: raise ValueError(\n",
    "                    f\"Unable to convert {value} to torch.Tensor required for initializing a mask's weight.\"\n",
    "                )\n",
    "\n",
    "        ## TODO: might refactor later: modify _get_ones() to handle scalar mode.\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            self.weight = nn.Parameter(value if value is not None else torch.tensor(1.0))\n",
    "            \n",
    "        elif mask_config.mode in (\"vector_input\", \"vector_output\"):\n",
    "            ones = self._get_ones(mask_config.mode)\n",
    "            self.weight = nn.Parameter(value if value is not None else ones)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "\n",
    "        self._check_shape_compatibility()\n",
    "\n",
    "    def _get_ones(self, mode: str) -> torch.Tensor:\n",
    "        \"\"\"Generates a tensor of ones based on mode and size.\"\"\"\n",
    "        dim = 0 if mode == \"vector_output\" else -1\n",
    "        features = self.size[dim]\n",
    "        if len(self.size) == 2 and mode == \"vector_output\":\n",
    "            return torch.ones(features, 1)\n",
    "        else:\n",
    "            return torch.ones(features)\n",
    "          \n",
    "\n",
    "    def _check_shape_compatibility(self):\n",
    "        \"\"\"Raises ValueError if the mask shape is incompatible with its size.\"\"\"\n",
    "        try:\n",
    "            in_test = torch.rand(self.size)\n",
    "            out_test = self.weight * in_test\n",
    "            assert out_test.shape == in_test.shape, (\n",
    "                \"After applying mask, the shape of input weight does not stay the same.\"\n",
    "            )\n",
    "        except RuntimeError:\n",
    "            raise ValueError(\"Mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.size != x.shape:\n",
    "            logger.warning(\"Warning: Input shape does not match mask shape.\")\n",
    "        return x * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5468b2fe-a132-40f8-bb06-5863243c9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleWithMask(nn.Module, ABC):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ModuleWithMask, self).__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "class ModulesWithMasks(nn.Module, ABC):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ModulesWithMasks, self).__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_raw_masks(self):\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_constrained_masks(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5435d9-2462-4e39-9083-e6b9a347127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constrainer(nn.Module):\n",
    "\n",
    "    def __init__(self, component_weights, constrain_mode, mask_mode):\n",
    "        super().__init__()\n",
    "        self.statistics = None\n",
    "        self.constrain_mode = constrain_mode\n",
    "        self.mask_mode = mask_mode\n",
    "        if self.constrain_mode not in (\"identity\", \"01\", \"-11\", \"spherical\"):\n",
    "            raise ValueError(f\"Does not support {self.constrain_mode} constraint yet!\")\n",
    "            \n",
    "        if (self.constrain_mode == \"spherical\" and all([w is not None for w in component_weights])):\n",
    "            self._get_spherical_stats(component_weights)\n",
    "\n",
    "    def _get_spherical_stats(\n",
    "        self, \n",
    "        component_weights: List[torch.Tensor], \n",
    "        DOT_THRESHOLD: float = 0.99995\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            if any([w is None for w in component_weights]):\n",
    "                raise ValueError(\"Spherical constraint (SLERP) does not support None weights.\")\n",
    "            if len(component_weights) != 2: \n",
    "                raise ValueError(\n",
    "                    \"Spherical constraint (SLERP) only supports 2 component weights, \" +\n",
    "                    f\"{len(component_weights)} components found.\"\n",
    "                )\n",
    "\n",
    "            dim = 0 if self.mask_mode in (\"vector_input\") else None\n",
    "            v0 = self._normalize(component_weights[0], dim=dim)\n",
    "            v1 = self._normalize(component_weights[1], dim=dim)\n",
    "            self.dots = torch.sum(v0 * v1, dim=dim, keepdim=False) ## (out_features, in_features) -> (in_features, )\n",
    "            self.theta_0s = torch.arccos(self.dots)\n",
    "            self.sin_theta_0s = torch.sin(self.theta_0s)\n",
    "            \n",
    "    def _normalize(self, v, dim, eps: float = 1e-8):\n",
    "        norm_v = torch.linalg.norm(v, dim=dim)\n",
    "        norm_v[norm_v < eps] = 1.0\n",
    "        v = v / norm_v\n",
    "        return v\n",
    "\n",
    "    def _constrain_identity(self, mask_weights: List[torch.Tensor]):\n",
    "        return mask_weights\n",
    "\n",
    "    def _constrain_0_1(self, mask_weights: List[torch.Tensor]):\n",
    "        W = [torch.exp(w) for w in mask_weights]\n",
    "        W = [w / sum(W) for w in W]\n",
    "        return W\n",
    "\n",
    "    def _constrain_neg1_1(self, mask_weights: List[torch.Tensor]):\n",
    "        return mask_weights\n",
    "\n",
    "    def _constrain_spherical(self, mask_weights: List[torch.Tensor], DOT_THRESHOLD: float = 0.9995):\n",
    "        assert len(mask_weights) == 2, (\n",
    "            \"Spherical constraint (SLERP) only supports 2 mask weights\"\n",
    "        )\n",
    "        \n",
    "        # Transform raw masks to factor t's.\n",
    "        ## QUESTION MASK: Does this modify mask_weights in-place? I only \n",
    "        ## update them via backprop.\n",
    "        W = [torch.exp(w) for w in mask_weights]\n",
    "        T = W[0] / sum(W)\n",
    "\n",
    "        # Angle at timestep t's\n",
    "        theta_ts = self.theta_0s * T\n",
    "        sin_theta_ts = torch.sin(theta_ts)\n",
    "\n",
    "        # Finish calculating slerp factors\n",
    "        S0 = torch.sin(self.theta_0s - theta_ts) / self.sin_theta_0s\n",
    "        S1 = sin_theta_ts / self.sin_theta_0s\n",
    "\n",
    "        # Avoid NaN\n",
    "        nan_indices = self.dots.float() > DOT_THRESHOLD\n",
    "        S0[nan_indices] = 1 - T[nan_indices]\n",
    "        S1[nan_indices] = T[nan_indices]\n",
    "        \n",
    "        return [S0, S1]\n",
    "        \n",
    "    def forward(self, mask_weights: List[torch.Tensor]):\n",
    "        if any([w is None for w in mask_weights]):\n",
    "            return mask_weights\n",
    "            \n",
    "        if self.constrain_mode == \"identity\":\n",
    "            return self._constrain_identity(mask_weights)\n",
    "        elif self.constrain_mode == \"01\":\n",
    "            return self._constrain_0_1(mask_weights)\n",
    "        elif self.constrain_mode == \"-11\":\n",
    "            return self._constrain_neg1_1(mask_weights)\n",
    "        elif self.constrain_mode == \"spherical\":\n",
    "            return self._constrain_spherical(mask_weights)\n",
    "        else:\n",
    "            raise ValueError(f\"Does not support {self.constrain_mode} constraint yet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "581acd6e-ad1f-4577-84d6-893346a62427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearsWithMasks(ModulesWithMasks):\n",
    "    def __init__(\n",
    "        self,\n",
    "        linears: List[nn.Linear],\n",
    "        weight_modes: List[str] = None,\n",
    "        weight_values: List[float] = None,\n",
    "        bias_modes: List[str] = None,\n",
    "        bias_values: List[float] = None,\n",
    "        constrain_mode: str = \"identity\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if not all(isinstance(linear, nn.Linear) for linear in linears):\n",
    "            raise ValueError(\"All elements in 'linears' must be instances of nn.Linear.\")\n",
    "\n",
    "        if weight_values is None or len(weight_values) != len(linears):\n",
    "            raise ValueError(\n",
    "                f\"Weight values for masks: {weight_values} do not match with linear layers: {linears}\"\n",
    "            )\n",
    "        if bias_values is None:\n",
    "            bias_values = [None] * len(linears)\n",
    "        if len(bias_values) != len(linears):\n",
    "            raise ValueError(\n",
    "                f\"Bias values for masks: {bias_values} do not match with linear layers: {linears}\"\n",
    "            )\n",
    "\n",
    "        self.linears = nn.ModuleList(linears)\n",
    "        self.constrain_mode = constrain_mode\n",
    "\n",
    "        self.weight_masks = nn.ModuleList([\n",
    "            Mask(MaskConfig(mode, value, linear.weight.shape))\n",
    "            for mode, value, linear in zip(weight_modes, weight_values, linears)\n",
    "        ])\n",
    "        \n",
    "        self.weight_masks_constrainer = Constrainer(\n",
    "            component_weights=[x.weight for x in self.linears], \n",
    "            constrain_mode=constrain_mode, mask_mode=weight_modes[0]\n",
    "        )\n",
    "\n",
    "        self.bias_masks = nn.ModuleList([\n",
    "            Mask(MaskConfig(mode, value, linear.bias.shape)) if linear.bias is not None else None\n",
    "            for mode, value, linear in zip(bias_modes, bias_values, linears)\n",
    "        ])\n",
    "        \n",
    "        self.bias_masks_constrainer = Constrainer(\n",
    "            component_weights=[x.bias if x.bias is not None else None for x in self.linears],\n",
    "            constrain_mode=constrain_mode, mask_mode=bias_modes[0]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        constrained_weight_masks = self.weight_masks_constrainer([m.weight for m in self.weight_masks])\n",
    "        masked_weights = [\n",
    "            w_mask * linear.weight for w_mask, linear in zip(constrained_weight_masks, self.linears)\n",
    "        ]\n",
    "        merged_weight = sum(masked_weights)\n",
    "\n",
    "        constrained_bias_masks = self.bias_masks_constrainer(\n",
    "            [m.weight if m is not None else None for m in self.bias_masks]\n",
    "        )\n",
    "        masked_biases = [\n",
    "            b_mask * linear.bias if linear.bias is not None and b_mask is not None else linear.bias\n",
    "            for b_mask, linear in zip(constrained_bias_masks, self.linears)\n",
    "        ]\n",
    "\n",
    "        merged_bias = (\n",
    "            sum(b if b is not None else torch.zeros_like(merged_weight[:, 0]) for b in masked_biases)\n",
    "            if not all(b is None for b in masked_biases) else None\n",
    "        )\n",
    "\n",
    "        return nn.functional.linear(x, merged_weight, merged_bias)\n",
    "\n",
    "    def get_raw_masks(self):\n",
    "        with torch.no_grad():\n",
    "            return {\n",
    "                \"weight_masks\": [m.weight for m in self.weight_masks],\n",
    "                \"bias_masks\": [m.weight if m is not None else None for m in self.bias_masks],\n",
    "            }\n",
    "\n",
    "    def get_constrained_masks(self):\n",
    "        with torch.no_grad():\n",
    "            constrained_weight_masks = self.weight_masks_constrainer(\n",
    "                [m.weight for m in self.weight_masks]\n",
    "            )\n",
    "            constrained_bias_masks = self.bias_masks_constrainer(\n",
    "                [m.weight if m is not None else None for m in self.bias_masks]\n",
    "            )\n",
    "            return {\n",
    "                \"weight_masks\": constrained_weight_masks,\n",
    "                \"bias_masks\": constrained_bias_masks,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923570d0-c4ea-4f5d-bd8c-e959a8d8b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormsWithMasks(ModulesWithMasks):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rms_norms: List[nn.Module],\n",
    "        modes: List[str] = None,\n",
    "        values: List[float] = None,\n",
    "        constrain_mode: str = \"identity\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [norm.weight.shape for norm in rms_norms]\n",
    "        if any([mode != \"scalar\" for mode in modes]):\n",
    "            logger.warning(\n",
    "                f\"Though you want to make a masks of modes {modes} \" + \\\n",
    "                \"for RMSNorms' weights, by default a mask only accepts a scalar mask. \" + \\\n",
    "                \"Converting modes to `scalar`.\"\n",
    "            )\n",
    "            modes = [\"scalar\"] * len(modes)\n",
    "            \n",
    "        if values is None or len(values) != len(rms_norms):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with RMSNorm layers: {rms_norms}\")\n",
    "\n",
    "        self.rms_norms = nn.ModuleList(rms_norms)\n",
    "        self.masks = nn.ModuleList([\n",
    "            Mask(MaskConfig(mode, value, norm.weight.shape))\n",
    "            for mode, value, norm in zip(modes, values, rms_norms)\n",
    "        ])\n",
    "        self.masks_constrainer = Constrainer(\n",
    "            component_weights=[norm.weight for norm in self.rms_norms], \n",
    "            constrain_mode=constrain_mode, mask_mode=modes[0]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        constrained_masks = self.masks_constrainer([m.weight for m in self.masks])\n",
    "        masked_weights = [mask * norm.weight for mask, norm in zip(constrained_masks, self.rms_norms)]\n",
    "        merged_weight = sum(masked_weights)\n",
    "        variance_epsilon = self.rms_norms[0].variance_epsilon\n",
    "        for norm in self.rms_norms:\n",
    "            assert variance_epsilon == norm.variance_epsilon, (\"Variance epsilon among models must be consistent\")\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "        return merged_weight * hidden_states.to(input_dtype)\n",
    "\n",
    "    def get_raw_masks(self):\n",
    "        with torch.no_grad():\n",
    "            return {\"masks\": [m.weight for m in self.masks]}\n",
    "\n",
    "    def get_constrained_masks(self):\n",
    "        with torch.no_grad():\n",
    "            constrained_masks = self.masks_constrainer([m.weight for m in self.masks])\n",
    "            return {\"masks\": constrained_masks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58d5cf5f-180f-4c9b-b007-ef6cc84a87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsWithMasks(ModulesWithMasks):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        modes: List[str] = None,\n",
    "        values: List[float] = None,\n",
    "        constrain_mode: str = \"identity\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if values is None or len(values) != len(embeddings):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with Embedding layers: {embeddings}\")\n",
    "\n",
    "        self.embeddings = nn.ModuleList(embeddings)\n",
    "        sizes = [emb.weight.shape for emb in embeddings]\n",
    "        self.masks = nn.ModuleList([\n",
    "            Mask(MaskConfig(mode, value, size))\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ])\n",
    "        self.masks_constrainer = Constrainer(\n",
    "            component_weights=[emb.weight for emb in self.embeddings], \n",
    "            constrain_mode=constrain_mode, mask_mode=modes[0]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        constrained_masks = self.masks_constrainer([m.weight for m in self.masks])\n",
    "        masked_weights = [mask * emb.weight for mask, emb in zip(constrained_masks, self.embeddings)]\n",
    "        merged_weight = sum(masked_weights)\n",
    "        \n",
    "        an_embedding = self.embeddings[0]\n",
    "        for other_embedding in self.embeddings:\n",
    "            assert an_embedding.padding_idx == other_embedding.padding_idx\n",
    "            assert an_embedding.max_norm == other_embedding.max_norm\n",
    "            assert an_embedding.norm_type == other_embedding.norm_type\n",
    "            assert an_embedding.scale_grad_by_freq == other_embedding.scale_grad_by_freq\n",
    "            assert an_embedding.sparse == other_embedding.sparse\n",
    "            \n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            merged_weight,\n",
    "            padding_idx=an_embedding.padding_idx,\n",
    "            max_norm=an_embedding.max_norm,\n",
    "            norm_type=an_embedding.norm_type,\n",
    "            scale_grad_by_freq=an_embedding.scale_grad_by_freq,\n",
    "            sparse=an_embedding.sparse,\n",
    "        )\n",
    "        \n",
    "    def get_raw_masks(self):\n",
    "        with torch.no_grad():\n",
    "            return {\"masks\": [m.weight for m in self.masks]}\n",
    "\n",
    "    def get_constrained_masks(self):\n",
    "        with torch.no_grad():\n",
    "            constrained_masks = self.masks_constrainer([m.weight for m in self.masks])\n",
    "            return {\"masks\": constrained_masks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef820ee-a05c-4ddb-a828-d5498150c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd_one_out(masked_module: nn.Module, selected_idx: int):  \n",
    "    assert selected_idx is not None and isinstance(selected_idx, int), (\n",
    "        \"Must provide valid model index. Check whether passed index is `int`\"\n",
    "    )\n",
    "    masks_modules = []\n",
    "    for name, child in masked_module.named_children():\n",
    "        if not isinstance(child, nn.ModuleList): continue\n",
    "        assert selected_idx < len(child), (\n",
    "            f\"There are only {len(child)} component models, passed model index is {selected_idx}\"\n",
    "        )\n",
    "        ## exclude sub_module that is None, aka bias_masks.\n",
    "        if all(isinstance(sub_module, Mask) for sub_module in child):\n",
    "            masks_modules.append(child)\n",
    "        \n",
    "    for masks in masks_modules:\n",
    "        for i, mask in enumerate(masks):\n",
    "            value = 1.0 if i == selected_idx else 0.0\n",
    "            with torch.no_grad():\n",
    "                mask.weight.data.fill_(value)\n",
    "\n",
    "def random_init(masked_module: nn.Module):\n",
    "    masks_modules = []\n",
    "    for name, child in masked_module.named_children():\n",
    "        if not isinstance(child, nn.ModuleList): continue\n",
    "        ## exclude sub_module that is None, aka bias_masks.\n",
    "        if all(isinstance(sub_module, Mask) for sub_module in child):\n",
    "            masks_modules.append(child)\n",
    "        \n",
    "    for masks in masks_modules:\n",
    "        for i, mask in enumerate(masks):\n",
    "            with torch.no_grad():\n",
    "                random_value = torch.rand_like(mask.weight.data)\n",
    "                mask.weight.data = random_value\n",
    "\n",
    "def uniform_init(masked_module: nn.Module, factors: List[float]):  \n",
    "    masks_modules = []\n",
    "    for name, child in masked_module.named_children():\n",
    "        if not isinstance(child, nn.ModuleList): continue\n",
    "        assert len(factors) == len(child), (\n",
    "            f\"There are {len(child)} component models, but your passed factors have {len(factors)} values.\"\n",
    "        )\n",
    "        ## exclude sub_module that is None, aka bias_masks.\n",
    "        if all(isinstance(sub_module, Mask) for sub_module in child):\n",
    "            masks_modules.append(child)\n",
    "\n",
    "    for masks in masks_modules:\n",
    "        for factor, mask in zip(factors, masks):\n",
    "            with torch.no_grad():\n",
    "                mask.weight.data.fill_(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75aa6538-2cf4-4b99-bfeb-f76fe348e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_masked_modules(module):\n",
    "    masked_module_names = []\n",
    "    for parent_name, parent_module in module.named_modules():\n",
    "        for name, child in parent_module.named_children():\n",
    "            full_child_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            if (\"WithMasks\" in type(child).__name__):\n",
    "                masked_module_names.append(full_child_name)\n",
    "\n",
    "    return masked_module_names\n",
    "\n",
    "def get_init_method(strategy):\n",
    "    MAP = {\n",
    "        \"random\": random_init,\n",
    "        \"odd_one_out\": odd_one_out,\n",
    "        \"uniform\": uniform_init\n",
    "    }\n",
    "    selected_init_method = MAP[strategy]\n",
    "    \n",
    "    return selected_init_method\n",
    "    \n",
    "def set_masks(root_module, strategy=\"random\", **kwargs):\n",
    "\n",
    "    init_method = get_init_method(strategy)\n",
    "    masked_module_names = find_masked_modules(root_module)\n",
    "    \n",
    "    for module_name in tqdm(masked_module_names, desc=\"Setting up masks\"):\n",
    "        module_names = module_name.split(\".\")\n",
    "        target_module = root_module\n",
    "        for m_name in module_names:\n",
    "            target_module = getattr(target_module, m_name)\n",
    "\n",
    "        init_method(target_module, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "efbd4777-0273-4e5d-ade5-0bf2bf2fe9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        mode: str = None,\n",
    "        constrain_mode: str = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        self.mode = mode\n",
    "        self.constrain_mode = constrain_mode\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.num_models = len(merge_config.model_paths)\n",
    "        self.configs = [\n",
    "            AutoConfig.from_pretrained(path) \n",
    "            # Qwen2Config.from_pretrained(path)\n",
    "            for path in merge_config.model_paths\n",
    "        ]\n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.models = nn.ModuleList([\n",
    "            # Qwen2ForCausalLM.from_pretrained(\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                merge_config.model_paths[i], \n",
    "                config=self.configs[i],\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ) \n",
    "            for i in range(self.num_models)\n",
    "        ])\n",
    "        # self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        for model in self.models:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.merger = copy.deepcopy(self.models[0])\n",
    "        init_masks(\n",
    "            self.merger, self.models, self.merge_config\n",
    "        )\n",
    "        free_memory()\n",
    "        \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_ids: Torch.Tensor(batch_size x len)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"merged_logits\": self.merger(input_ids)[\"logits\"][:,-1],\n",
    "            \"components_logits\": [model(input_ids)[\"logits\"][:, -1] for model in self.models] \n",
    "        }\n",
    "# model = Merger(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "384cbdd9-25ef-48e5-ad09-0c1cb718adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import KLDivLoss, CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "# kl_loss = KLDivLoss(reduction=\"batchmean\")\n",
    "# # input should be a distribution in the log space\n",
    "# input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n",
    "# # Sample a batch of distributions. Usually this would come from the dataset\n",
    "# target = F.softmax(torch.rand(3, 5), dim=1)\n",
    "# print(target)\n",
    "# output = kl_loss(input, input)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fca4fc3e-9637-4fb4-894c-c236520491f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# # Example 2D array\n",
    "# array = torch.tensor([[0.1, 0.2, 0.7],\n",
    "#                   [0.3, 0.4, 0.3],\n",
    "#                   [0.25, 0.25, 0.5]])\n",
    "\n",
    "# # Compute entropy for each row\n",
    "# row_entropies = np.apply_along_axis(entropy, 1, array)\n",
    "\n",
    "# type(torch.tensor(row_entropies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8bade09c-3f05-43d4-b6c6-3e262b44d8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-27.1689, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "loss(input, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7efe4eb7-4ce2-413b-9829-f6f6a9ab2486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.11/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    \"wikitext2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0c43ff70-cbd8-4746-b3e4-d33f8402c00b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m----> 2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mlm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mlm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     compute_loss\u001b[38;5;241m=\u001b[39mcompute_loss,\n\u001b[1;32m      7\u001b[0m     compute_loss_func\u001b[38;5;241m=\u001b[39mcompute_loss,\n\u001b[1;32m      8\u001b[0m     optimizers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    compute_loss=compute_loss,\n",
    "    compute_loss_func=compute_loss,\n",
    "    optimizers=None,\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3361c54a-b856-483a-8d7c-f8c5ce5aedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_modules_to_add_masks(target_module):\n",
    "    module_names_to_replace = []\n",
    "    for parent_name, parent_module in target_module.named_modules():\n",
    "        for name, child in parent_module.named_children():\n",
    "            full_child_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            if (isinstance(child, (nn.Linear, nn.Embedding)) \n",
    "                or \"RMSNorm\" in type(child).__name__):\n",
    "                module_names_to_replace.append(full_child_name)\n",
    "\n",
    "    return module_names_to_replace\n",
    "\n",
    "def init_masks(target_module: nn.Module, ref_modules: nn.Module, merge_config: MergerConfig):\n",
    "    \"\"\"\n",
    "    Replaces eligible submodules in target_module with masked versions, \n",
    "    using corresponding modules from ref_modules as a reference for weights.\n",
    "\n",
    "    Args:\n",
    "        target_module: The module in which to replace submodules.\n",
    "        ref_modules: A list of modules to use as a reference for weights.\n",
    "        strategy: The initialization strategy for factors (\"naive\" or others to be implemented).\n",
    "    \"\"\"\n",
    "    mode = merge_config.mode\n",
    "    constrain_mode = merge_config.constrain_mode\n",
    "    module_names_to_replace = find_modules_to_add_masks(target_module)\n",
    "    \n",
    "    for module_name in tqdm(module_names_to_replace, desc=\"Initializing masks\"):\n",
    "        module_names = module_name.split(\".\")\n",
    "        target_child = target_module\n",
    "        ref_children = ref_modules\n",
    "\n",
    "        for m_name in module_names:\n",
    "            target_child = getattr(target_child, m_name)\n",
    "            ref_children = [getattr(ref_module, m_name) for ref_module in ref_children]\n",
    "\n",
    "        num_components = len(ref_modules)\n",
    "        modes = [mode for _ in ref_children]\n",
    "        factors = [None for _ in ref_children]\n",
    "\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            new_module = LinearsWithMasks(\n",
    "                linears=ref_children,\n",
    "                weight_modes=modes,\n",
    "                weight_values=factors,\n",
    "                bias_modes=modes,\n",
    "                bias_values=factors,\n",
    "                constrain_mode=constrain_mode\n",
    "            )\n",
    "\n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            new_module = EmbeddingsWithMasks(ref_children, modes, factors, constrain_mode)\n",
    "        elif \"RMSNorm\" in type(target_child).__name__:\n",
    "            new_module = RMSNormsWithMasks(ref_children, modes, factors, constrain_mode)\n",
    "\n",
    "        # Replace the original module with the new masked module\n",
    "        parent_module = target_module\n",
    "        for m_name in module_names[:-1]:\n",
    "            parent_module = getattr(parent_module, m_name)\n",
    "        setattr(parent_module, module_names[-1], new_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4473891a-f55e-4274-adcb-2b3239c2f2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"constrain_mode\": \"identity\",\n",
       "  \"mode\": \"scalar\",\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/dont15/models/llama32_smol_rewrite_20k/\",\n",
       "    \"/workspace/dont15/models/llama32_smol_summarize_20k/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.47.1\"\n",
       "}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/dont15/models/llama32_smol_rewrite_20k/\",\n",
    "        \"/workspace/dont15/models/llama32_smol_summarize_20k/\"\n",
    "    ],\n",
    "    # mode = \"vector_input\",\n",
    "    mode = \"scalar\",\n",
    "    constrain_mode = \"identity\"\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe952ee3-c3e2-440c-8486-63d2b974cf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19fefb2123143aba5ad109d8d4f5431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2797eb96d4fd4ee8b66e45597c5bdada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [01:29<00:00,  2.86it/s]\n",
      "2024-12-23 13:56:53,281 - INFO - Initial GPU memory allocated: 0.00 GB\n",
      "2024-12-23 13:56:54,681 - INFO - Final GPU memory allocated: 0.00 GB\n",
      "2024-12-23 13:56:54,682 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "merger = Merger(merge_config)\n",
    "merger.__post_init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4b981ed-c078-42fa-8232-cdf646984cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 13:57:01,303 - INFO - Initial GPU memory allocated: 0.00 GB\n",
      "2024-12-23 13:57:01,671 - INFO - Final GPU memory allocated: 0.00 GB\n",
      "2024-12-23 13:57:01,671 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "merger = merger.to(device=\"cuda:7\", dtype=torch.bfloat16)\n",
    "free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c8697f1d-cd7b-4dac-8607-9215fad000a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = merger(torch.tensor([[40, 20], [40, 20]]).to(\"cuda:7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de194a-bbe4-4c8d-bfd2-3f90eaeff209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example 2D array\n",
    "array = torch.tensor([[0.1, 0.2, 0.7],\n",
    "                      [0.3, 0.4, 0.3],\n",
    "                      [0.25, 0.25, 0.5]])\n",
    "\n",
    "# Compute entropy for each row\n",
    "def compute_entropy(row):\n",
    "    return -torch.sum(row * torch.log(row + 1e-9))\n",
    "\n",
    "row_entropies = torch.tensor([compute_entropy(row) for row in array])\n",
    "\n",
    "print(\"Entropy of each row:\", row_entropies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eb58329d-272b-42ef-a785-8d17bca8ba29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.3438, device='cuda:7', dtype=torch.bfloat16, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from torch.nn import KLDivLoss, CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def compute_entropy(row):\n",
    "    return -torch.sum(row * torch.log(row + 1e-9))\n",
    "    \n",
    "def compute_loss(args):\n",
    "    merged_logits = F.softmax(args[\"merged_logits\"], dim = 1)\n",
    "    base_logits = F.softmax(args[\"components_logits\"][0], dim = 1)\n",
    "    finetuned_logits = [F.softmax(component, dim = 1) for component in args[\"components_logits\"][1:]]\n",
    "\n",
    "    kl_loss = KLDivLoss(reduction=\"batchmean\")\n",
    "    entropy_loss = nn.CrossEntropyLoss()\n",
    "    entropy_list = [np.apply_along_axis(entropy, 1, logit.float().cpu().detach().numpy()) for logit in finetuned_logits][0]\n",
    "    best_index = np.argmax(entropy_list)\n",
    "    best_finetuned_logits = finetuned_logits[best_index]\n",
    "    \n",
    "    ### Select best model\n",
    "    loss = kl_loss(merged_logits, base_logits) # + kl_loss(merged_logits, best_finetuned_logits)\n",
    "    return loss\n",
    "compute_loss(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ea154af0-f9bc-475a-bc89-d6d224c32c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'merged_logits': tensor([[ 20.5000,  20.1250,  23.8750,  ..., -10.5625, -10.5625, -10.5625],\n",
       "         [ 20.5000,  20.1250,  23.8750,  ..., -10.5625, -10.5625, -10.5625]],\n",
       "        device='cuda:7', dtype=torch.bfloat16, grad_fn=<SelectBackward0>),\n",
       " 'components_logits': [tensor([[ 7.1875,  9.9375,  8.5000,  ..., -4.8438, -4.8438, -4.8438],\n",
       "          [ 7.1875,  9.9375,  8.5000,  ..., -4.8438, -4.8438, -4.8438]],\n",
       "         device='cuda:7', dtype=torch.bfloat16),\n",
       "  tensor([[ 7.4062,  9.6875,  8.3750,  ..., -4.5312, -4.5312, -4.5312],\n",
       "          [ 7.4062,  9.6875,  8.3750,  ..., -4.5312, -4.5312, -4.5312]],\n",
       "         device='cuda:7', dtype=torch.bfloat16)]}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
