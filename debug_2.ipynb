{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cdad09-d3c0-45e8-8f7d-1e73cac721b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from modeling_qwen2 import (\n",
    "    Qwen2RMSNorm, \n",
    "    Qwen2RotaryEmbedding, \n",
    "    Qwen2MLP, \n",
    "    Qwen2Attention, \n",
    "    Qwen2FlashAttention2, \n",
    "    Qwen2SdpaAttention, \n",
    "    Qwen2DecoderLayer, \n",
    "    Qwen2PreTrainedModel, \n",
    "    Qwen2Model, \n",
    "    Qwen2ForCausalLM,\n",
    "    # logger\n",
    ")\n",
    "\n",
    "from configuration_qwen2 import Qwen2Config\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    ")\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b39d8d-c4dd-4231-89a9-6422aa0de49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import are_tokenizers_same\n",
    "# are_tokenizers_same(\n",
    "#     paths = [\n",
    "#         \"/workspace/models/Arcee-VyLinh/\",\n",
    "#         \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbb9c5f-c766-441f-80f7-c37014ffd078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer(path, layer_idx=33):\n",
    "\tstate_dict = {}\n",
    "\tshard_paths = [f for f in os.listdir(path) if f.endswith('.safetensors')]\n",
    "\tfor shard_path in sorted(shard_paths, key=lambda x: int(x.split('-')[1])):\n",
    "\t\tapath = os.path.join(path, shard_path)\n",
    "\t\twith safe_open(apath, framework=\"pt\", device=\"cpu\") as f:\n",
    "\t\t\tfor key in f.keys():\n",
    "\t\t\t\tif f\"layers.{str(layer_idx)}.\" in key:\n",
    "\t\t\t\t\tstate_dict[key] = f.get_tensor(key)\n",
    "\treturn state_dict\n",
    "\n",
    "def strip_prefix(state_dict, prefix=\"model.layers.\"):\n",
    "    \"\"\"Strips 'model.layers.*.' prefix from 'input_layernorm.weight' keys.\"\"\"\n",
    "    return {\n",
    "      k.replace(f\"{prefix}{k.split('.')[2]}.\", \"\") if k.startswith(prefix)\n",
    "      else k: v for k, v in state_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9560a0b5-7194-4096-8a2c-5d4bddeee216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(\n",
    "    t: float, v0: Union[np.ndarray, torch.Tensor], v1: Union[np.ndarray, torch.Tensor]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    return (1 - t) * v0 + t * v1\n",
    "\n",
    "def weighted_sum(\n",
    "    factors: List[float], \n",
    "    tensors: Union[List[np.ndarray], List[torch.Tensor]]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    result = 0.0\n",
    "    # for factor, tensor in zip(factors, tensors):\n",
    "    #     result += factor * tensor\n",
    "    return sum([tensor * factor for tensor, factor in zip(tensors, factors)])\n",
    "\n",
    "def merge_modules(modules, factors):\n",
    "    \"\"\"\n",
    "    This is only applicable for cases where a static set of scalars\n",
    "    playing as merging factor for every submodules of the passed module.\n",
    "    Not recommend for fine-grained usecases.\n",
    "    \"\"\"\n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out\n",
    "\n",
    "def merge_linears(modules, weight_factors, bias_factors):\n",
    "    param_names = sorted([name for name, _ in modules[0].named_parameters()])\n",
    "    for module in modules:\n",
    "        other_param_names = sorted([name for name, _ in module.named_parameters()])\n",
    "        assert param_names == other_param_names, \"Mismatch tensor names.\"\n",
    "        \n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        if \"weight\" in tensor_name:\n",
    "            factors = weight_factors\n",
    "        elif \"bias\" in tensor_name:\n",
    "            factors = bias_factors\n",
    "        else:\n",
    "            raise ValueError(\"Hey this tensor is neither weight or bias.\")\n",
    "            \n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8d01ac-aef2-440c-86bc-7accd0932b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_masks(target_module, ref_modules, factors):\n",
    "    \"\"\"\n",
    "    Recursively replaces normal components with masked components.\n",
    "    \n",
    "    Args:\n",
    "      module: The module in which to replace layers.\n",
    "    \"\"\"\n",
    "    assert len(ref_modules) == len(factors)\n",
    "    num_components = len(ref_modules)\n",
    "    for name, target_child in target_module.named_children():\n",
    "        ref_children = [getattr(module, name) for module in ref_modules]\n",
    "        modes = [\"scalar\" for _ in ref_children]\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            new_module = LinearsWithMasks(\n",
    "                linears=ref_children,\n",
    "                weight_modes=[\"scalar\"] * num_components,\n",
    "                weight_values=factors,\n",
    "                bias_modes=[\"scalar\"] * num_components,\n",
    "                bias_values=factors,\n",
    "            )\n",
    "            setattr(target_module, name, new_module)\n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            setattr(target_module, name, EmbeddingsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "        elif type(target_child).__name__ == Qwen2RMSNorm.__name__:\n",
    "            # print(\"Hehe placing masks to a cutie RMSNorm\")\n",
    "            setattr(target_module, name, RMSNormsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "        else:\n",
    "            place_masks(target_child, ref_children, factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79dd85d4-c7e9-4d62-9813-5c590b63a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, model, tokenizer, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()\n",
    "\n",
    "def get_logits(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_ids).logits\n",
    "    return logits\n",
    "\n",
    "def get_hidden_states(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids, output_hidden_states=True, use_cache=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdce16a2-1565-4d6e-8506-8972d34271f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_input(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    past_key_values = None\n",
    "    cache_position = None\n",
    "    position_ids = None\n",
    "    output_hidden_states = True\n",
    "    output_attentions = False\n",
    "    use_cache = False\n",
    "    return_dict = True\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        return_legacy_cache = False\n",
    "        inputs_embeds = model.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = model._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "    return dict(\n",
    "        hidden_states=hidden_states,\n",
    "        attention_mask=causal_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=past_key_values,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "        cache_position=cache_position,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c25f7fa-f0b7-4ecc-903f-c072d016813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(mlp, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    ref: self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n",
    "    \"\"\"\n",
    "    steps = {}\n",
    "    steps.update({\"step 0 (input)\": x})\n",
    "    \n",
    "    gate = mlp.gate_proj(x)\n",
    "    steps.update({\"step 1 (gate)\": gate})\n",
    "    \n",
    "    up = mlp.up_proj(x)\n",
    "    steps.update({\"step 2 (up)\": up})\n",
    "    \n",
    "    act = mlp.act_fn(gate) # The activation function should be applied to the gate projection\n",
    "    steps.update({\"step 3 (activation)\": act})\n",
    "    \n",
    "    act_up = act * up  # Multiply the activated gate with the up projection\n",
    "    steps.update({\"step 4 (act_up)\": act_up})\n",
    "\n",
    "    down = mlp.down_proj(act_up) # Apply the down projection to the result of act * up\n",
    "    steps.update({\"step 5 (down - output)\": down})\n",
    "    \n",
    "    return dict(\n",
    "        output=down,\n",
    "        steps=steps\n",
    "    )\n",
    "\n",
    "# def mlp_forward(self, hidden_state):\n",
    "#     return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad60458-d15d-4392-85d8-03fda6ebcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward(\n",
    "    decoder,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "    output_attentions: Optional[bool] = False,\n",
    "    use_cache: Optional[bool] = False,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "\n",
    "    steps = {}\n",
    "    # logger.warning(f\"-------- Logging hidden_states in decoder forward:\")\n",
    "    residual = hidden_states\n",
    "    # logger.warning(f\" hidden_states step 1 (as input): {hidden_states}\")\n",
    "    steps.update({\"step 1\": hidden_states})\n",
    "\n",
    "    hidden_states = decoder.input_layernorm(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 2 (after input_layernorm): {hidden_states}\")\n",
    "    steps.update({\"step 2\": hidden_states})\n",
    "    # Self Attention\n",
    "    hidden_states, self_attn_weights, present_key_value = decoder.self_attn(\n",
    "        hidden_states=hidden_states,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=past_key_value,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "        cache_position=cache_position,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )\n",
    "    # logger.warning(f\" hidden_states step 3 (after self_attn): {hidden_states}\")\n",
    "    steps.update({\"step 3\": hidden_states})\n",
    "    \n",
    "    hidden_states = residual + hidden_states\n",
    "    # logger.warning(f\" hidden_states step 4 (after first skip connection): {hidden_states}\")\n",
    "    steps.update({\"step 4\": hidden_states})\n",
    "    # Fully Connected\n",
    "    residual = hidden_states\n",
    "    hidden_states = decoder.post_attention_layernorm(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 5 (after post_attention_layernorm): {hidden_states}\")\n",
    "    steps.update({\"step 5\": hidden_states})\n",
    "    \n",
    "    hidden_states = decoder.mlp(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 6 (after mlp): {hidden_states}\")\n",
    "    steps.update({\"step 6\": hidden_states})\n",
    "    \n",
    "    hidden_states = residual + hidden_states\n",
    "    # logger.warning(f\" hidden_states step 7 (after second skip connection): {hidden_states}\")\n",
    "    steps.update({\"step 7\": hidden_states})\n",
    "\n",
    "    outputs = (hidden_states,)\n",
    "\n",
    "    if output_attentions:\n",
    "        outputs += (self_attn_weights,)\n",
    "\n",
    "    if use_cache:\n",
    "        outputs += (present_key_value,)\n",
    "\n",
    "    outputs += (steps,)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835816e2-51ff-407d-bc01-7508c55426fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    past_key_values = None\n",
    "    cache_position = None\n",
    "    position_ids = None\n",
    "    output_hidden_states = True\n",
    "    output_attentions = False\n",
    "    use_cache = False\n",
    "    return_dict = True\n",
    "    #############\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # kept for BC (non `Cache` `past_key_values` inputs)\n",
    "        return_legacy_cache = False\n",
    "        inputs_embeds = model.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = model._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "        all_decoder_steps = ()\n",
    "\n",
    "        for i, decoder_layer in enumerate(model.layers[:2]):   \n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "          \n",
    "            layer_outputs = decoder_forward(\n",
    "                decoder_layer,\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            steps = layer_outputs[-1]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "            all_decoder_steps += (steps,)\n",
    "\n",
    "        hidden_states = model.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if return_legacy_cache:\n",
    "            next_cache = next_cache.to_legacy_cache()\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=(),\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_decoder_steps,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5a8d8-eef0-4dfd-b26d-8a27d0aed4e5",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b5bbb8-1a19-4aa7-9d3b-8e6dfc628d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.weight = nn.Parameter(torch.tensor(value))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        if self.size is not None:\n",
    "            try:\n",
    "                self.weight * torch.rand(self.size)\n",
    "            except RuntimeError:\n",
    "                print(\"mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Be really careful here (though I do not think it matters that much),\n",
    "        When testing, it's important that the masking operation is implemented\n",
    "        with `x = self.weight * x` instead of `x = x * self.weight`.\n",
    "\n",
    "        Neither of those two implementation is superior, however I need to be\n",
    "        consistent when doing testing because the phenonmenon above could lead\n",
    "        to some number imprecision, which may fail `torch.testing.assert_close`\n",
    "        \"\"\"\n",
    "        if self.size is None:\n",
    "            return self.weight * x\n",
    "        else:\n",
    "            if self.size != x.shape:\n",
    "                print(\"The shape of input does not match that of the mask.\")\n",
    "            return self.weight * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce5f133-2e26-4855-89c4-10853bd7de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithMask(nn.Module):\n",
    "    def __init__(self, linear, weight_mask_config: MaskConfig, bias_mask_config: MaskConfig = None):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.weight_mask_config = weight_mask_config\n",
    "        self.bias_mask_config = bias_mask_config\n",
    "\n",
    "        if linear.weight.shape != weight_mask_config.size:\n",
    "            print(\"Weight mask shape is not compatible with linear, reinitializing...\")\n",
    "            self.weight_mask_config.size = linear.weight.shape\n",
    "        self.weight_mask = Mask(self.weight_mask_config)\n",
    "\n",
    "        if linear.bias is not None and bias_mask_config is not None:\n",
    "            if linear.bias.shape != bias_mask_config.size:\n",
    "                print(\"Bias mask shape is not compatible with linear, reinitializing...\")\n",
    "                self.bias_mask_config.size = linear.bias.shape\n",
    "            self.bias_mask = Mask(self.bias_mask_config)\n",
    "        else:\n",
    "            self.bias_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        masked_weight = self.weight_mask(self.linear.weight)\n",
    "        if self.linear.bias is not None and self.bias_mask is not None:\n",
    "            masked_bias = self.bias_mask(self.linear.bias)\n",
    "        else:\n",
    "            masked_bias = self.linear.bias\n",
    "        return nn.functional.linear(x, masked_weight, masked_bias)\n",
    "\n",
    "class LinearsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        linears: List[nn.Module],\n",
    "        weight_modes: List[str] = [\"scalar\"],\n",
    "        weight_values: List[float] = None,\n",
    "        bias_modes: List[str] = [\"scalar\"],\n",
    "        bias_values: List[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not all(isinstance(linear, nn.Linear) for linear in linears):\n",
    "            raise ValueError(\"All elements in 'linears' must be instances of nn.Linear.\")\n",
    "\n",
    "        weight_sizes = [linear.weight.shape for linear in linears]\n",
    "        bias_sizes = [linear.bias.shape if linear.bias is not None else None for linear in linears]\n",
    "        \n",
    "        if weight_values is None or len(weight_values) != len(linears):\n",
    "            raise ValueError(f\"weight_values for masks: {weight_values} do not match with linear layers: {linears}\")\n",
    "        if bias_values is None:\n",
    "            bias_values = [None] * len(linears)\n",
    "        if len(bias_values) != len(linears):\n",
    "            raise ValueError(f\"bias_values for masks: {bias_values} do not match with linear layers: {linears}\")\n",
    "\n",
    "        weight_mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(weight_modes, weight_values, weight_sizes)\n",
    "        ]\n",
    "        bias_mask_configs = [\n",
    "            MaskConfig(mode, value, size) if size is not None else None\n",
    "            for mode, value, size in zip(bias_modes, bias_values, bias_sizes)\n",
    "        ]\n",
    "\n",
    "        self.masked_linears = nn.ModuleList(\n",
    "            [LinearWithMask(linear, weight_mask_config, bias_mask_config)\n",
    "             for linear, weight_mask_config, bias_mask_config \n",
    "             in zip(linears, weight_mask_configs, bias_mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = [linear.weight_mask(linear.linear.weight) \n",
    "                   for linear in self.masked_linears]\n",
    "        # merged_weight = torch.sum(torch.stack(weights), dim=0)\n",
    "        merged_weight = sum(weights)\n",
    "\n",
    "        biases = [\n",
    "            linear.bias_mask(linear.linear.bias) \n",
    "            if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "            else linear.linear.bias for linear in self.masked_linears\n",
    "        ]\n",
    "        \n",
    "        if all(b is None for b in biases):\n",
    "            merged_bias = None\n",
    "        else:\n",
    "            biases = [\n",
    "                b if b is not None\n",
    "                else torch.zeros_like(weights[0][:, 0])\n",
    "                for b in biases\n",
    "            ]\n",
    "            merged_bias = sum(biases)\n",
    "\n",
    "        return nn.functional.linear(x, merged_weight, merged_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8eb68e3-ff6a-46c7-be08-f95e9705a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormWithMask(nn.Module):\n",
    "    def __init__(self, rms_norm: Qwen2RMSNorm, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.rms_norm = rms_norm\n",
    "        self.mask_config = mask_config\n",
    "        if rms_norm.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with RMSNorm, reinitializing...\")\n",
    "        self.mask_config.size = rms_norm.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        masked_weight = self.mask(self.rms_norm.weight)\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.rms_norm.variance_epsilon)\n",
    "        return masked_weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RMSNormsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rms_norms: List[Qwen2RMSNorm],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [rms_norm.weight.shape for rms_norm in rms_norms]\n",
    "        if values is None or len(values) != len(rms_norms):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with RMSNorm layers: {rms_norms}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_rms_norms = nn.ModuleList(\n",
    "            [RMSNormWithMask(rms_norm, mask_config)\n",
    "             for rms_norm, mask_config in zip(rms_norms, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        weights = [rms.mask(rms.rms_norm.weight) for rms in self.masked_rms_norms]\n",
    "        merged_weight = sum(weights)\n",
    "        variance_epsilon = self.masked_rms_norms[0].rms_norm.variance_epsilon\n",
    "        for rms in self.masked_rms_norms:\n",
    "            assert variance_epsilon == rms.rms_norm.variance_epsilon, \"Variance epsilon among models must be consistent\"\n",
    "        \n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "        return merged_weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "240c3b14-7d05-4a09-84c6-12b411f3ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithMask(nn.Module):\n",
    "    def __init__(self, embedding: nn.Embedding, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.mask_config = mask_config\n",
    "        if embedding.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with Embedding, reinitializing...\")\n",
    "        self.mask_config.size = embedding.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        masked_weight = self.mask(self.embedding.weight)\n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            masked_weight,\n",
    "            padding_idx=self.embedding.padding_idx,\n",
    "            max_norm=self.embedding.max_norm,\n",
    "            norm_type=self.embedding.norm_type,\n",
    "            scale_grad_by_freq=self.embedding.scale_grad_by_freq,\n",
    "            sparse=self.embedding.sparse,\n",
    "        )\n",
    "\n",
    "class EmbeddingsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [embedding.weight.shape for embedding in embeddings]\n",
    "        if values is None or len(values) != len(embeddings):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with Embedding layers: {embeddings}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_embeddings = nn.ModuleList(\n",
    "            [EmbeddingWithMask(embedding, mask_config)\n",
    "             for embedding, mask_config in zip(embeddings, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        weights = [emb.mask(emb.embedding.weight) for emb in self.masked_embeddings]\n",
    "        merged_weight = sum(weights)\n",
    "        an_embedding = self.masked_embeddings[0].embedding\n",
    "        for other in self.masked_embeddings:\n",
    "            other_embedding = other.embedding\n",
    "            assert an_embedding.padding_idx == other_embedding.padding_idx\n",
    "            assert an_embedding.max_norm == other_embedding.max_norm\n",
    "            assert an_embedding.norm_type == other_embedding.norm_type\n",
    "            assert an_embedding.scale_grad_by_freq == other_embedding.scale_grad_by_freq\n",
    "            assert an_embedding.sparse == other_embedding.sparse\n",
    "            \n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            merged_weight,\n",
    "            padding_idx=an_embedding.padding_idx,\n",
    "            max_norm=an_embedding.max_norm,\n",
    "            norm_type=an_embedding.norm_type,\n",
    "            scale_grad_by_freq=an_embedding.scale_grad_by_freq,\n",
    "            sparse=an_embedding.sparse,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c979f89-ab78-440f-80c8-157b83014548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.num_models = len(merge_config.model_paths)\n",
    "        self.configs = [\n",
    "            # AutoConfig.from_pretrained(path) \n",
    "            Qwen2Config.from_pretrained(path)\n",
    "            for path in merge_config.model_paths\n",
    "        ]\n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.models = nn.ModuleList([\n",
    "            Qwen2ForCausalLM.from_pretrained(\n",
    "            # AutoModelForCausalLM.from_pretrained(\n",
    "                merge_config.model_paths[i], \n",
    "                config=self.configs[i],\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ) \n",
    "            for i in range(self.num_models)\n",
    "        ])\n",
    "        # self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config, factors):\n",
    "        # dummy_config = copy.deepcopy(self.configs[0])\n",
    "        # dummy_config.update({\"hidden_size\": 1, \"intermediate_size\": 1})\n",
    "        # self.merger = AutoModelForCausalLM.from_config(dummy_config)\n",
    "        self.merger = copy.deepcopy(self.models[0])\n",
    "        place_masks(self.merger, self.models, factors=factors)\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2bbe46e-0b19-48b8-8b3e-e489adbdf0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caa47700-3b99-4630-8a59-0d437799977a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634a263daabe4ebe98b247fe787e599a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698a3f45ddc94a368b7223fa9240606c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99134fb8-068b-4840-b4c6-794b94b859e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merger = merger.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4b98853-8d28-4c26-9d8c-7ab9f5fbe2ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merger.__post_init__(merge_config, factors=[0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bfc6b5f-69c2-4aa8-aaf9-48b10a520737",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5264369e-20c8-4c60-a7ca-356bcf1b4650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nContinue this text: A dog is a cat<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "prompt = \"Continue this text: A dog is a cat\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e82de2e3-d5f8-4d42-81b2-b916e8aeb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dog is indeed a type of animal, specifically a mammal belonging to the Canidae family. Dogs have been domesticated for thousands of years and serve various purposes such as companionship, hunting, herding, protection, and assistance in tasks like guide dogs or service animals.\n",
      "\n",
      "Dogs come in different breeds with varying physical characteristics, sizes, colors, and temperaments. Some common types include:\n",
      "\n",
      "1. Labrador Retrievers - known for their friendly nature, intelligence, and love of water.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.merger, tokenizer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a743fbc-cd0a-4985-9fdb-d2a0ae8cb345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dog is indeed a type of animal, specifically a mammal belonging to the Canidae family. Dogs have been domesticated for thousands of years and serve various purposes such as companionship, hunting, herding, protection, and assistance in tasks like guide dogs or service animals.\n",
      "\n",
      "Dogs come in different breeds with varying physical characteristics, sizes, colors, and temperaments. Some common types include:\n",
      "\n",
      "1. Labrador Retrievers - Known for their friendly nature, intelligence, and love for water.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.models[1], tokenizer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd49f453-5057-4451-83d0-1d3e5b0b5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_with_masks = merger.merger.model.embed_tokens\n",
    "# embedding1 = merger.models[1].model.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3080324-69cf-4023-8d97-1750de78ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# num_embeddings = embedding1.weight.data.shape[0]\n",
    "# device = merger.device\n",
    "# input_ids = torch.randint(0, num_embeddings, (2, 5)).to(device=device)  # Example input_ids\n",
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62fa2792-af53-4dac-bd6f-9ca3db225a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o_merged = embeddings_with_masks(input_ids)\n",
    "# o1 = embedding1(input_ids)\n",
    "# torch.allclose(o_merged, o1, rtol=1e-10, atol=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b053dd57-8e09-400d-8354-989a8c54bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_merged = get_logits(text, merger.merger, tokenizer)\n",
    "logits1 = get_logits(text, merger.models[1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6af0edee-f796-4b43-ac8b-7c307587409e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[14.3750, 11.8750, 12.8125,  ...,  2.4062,  2.4062,  2.4062],\n",
       "          [11.9375, 14.3125, 13.6875,  ...,  2.6875,  2.6875,  2.6875],\n",
       "          [15.0000, 12.9375, 17.5000,  ...,  1.6719,  1.6719,  1.6719],\n",
       "          ...,\n",
       "          [ 8.2500,  8.2500,  6.3750,  ..., -0.9414, -0.9414, -0.9414],\n",
       "          [10.4375, 12.5625,  9.8750,  ...,  1.9766,  1.9766,  1.9766],\n",
       "          [ 8.5625, 10.4375,  7.0938,  ...,  0.0248,  0.0248,  0.0248]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[14.3750, 11.8750, 12.8125,  ...,  2.4062,  2.4062,  2.4062],\n",
       "          [11.9375, 14.3125, 13.6875,  ...,  2.6875,  2.6875,  2.6875],\n",
       "          [15.0000, 12.9375, 17.5000,  ...,  1.6719,  1.6719,  1.6719],\n",
       "          ...,\n",
       "          [ 8.2500,  8.2500,  6.3750,  ..., -0.9414, -0.9414, -0.9414],\n",
       "          [10.4375, 12.5625,  9.8750,  ...,  1.9766,  1.9766,  1.9766],\n",
       "          [ 8.5625, 10.4375,  7.0938,  ...,  0.0248,  0.0248,  0.0248]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_merged, logits1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b58ed39-0aaf-40db-8d96-eb26c2b410ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_merged = get_hidden_states(text, merger.merger, tokenizer)\n",
    "outputs1 = get_hidden_states(text, merger.models[1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e1e4ba2-f1c5-4633-973d-896d985e1e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs_test_merged = model_forward(text, merger.merger.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01a16240-ff0f-4515-aaeb-741705c77864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs_test_1 = model_forward(text, merger.models[1].model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9492ddfc-4f22-47dd-b1d7-ace14c24a2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0, step 1 passed!\n",
      "layer 0, step 2 passed!\n",
      "layer 0, step 3 passed!\n",
      "layer 0, step 4 passed!\n",
      "layer 0, step 5 passed!\n",
      "layer 0, step 6 passed!\n",
      "layer 0, step 7 passed!\n",
      "layer 1, step 1 passed!\n",
      "layer 1, step 2 passed!\n",
      "layer 1, step 3 passed!\n",
      "layer 1, step 4 passed!\n",
      "layer 1, step 5 passed!\n",
      "layer 1, step 6 passed!\n",
      "layer 1, step 7 passed!\n"
     ]
    }
   ],
   "source": [
    "for j, layer_output in enumerate(outputs_test_merged.attentions):\n",
    "    other_output = outputs_test_1.attentions[j]\n",
    "    for i in range(7):\n",
    "        key = f\"step {i+1}\"\n",
    "        if torch.allclose(layer_output[key], other_output[key], atol=0, rtol=0):\n",
    "            print(f\"layer {j}, step {i+1} passed!\")\n",
    "        else:\n",
    "            print(f\"FAIL AT layer {j}, step {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba33b0c0-dec9-48b1-b351-176352c1cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_merged = merger.merger.model.layers[0]\n",
    "decoder_1 = merger.models[1].model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2550e84e-f2ed-489b-8b79-1cc3c2d305e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = init_input(text, merger.merger.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc3b2adf-d55b-4691-a418-73e17ed5687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dec_merged = decoder_merged(\n",
    "    model_inputs['hidden_states'], \n",
    "    position_embeddings=model_inputs[\"position_embeddings\"]\n",
    ")\n",
    "out_dec_1 = decoder_1(\n",
    "    model_inputs['hidden_states'], \n",
    "    position_embeddings=model_inputs[\"position_embeddings\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03deb9b1-06ea-4705-81cf-f40b9bc73fee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not equal!\n\nMismatched elements: 27697 / 57344 (48.3%)\nGreatest absolute difference: 0.03125 at index (0, 2, 1874)\nGreatest relative difference: inf at index (0, 0, 533)\n\nThe failure occurred for item [0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_dec_merged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dec_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/testing/_comparison.py:1530\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1508\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1509\u001b[0m     actual,\n\u001b[1;32m   1510\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1526\u001b[0m )\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not equal!\n\nMismatched elements: 27697 / 57344 (48.3%)\nGreatest absolute difference: 0.03125 at index (0, 2, 1874)\nGreatest relative difference: inf at index (0, 0, 533)\n\nThe failure occurred for item [0]"
     ]
    }
   ],
   "source": [
    "torch.testing.assert_close(out_dec_merged, out_dec_1, atol=0, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "295f82f1-92fb-49aa-ba2a-73c77be96a2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not equal!\n\nMismatched elements: 38040 / 57344 (66.3%)\nGreatest absolute difference: 3.0517578125e-05 at index (0, 1, 71)\nGreatest relative difference: inf at index (0, 1, 926)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m out_mlp_merged \u001b[38;5;241m=\u001b[39m mlp_forward(decoder_merged\u001b[38;5;241m.\u001b[39mmlp, model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m out_mlp_1 \u001b[38;5;241m=\u001b[39m mlp_forward(decoder_1\u001b[38;5;241m.\u001b[39mmlp, model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_mlp_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_mlp_merged\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/testing/_comparison.py:1530\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1508\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1509\u001b[0m     actual,\n\u001b[1;32m   1510\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1526\u001b[0m )\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not equal!\n\nMismatched elements: 38040 / 57344 (66.3%)\nGreatest absolute difference: 3.0517578125e-05 at index (0, 1, 71)\nGreatest relative difference: inf at index (0, 1, 926)"
     ]
    }
   ],
   "source": [
    "out_mlp_merged = mlp_forward(decoder_merged.mlp, model_inputs[\"hidden_states\"])\n",
    "out_mlp_1 = mlp_forward(decoder_1.mlp, model_inputs[\"hidden_states\"])\n",
    "torch.testing.assert_close(out_mlp_1[\"output\"], out_mlp_merged[\"output\"], atol=0, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff490425-2c75-4ec7-818f-77f225f736b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 (input) -> step 1 (gate) -> step 2 (up) -> step 3 (activation) -> step 4 (act_up) -> step 5 (down - output)\n",
      "------------------------------\n",
      "step 0 (input) passed!\n",
      "FAIL AT step 1 (gate)\n",
      "FAIL AT step 2 (up)\n",
      "FAIL AT step 3 (activation)\n",
      "FAIL AT step 4 (act_up)\n",
      "FAIL AT step 5 (down - output)\n"
     ]
    }
   ],
   "source": [
    "step_keys = sorted(out_mlp_merged[\"steps\"].keys())\n",
    "print(\" -> \".join(step_keys))\n",
    "print(\"---\" * 10)\n",
    "for key in step_keys:\n",
    "    tensor_1 = out_mlp_1[\"steps\"][key]\n",
    "    tensor_merged = out_mlp_merged[\"steps\"][key]\n",
    "    if torch.allclose(tensor_1, tensor_merged, atol=0, rtol=0):\n",
    "        print(f\"{key} passed!\")\n",
    "    else:\n",
    "        print(f\"FAIL AT {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac5b7e70-f76a-48d5-9925-220db1ae3bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0179,  0.0153,  0.0164,  ..., -0.0186, -0.0030,  0.0052],\n",
       "         [ 0.0194, -0.0164,  0.0254,  ...,  0.0014, -0.0325,  0.0043],\n",
       "         [ 0.0347, -0.0330, -0.0273,  ...,  0.0630,  0.0035, -0.0332],\n",
       "         ...,\n",
       "         [-0.0179,  0.0153,  0.0164,  ..., -0.0186, -0.0030,  0.0052],\n",
       "         [ 0.0344,  0.0045,  0.0212,  ...,  0.0070,  0.0222,  0.0010],\n",
       "         [ 0.0347, -0.0330, -0.0273,  ...,  0.0630,  0.0035, -0.0332]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_1.mlp.gate_proj(model_inputs[\"hidden_states\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14487826-6d30-4f24-bc90-4dbf8fef5da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0179,  0.0153,  0.0162,  ..., -0.0186, -0.0031,  0.0052],\n",
       "         [ 0.0194, -0.0164,  0.0254,  ...,  0.0014, -0.0325,  0.0042],\n",
       "         [ 0.0349, -0.0327, -0.0273,  ...,  0.0630,  0.0035, -0.0332],\n",
       "         ...,\n",
       "         [-0.0179,  0.0153,  0.0162,  ..., -0.0186, -0.0031,  0.0052],\n",
       "         [ 0.0344,  0.0045,  0.0212,  ...,  0.0070,  0.0221,  0.0011],\n",
       "         [ 0.0349, -0.0327, -0.0273,  ...,  0.0630,  0.0035, -0.0332]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_merged.mlp.gate_proj(model_inputs[\"hidden_states\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "262b2b55-e7ed-4491-8373-aa0a112361d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdecoder_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Define the computation performed at every call.\n",
       "\n",
       "Should be overridden by all subclasses.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within\n",
       "    this function, one should call the :class:`Module` instance afterwards\n",
       "    instead of this since the former takes care of running the\n",
       "    registered hooks while the latter silently ignores them.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                   \u001b[0;32mfor\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_linears\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# merged_weight = torch.sum(torch.stack(weights), dim=0)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmerged_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_linears\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# merged_bias = torch.sum(torch.stack(biases))\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmerged_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mmerged_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_2646984/2665586263.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder_merged.mlp.gate_proj.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fcf7af87-3b4a-42c1-b5bb-384220fc5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_and_bias(masked):\n",
    "    weights = [linear.weight_mask(linear.linear.weight) \n",
    "               for linear in masked.masked_linears]\n",
    "    # merged_weight = torch.sum(torch.stack(weights), dim=0)\n",
    "    merged_weight = sum(weights)\n",
    "    biases = [\n",
    "        linear.bias_mask(linear.linear.bias) \n",
    "        if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "        else linear.linear.bias for linear in masked.masked_linears\n",
    "    ]\n",
    "    if all(b is None for b in biases):\n",
    "        merged_bias = None\n",
    "    else:\n",
    "        biases = [\n",
    "            b if b is not None\n",
    "            else torch.zeros_like(weights[0][:, 0])\n",
    "            for b in biases\n",
    "        ]\n",
    "        merged_bias = sum(biases)\n",
    "\n",
    "\n",
    "    return dict(\n",
    "        weight=merged_weight,\n",
    "        bias=merged_bias\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c800eb8-2428-4431-9fb5-7d5a87692910",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_masked_linears_params = get_weight_and_bias(decoder_merged.mlp.gate_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc663190-bd02-4644-b6d2-b2dda5ad682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(a_masked_linears_params[\"weight\"], decoder_1.mlp.gate_proj.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b44224d9-80ef-436a-823e-a4a546dbdd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert a_masked_linears_params[\"bias\"] == decoder_1.mlp.gate_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0e83c78-b991-4906-ae0e-b7e0be8ca6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0fe5958-2e8b-42c0-a304-838bc27ec04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bounty = \"\"\"\n",
    "Damn there is something wrong at step 6 decoder.\n",
    "Which is the MLP :D\n",
    "I'll fix it tmr.\n",
    "-------------------\n",
    "The error is fixed now. nn.Linear is kinda weird, which results in\n",
    "some number instability. In short, bias=torch.zeros_like() is different\n",
    "from bias=None when initializing an nn.Linear.\n",
    "\n",
    "In specific, in older implementations, I handle biases in LinearsWithMasks \n",
    "like this:\n",
    "```\n",
    "def forward(...):\n",
    "    ...\n",
    "    biases = [\n",
    "        linear.bias_mask(linear.linear.bias) \n",
    "        if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "        else linear.linear.bias for linear in self.masked_linears\n",
    "    ]\n",
    "    biases = [\n",
    "        b if b is not None\n",
    "        else torch.zeros_like(weights[0][:, 0])\n",
    "        for b in biases\n",
    "    ]\n",
    "    # merged_bias = torch.sum(torch.stack(biases))\n",
    "    merged_bias = sum(biases)\n",
    "    if all(b is None for b in biases):\n",
    "        merged_bias = None\n",
    "    ...\n",
    "```\n",
    "This implementation is not accurate as it will always assign torch.zeros_like()\n",
    "to biases when they are supposed to be None, making the last `if` redundant.\n",
    "\n",
    "A quick and accurate fix should be:\n",
    "```\n",
    "def forward(...):\n",
    "    ...\n",
    "    biases = [\n",
    "        linear.bias_mask(linear.linear.bias) \n",
    "        if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "        else linear.linear.bias for linear in self.masked_linears\n",
    "    ]\n",
    "    \n",
    "    if all(b is None for b in biases):\n",
    "        merged_bias = None\n",
    "    else:\n",
    "        biases = [\n",
    "            b if b is not None\n",
    "            else torch.zeros_like(weights[0][:, 0])\n",
    "            for b in biases\n",
    "        ]\n",
    "        merged_bias = sum(biases)\n",
    "    ...\n",
    "```\n",
    "Yeah, this is it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70938add-6ce9-40e9-a4a6-a10ad06ddae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_strategy = \"\"\"\n",
    "One of the cool things I figured out myself is how to do surgeon to class methods.\n",
    "Instead of having to re-implement a class with a modified method, like adding\n",
    "logging statements between lines of codes.\n",
    "\n",
    "For example, with the class Qwen2MLP, instead of writing a new class Qwen2MLPDebug\n",
    "with a new forward method when I want to inspect intermediate steps within the \n",
    "forward method of Qwen2MLP, I could just write a dedicated function for debugging:\n",
    "\n",
    "def mlp_forward(mlp, *args, **kwargs):\n",
    "    ```\n",
    "    this function is a copy of Qwen2MLP.forward, but:\n",
    "    - the input and output signatures largely reflect what Qwen2MLP.forward has,\n",
    "    however, I can plant additional debugging flags and logging statements.\n",
    "    - change `self` to `mlp`. not world-changing. you can also decide to let \n",
    "    `self` in the mlp_forward's input signature, which will save you some minutes\n",
    "    from replacing `self` in Qwen2MLP.forward to `mlp` like I do.\n",
    "    ```\n",
    "    ...\n",
    "\n",
    "This function is exactly the Qwen2MLP.forward method, however I replace self -> mlp,\n",
    "with mlp is an instance of Qwen2MLP. I know I know this is obvious, but by applying \n",
    "the pattern for debugging, it saves so much time (do not have write a class from \n",
    "scratch and import everything), ensures correctness (it's a copy of an established \n",
    "class method duh), and most importantly leaves the existing classes and methods \n",
    "intact (extremely helpful when experimenting, because I don't have to restart \n",
    "everything in my notebook to reflect a minor change in my debugging code, which \n",
    "would have been the case if I decided to implement a Qwen2MLPDebug class).\n",
    "-----------\n",
    "In this notebook, I've implemented 3 debugging function with the principle above,\n",
    "which has helped me localize the bug causing inconsistencies between outputs of\n",
    "merged_model = merge(models=[model1, model2], factors=[1.0, 0.0]) and model1.\n",
    "These two outputs should be identical, but due to a weird feature of torch, this \n",
    "properties did not hold.\n",
    "\n",
    "In retrorespect, here is what happened:\n",
    "- Inspected logits of `merged_model` and `model1`, expected them to be identical.\n",
    "  They did not.\n",
    "- Implemented mlp_forward to inspect at what part of Qwen2ForCausalLM's forward pass \n",
    "did the intermediate states diverged between two models.\n",
    "- Outputs of embed_tokens layer OKAY, they were identical.\n",
    "- Outputs of the first decoder layer failed.\n",
    "- Implemented decoder_forward to insptect at what step it failed (input norm, \n",
    "two skip connections, self attn, mlp, or output norm)\n",
    "- It was mlp that failed the consistency test. What happened? I did unittests at \n",
    "`debug_1.ipynb`, might missed something.\n",
    "- Implemented mlp_forward.\n",
    "- It was the gate_proj and up_proj that failed, both of which are nn.Linear with no\n",
    "bias. This indicated that my LinearsWithMasks implementation was not correct.\n",
    "- It was indeed not correct. Having inspected further, I found that gate_proj of a\n",
    "MLP within the merged_model had bias=torch.zeros_like(), while it should be None \n",
    "instead. Theoretically W * X should be IDENTICAL to W * X + torch.zeros_like(n_columns).\n",
    "But it's not. This weird fuck bug.\n",
    "- Fixed the forward pass within the LinearsWithMasks class. Everything OKAY now. \n",
    "Fully functional scalar merging.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
