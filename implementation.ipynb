{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cdad09-d3c0-45e8-8f7d-1e73cac721b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from modeling_qwen2 import (\n",
    "    Qwen2RMSNorm, \n",
    "    Qwen2RotaryEmbedding, \n",
    "    Qwen2MLP, \n",
    "    Qwen2Attention, \n",
    "    Qwen2FlashAttention2, \n",
    "    Qwen2SdpaAttention, \n",
    "    Qwen2DecoderLayer, \n",
    "    Qwen2PreTrainedModel, \n",
    "    Qwen2Model, \n",
    "    Qwen2ForCausalLM,\n",
    "    # logger\n",
    ")\n",
    "\n",
    "from configuration_qwen2 import Qwen2Config\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    ")\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b39d8d-c4dd-4231-89a9-6422aa0de49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import are_tokenizers_same\n",
    "# are_tokenizers_same(\n",
    "#     paths = [\n",
    "#         \"/workspace/models/Arcee-VyLinh/\",\n",
    "#         \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbb9c5f-c766-441f-80f7-c37014ffd078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer(path, layer_idx=33):\n",
    "\tstate_dict = {}\n",
    "\tshard_paths = [f for f in os.listdir(path) if f.endswith('.safetensors')]\n",
    "\tfor shard_path in sorted(shard_paths, key=lambda x: int(x.split('-')[1])):\n",
    "\t\tapath = os.path.join(path, shard_path)\n",
    "\t\twith safe_open(apath, framework=\"pt\", device=\"cpu\") as f:\n",
    "\t\t\tfor key in f.keys():\n",
    "\t\t\t\tif f\"layers.{str(layer_idx)}.\" in key:\n",
    "\t\t\t\t\tstate_dict[key] = f.get_tensor(key)\n",
    "\treturn state_dict\n",
    "\n",
    "def strip_prefix(state_dict, prefix=\"model.layers.\"):\n",
    "    \"\"\"Strips 'model.layers.*.' prefix from 'input_layernorm.weight' keys.\"\"\"\n",
    "    return {\n",
    "      k.replace(f\"{prefix}{k.split('.')[2]}.\", \"\") if k.startswith(prefix)\n",
    "      else k: v for k, v in state_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9560a0b5-7194-4096-8a2c-5d4bddeee216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(\n",
    "    t: float, v0: Union[np.ndarray, torch.Tensor], v1: Union[np.ndarray, torch.Tensor]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    return (1 - t) * v0 + t * v1\n",
    "\n",
    "def weighted_sum(\n",
    "    factors: List[float], \n",
    "    tensors: Union[List[np.ndarray], List[torch.Tensor]]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    result = 0.0\n",
    "    # for factor, tensor in zip(factors, tensors):\n",
    "    #     result += factor * tensor\n",
    "    return sum([tensor * factor for tensor, factor in zip(tensors, factors)])\n",
    "\n",
    "def merge_modules(modules, factors):\n",
    "    \"\"\"\n",
    "    This is only applicable for cases where a static set of scalars\n",
    "    playing as merging factor for every submodules of the passed module.\n",
    "    Not recommend for fine-grained usecases.\n",
    "    \"\"\"\n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out\n",
    "\n",
    "def merge_linears(modules, weight_factors, bias_factors):\n",
    "    param_names = sorted([name for name, _ in modules[0].named_parameters()])\n",
    "    for module in modules:\n",
    "        other_param_names = sorted([name for name, _ in module.named_parameters()])\n",
    "        assert param_names == other_param_names, \"Mismatch tensor names.\"\n",
    "        \n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        if \"weight\" in tensor_name:\n",
    "            factors = weight_factors\n",
    "        elif \"bias\" in tensor_name:\n",
    "            factors = bias_factors\n",
    "        else:\n",
    "            raise ValueError(\"Hey this tensor is neither weight or bias.\")\n",
    "            \n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8d01ac-aef2-440c-86bc-7accd0932b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_masks(target_module, ref_modules, factors):\n",
    "    \"\"\"\n",
    "    Recursively replaces normal components with masked components.\n",
    "    \n",
    "    Args:\n",
    "      module: The module in which to replace layers.\n",
    "    \"\"\"\n",
    "    assert len(ref_modules) == len(factors)\n",
    "    num_components = len(ref_modules)\n",
    "    for name, target_child in target_module.named_children():\n",
    "        ref_children = [getattr(module, name) for module in ref_modules]\n",
    "        modes = [\"scalar\" for _ in ref_children]\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            new_module = LinearsWithMasks(\n",
    "                linears=ref_children,\n",
    "                weight_modes=[\"scalar\"] * num_components,\n",
    "                weight_values=factors,\n",
    "                bias_modes=[\"scalar\"] * num_components,\n",
    "                bias_values=factors,\n",
    "            )\n",
    "            setattr(target_module, name, new_module)\n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            setattr(target_module, name, EmbeddingsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "        elif type(target_child).__name__ == Qwen2RMSNorm.__name__:\n",
    "            # print(\"Hehe placing masks to a cutie RMSNorm\")\n",
    "            setattr(target_module, name, RMSNormsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "        else:\n",
    "            place_masks(target_child, ref_children, factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79dd85d4-c7e9-4d62-9813-5c590b63a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, model, tokenizer, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()\n",
    "\n",
    "def get_logits(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_ids).logits\n",
    "    return logits\n",
    "\n",
    "def get_hidden_states(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids, output_hidden_states=True, use_cache=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdce16a2-1565-4d6e-8506-8972d34271f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_input(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    past_key_values = None\n",
    "    cache_position = None\n",
    "    position_ids = None\n",
    "    output_hidden_states = True\n",
    "    output_attentions = False\n",
    "    use_cache = False\n",
    "    return_dict = True\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        return_legacy_cache = False\n",
    "        inputs_embeds = model.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = model._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "    return dict(\n",
    "        hidden_states=hidden_states,\n",
    "        attention_mask=causal_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=past_key_values,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "        cache_position=cache_position,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c25f7fa-f0b7-4ecc-903f-c072d016813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(mlp, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    ref: self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n",
    "    \"\"\"\n",
    "    steps = {}\n",
    "    steps.update({\"step 0 (input)\": x})\n",
    "    \n",
    "    gate = mlp.gate_proj(x)\n",
    "    steps.update({\"step 1 (gate)\": gate})\n",
    "    \n",
    "    up = mlp.up_proj(x)\n",
    "    steps.update({\"step 2 (up)\": up})\n",
    "    \n",
    "    act = mlp.act_fn(gate) # The activation function should be applied to the gate projection\n",
    "    steps.update({\"step 3 (activation)\": act})\n",
    "    \n",
    "    act_up = act * up  # Multiply the activated gate with the up projection\n",
    "    steps.update({\"step 4 (act_up)\": act_up})\n",
    "\n",
    "    down = mlp.down_proj(act_up) # Apply the down projection to the result of act * up\n",
    "    steps.update({\"step 5 (down - output)\": down})\n",
    "    \n",
    "    return dict(\n",
    "        output=down,\n",
    "        steps=steps\n",
    "    )\n",
    "\n",
    "# def mlp_forward(self, hidden_state):\n",
    "#     return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad60458-d15d-4392-85d8-03fda6ebcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward(\n",
    "    decoder,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "    output_attentions: Optional[bool] = False,\n",
    "    use_cache: Optional[bool] = False,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "\n",
    "    steps = {}\n",
    "    # logger.warning(f\"-------- Logging hidden_states in decoder forward:\")\n",
    "    residual = hidden_states\n",
    "    # logger.warning(f\" hidden_states step 1 (as input): {hidden_states}\")\n",
    "    steps.update({\"step 1\": hidden_states})\n",
    "\n",
    "    hidden_states = decoder.input_layernorm(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 2 (after input_layernorm): {hidden_states}\")\n",
    "    steps.update({\"step 2\": hidden_states})\n",
    "    # Self Attention\n",
    "    hidden_states, self_attn_weights, present_key_value = decoder.self_attn(\n",
    "        hidden_states=hidden_states,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=past_key_value,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "        cache_position=cache_position,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )\n",
    "    # logger.warning(f\" hidden_states step 3 (after self_attn): {hidden_states}\")\n",
    "    steps.update({\"step 3\": hidden_states})\n",
    "    \n",
    "    hidden_states = residual + hidden_states\n",
    "    # logger.warning(f\" hidden_states step 4 (after first skip connection): {hidden_states}\")\n",
    "    steps.update({\"step 4\": hidden_states})\n",
    "    # Fully Connected\n",
    "    residual = hidden_states\n",
    "    hidden_states = decoder.post_attention_layernorm(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 5 (after post_attention_layernorm): {hidden_states}\")\n",
    "    steps.update({\"step 5\": hidden_states})\n",
    "    \n",
    "    hidden_states = decoder.mlp(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 6 (after mlp): {hidden_states}\")\n",
    "    steps.update({\"step 6\": hidden_states})\n",
    "    \n",
    "    hidden_states = residual + hidden_states\n",
    "    # logger.warning(f\" hidden_states step 7 (after second skip connection): {hidden_states}\")\n",
    "    steps.update({\"step 7\": hidden_states})\n",
    "\n",
    "    outputs = (hidden_states,)\n",
    "\n",
    "    if output_attentions:\n",
    "        outputs += (self_attn_weights,)\n",
    "\n",
    "    if use_cache:\n",
    "        outputs += (present_key_value,)\n",
    "\n",
    "    outputs += (steps,)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835816e2-51ff-407d-bc01-7508c55426fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    past_key_values = None\n",
    "    cache_position = None\n",
    "    position_ids = None\n",
    "    output_hidden_states = True\n",
    "    output_attentions = False\n",
    "    use_cache = False\n",
    "    return_dict = True\n",
    "    #############\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # kept for BC (non `Cache` `past_key_values` inputs)\n",
    "        return_legacy_cache = False\n",
    "        inputs_embeds = model.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = model._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "        all_decoder_steps = ()\n",
    "\n",
    "        for i, decoder_layer in enumerate(model.layers[:2]):   \n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "          \n",
    "            layer_outputs = decoder_forward(\n",
    "                decoder_layer,\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            steps = layer_outputs[-1]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "            all_decoder_steps += (steps,)\n",
    "\n",
    "        hidden_states = model.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if return_legacy_cache:\n",
    "            next_cache = next_cache.to_legacy_cache()\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=(),\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_decoder_steps,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5a8d8-eef0-4dfd-b26d-8a27d0aed4e5",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b5bbb8-1a19-4aa7-9d3b-8e6dfc628d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.weight = nn.Parameter(torch.tensor(value))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        if self.size is not None:\n",
    "            try:\n",
    "                self.weight * torch.rand(self.size)\n",
    "            except RuntimeError:\n",
    "                print(\"mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Be really careful here (though I do not think it matters that much),\n",
    "        When testing, it's important that the masking operation is implemented\n",
    "        with `x = self.weight * x` instead of `x = x * self.weight`.\n",
    "\n",
    "        Neither of those two implementation is superior, however I need to be\n",
    "        consistent when doing testing because the phenonmenon above could lead\n",
    "        to some number imprecision, which may fail `torch.testing.assert_close`\n",
    "        \"\"\"\n",
    "        if self.size is None:\n",
    "            return self.weight * x\n",
    "        else:\n",
    "            if self.size != x.shape:\n",
    "                print(\"The shape of input does not match that of the mask.\")\n",
    "            return self.weight * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce5f133-2e26-4855-89c4-10853bd7de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithMask(nn.Module):\n",
    "    def __init__(self, linear, weight_mask_config: MaskConfig, bias_mask_config: MaskConfig = None):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.weight_mask_config = weight_mask_config\n",
    "        self.bias_mask_config = bias_mask_config\n",
    "\n",
    "        if linear.weight.shape != weight_mask_config.size:\n",
    "            print(\"Weight mask shape is not compatible with linear, reinitializing...\")\n",
    "            self.weight_mask_config.size = linear.weight.shape\n",
    "        self.weight_mask = Mask(self.weight_mask_config)\n",
    "\n",
    "        if linear.bias is not None and bias_mask_config is not None:\n",
    "            if linear.bias.shape != bias_mask_config.size:\n",
    "                print(\"Bias mask shape is not compatible with linear, reinitializing...\")\n",
    "                self.bias_mask_config.size = linear.bias.shape\n",
    "            self.bias_mask = Mask(self.bias_mask_config)\n",
    "        else:\n",
    "            self.bias_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        masked_weight = self.weight_mask(self.linear.weight)\n",
    "        if self.linear.bias is not None and self.bias_mask is not None:\n",
    "            masked_bias = self.bias_mask(self.linear.bias)\n",
    "        else:\n",
    "            masked_bias = self.linear.bias\n",
    "        return nn.functional.linear(x, masked_weight, masked_bias)\n",
    "\n",
    "class LinearsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        linears: List[nn.Module],\n",
    "        weight_modes: List[str] = [\"scalar\"],\n",
    "        weight_values: List[float] = None,\n",
    "        bias_modes: List[str] = [\"scalar\"],\n",
    "        bias_values: List[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not all(isinstance(linear, nn.Linear) for linear in linears):\n",
    "            raise ValueError(\"All elements in 'linears' must be instances of nn.Linear.\")\n",
    "\n",
    "        weight_sizes = [linear.weight.shape for linear in linears]\n",
    "        bias_sizes = [linear.bias.shape if linear.bias is not None else None for linear in linears]\n",
    "        \n",
    "        if weight_values is None or len(weight_values) != len(linears):\n",
    "            raise ValueError(f\"weight_values for masks: {weight_values} do not match with linear layers: {linears}\")\n",
    "        if bias_values is None:\n",
    "            bias_values = [None] * len(linears)\n",
    "        if len(bias_values) != len(linears):\n",
    "            raise ValueError(f\"bias_values for masks: {bias_values} do not match with linear layers: {linears}\")\n",
    "\n",
    "        weight_mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(weight_modes, weight_values, weight_sizes)\n",
    "        ]\n",
    "        bias_mask_configs = [\n",
    "            MaskConfig(mode, value, size) if size is not None else None\n",
    "            for mode, value, size in zip(bias_modes, bias_values, bias_sizes)\n",
    "        ]\n",
    "\n",
    "        self.masked_linears = nn.ModuleList(\n",
    "            [LinearWithMask(linear, weight_mask_config, bias_mask_config)\n",
    "             for linear, weight_mask_config, bias_mask_config \n",
    "             in zip(linears, weight_mask_configs, bias_mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = [linear.weight_mask(linear.linear.weight) \n",
    "                   for linear in self.masked_linears]\n",
    "        # merged_weight = torch.sum(torch.stack(weights), dim=0)\n",
    "        merged_weight = sum(weights)\n",
    "\n",
    "        biases = [\n",
    "            linear.bias_mask(linear.linear.bias) \n",
    "            if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "            else linear.linear.bias for linear in self.masked_linears\n",
    "        ]\n",
    "        \n",
    "        if all(b is None for b in biases):\n",
    "            merged_bias = None\n",
    "        else:\n",
    "            biases = [\n",
    "                b if b is not None\n",
    "                else torch.zeros_like(weights[0][:, 0])\n",
    "                for b in biases\n",
    "            ]\n",
    "            merged_bias = sum(biases)\n",
    "\n",
    "        return nn.functional.linear(x, merged_weight, merged_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8eb68e3-ff6a-46c7-be08-f95e9705a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormWithMask(nn.Module):\n",
    "    def __init__(self, rms_norm: Qwen2RMSNorm, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.rms_norm = rms_norm\n",
    "        self.mask_config = mask_config\n",
    "        if rms_norm.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with RMSNorm, reinitializing...\")\n",
    "        self.mask_config.size = rms_norm.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        masked_weight = self.mask(self.rms_norm.weight)\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.rms_norm.variance_epsilon)\n",
    "        return masked_weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RMSNormsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rms_norms: List[Qwen2RMSNorm],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [rms_norm.weight.shape for rms_norm in rms_norms]\n",
    "        if values is None or len(values) != len(rms_norms):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with RMSNorm layers: {rms_norms}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_rms_norms = nn.ModuleList(\n",
    "            [RMSNormWithMask(rms_norm, mask_config)\n",
    "             for rms_norm, mask_config in zip(rms_norms, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        weights = [rms.mask(rms.rms_norm.weight) for rms in self.masked_rms_norms]\n",
    "        merged_weight = sum(weights)\n",
    "        variance_epsilon = self.masked_rms_norms[0].rms_norm.variance_epsilon\n",
    "        for rms in self.masked_rms_norms:\n",
    "            assert variance_epsilon == rms.rms_norm.variance_epsilon, \"Variance epsilon among models must be consistent\"\n",
    "        \n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "        return merged_weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "240c3b14-7d05-4a09-84c6-12b411f3ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithMask(nn.Module):\n",
    "    def __init__(self, embedding: nn.Embedding, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.mask_config = mask_config\n",
    "        if embedding.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with Embedding, reinitializing...\")\n",
    "        self.mask_config.size = embedding.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        masked_weight = self.mask(self.embedding.weight)\n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            masked_weight,\n",
    "            padding_idx=self.embedding.padding_idx,\n",
    "            max_norm=self.embedding.max_norm,\n",
    "            norm_type=self.embedding.norm_type,\n",
    "            scale_grad_by_freq=self.embedding.scale_grad_by_freq,\n",
    "            sparse=self.embedding.sparse,\n",
    "        )\n",
    "\n",
    "class EmbeddingsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [embedding.weight.shape for embedding in embeddings]\n",
    "        if values is None or len(values) != len(embeddings):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with Embedding layers: {embeddings}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_embeddings = nn.ModuleList(\n",
    "            [EmbeddingWithMask(embedding, mask_config)\n",
    "             for embedding, mask_config in zip(embeddings, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        weights = [emb.mask(emb.embedding.weight) for emb in self.masked_embeddings]\n",
    "        merged_weight = sum(weights)\n",
    "        an_embedding = self.masked_embeddings[0].embedding\n",
    "        for other in self.masked_embeddings:\n",
    "            other_embedding = other.embedding\n",
    "            assert an_embedding.padding_idx == other_embedding.padding_idx\n",
    "            assert an_embedding.max_norm == other_embedding.max_norm\n",
    "            assert an_embedding.norm_type == other_embedding.norm_type\n",
    "            assert an_embedding.scale_grad_by_freq == other_embedding.scale_grad_by_freq\n",
    "            assert an_embedding.sparse == other_embedding.sparse\n",
    "            \n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            merged_weight,\n",
    "            padding_idx=an_embedding.padding_idx,\n",
    "            max_norm=an_embedding.max_norm,\n",
    "            norm_type=an_embedding.norm_type,\n",
    "            scale_grad_by_freq=an_embedding.scale_grad_by_freq,\n",
    "            sparse=an_embedding.sparse,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c979f89-ab78-440f-80c8-157b83014548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.num_models = len(merge_config.model_paths)\n",
    "        self.configs = [\n",
    "            # AutoConfig.from_pretrained(path) \n",
    "            Qwen2Config.from_pretrained(path)\n",
    "            for path in merge_config.model_paths\n",
    "        ]\n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.models = nn.ModuleList([\n",
    "            Qwen2ForCausalLM.from_pretrained(\n",
    "            # AutoModelForCausalLM.from_pretrained(\n",
    "                merge_config.model_paths[i], \n",
    "                config=self.configs[i],\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ) \n",
    "            for i in range(self.num_models)\n",
    "        ])\n",
    "        # self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config, factors):\n",
    "        # dummy_config = copy.deepcopy(self.configs[0])\n",
    "        # dummy_config.update({\"hidden_size\": 1, \"intermediate_size\": 1})\n",
    "        # self.merger = AutoModelForCausalLM.from_config(dummy_config)\n",
    "        self.merger = copy.deepcopy(self.models[0])\n",
    "        place_masks(self.merger, self.models, factors=factors)\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2bbe46e-0b19-48b8-8b3e-e489adbdf0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caa47700-3b99-4630-8a59-0d437799977a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634a263daabe4ebe98b247fe787e599a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698a3f45ddc94a368b7223fa9240606c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99134fb8-068b-4840-b4c6-794b94b859e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merger = merger.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4b98853-8d28-4c26-9d8c-7ab9f5fbe2ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merger.__post_init__(merge_config, factors=[0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bfc6b5f-69c2-4aa8-aaf9-48b10a520737",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5264369e-20c8-4c60-a7ca-356bcf1b4650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nContinue this text: A dog is a cat<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "prompt = \"Continue this text: A dog is a cat\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e82de2e3-d5f8-4d42-81b2-b916e8aeb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dog is indeed a type of animal, specifically a mammal belonging to the Canidae family. Dogs have been domesticated for thousands of years and serve various purposes such as companionship, hunting, herding, protection, and assistance in tasks like guide dogs or service animals.\n",
      "\n",
      "Dogs come in different breeds with varying physical characteristics, sizes, colors, and temperaments. Some common types include:\n",
      "\n",
      "1. Labrador Retrievers - known for their friendly nature, intelligence, and love of water.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.merger, tokenizer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a743fbc-cd0a-4985-9fdb-d2a0ae8cb345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dog is indeed a type of animal, specifically a mammal belonging to the Canidae family. Dogs have been domesticated for thousands of years and serve various purposes such as companionship, hunting, herding, protection, and assistance in tasks like guide dogs or service animals.\n",
      "\n",
      "Dogs come in different breeds with varying physical characteristics, sizes, colors, and temperaments. Some common types include:\n",
      "\n",
      "1. Labrador Retrievers - Known for their friendly nature, intelligence, and love for water.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.models[1], tokenizer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd49f453-5057-4451-83d0-1d3e5b0b5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_with_masks = merger.merger.model.embed_tokens\n",
    "# embedding1 = merger.models[1].model.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3080324-69cf-4023-8d97-1750de78ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# num_embeddings = embedding1.weight.data.shape[0]\n",
    "# device = merger.device\n",
    "# input_ids = torch.randint(0, num_embeddings, (2, 5)).to(device=device)  # Example input_ids\n",
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62fa2792-af53-4dac-bd6f-9ca3db225a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o_merged = embeddings_with_masks(input_ids)\n",
    "# o1 = embedding1(input_ids)\n",
    "# torch.allclose(o_merged, o1, rtol=1e-10, atol=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b053dd57-8e09-400d-8354-989a8c54bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_merged = get_logits(text, merger.merger, tokenizer)\n",
    "logits1 = get_logits(text, merger.models[1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6af0edee-f796-4b43-ac8b-7c307587409e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[14.3750, 11.8750, 12.8125,  ...,  2.4062,  2.4062,  2.4062],\n",
       "          [11.9375, 14.3125, 13.6875,  ...,  2.6875,  2.6875,  2.6875],\n",
       "          [15.0000, 12.9375, 17.5000,  ...,  1.6719,  1.6719,  1.6719],\n",
       "          ...,\n",
       "          [ 8.2500,  8.2500,  6.3750,  ..., -0.9414, -0.9414, -0.9414],\n",
       "          [10.4375, 12.5625,  9.8750,  ...,  1.9766,  1.9766,  1.9766],\n",
       "          [ 8.5625, 10.4375,  7.0938,  ...,  0.0248,  0.0248,  0.0248]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[14.3750, 11.8750, 12.8125,  ...,  2.4062,  2.4062,  2.4062],\n",
       "          [11.9375, 14.3125, 13.6875,  ...,  2.6875,  2.6875,  2.6875],\n",
       "          [15.0000, 12.9375, 17.5000,  ...,  1.6719,  1.6719,  1.6719],\n",
       "          ...,\n",
       "          [ 8.2500,  8.2500,  6.3750,  ..., -0.9414, -0.9414, -0.9414],\n",
       "          [10.4375, 12.5625,  9.8750,  ...,  1.9766,  1.9766,  1.9766],\n",
       "          [ 8.5625, 10.4375,  7.0938,  ...,  0.0248,  0.0248,  0.0248]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_merged, logits1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b58ed39-0aaf-40db-8d96-eb26c2b410ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_merged = get_hidden_states(text, merger.merger, tokenizer)\n",
    "outputs1 = get_hidden_states(text, merger.models[1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e1e4ba2-f1c5-4633-973d-896d985e1e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs_test_merged = model_forward(text, merger.merger.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01a16240-ff0f-4515-aaeb-741705c77864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs_test_1 = model_forward(text, merger.models[1].model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9492ddfc-4f22-47dd-b1d7-ace14c24a2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0, step 1 passed!\n",
      "layer 0, step 2 passed!\n",
      "layer 0, step 3 passed!\n",
      "layer 0, step 4 passed!\n",
      "layer 0, step 5 passed!\n",
      "layer 0, step 6 passed!\n",
      "layer 0, step 7 passed!\n",
      "layer 1, step 1 passed!\n",
      "layer 1, step 2 passed!\n",
      "layer 1, step 3 passed!\n",
      "layer 1, step 4 passed!\n",
      "layer 1, step 5 passed!\n",
      "layer 1, step 6 passed!\n",
      "layer 1, step 7 passed!\n"
     ]
    }
   ],
   "source": [
    "for j, layer_output in enumerate(outputs_test_merged.attentions):\n",
    "    other_output = outputs_test_1.attentions[j]\n",
    "    for i in range(7):\n",
    "        key = f\"step {i+1}\"\n",
    "        if torch.allclose(layer_output[key], other_output[key], atol=0, rtol=0):\n",
    "            print(f\"layer {j}, step {i+1} passed!\")\n",
    "        else:\n",
    "            print(f\"FAIL AT layer {j}, step {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba33b0c0-dec9-48b1-b351-176352c1cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_merged = merger.merger.model.layers[0]\n",
    "decoder_1 = merger.models[1].model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2550e84e-f2ed-489b-8b79-1cc3c2d305e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = init_input(text, merger.merger.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc3b2adf-d55b-4691-a418-73e17ed5687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dec_merged = decoder_merged(\n",
    "    model_inputs['hidden_states'], \n",
    "    position_embeddings=model_inputs[\"position_embeddings\"]\n",
    ")\n",
    "out_dec_1 = decoder_1(\n",
    "    model_inputs['hidden_states'], \n",
    "    position_embeddings=model_inputs[\"position_embeddings\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03deb9b1-06ea-4705-81cf-f40b9bc73fee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not equal!\n\nMismatched elements: 27697 / 57344 (48.3%)\nGreatest absolute difference: 0.03125 at index (0, 2, 1874)\nGreatest relative difference: inf at index (0, 0, 533)\n\nThe failure occurred for item [0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_dec_merged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dec_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/testing/_comparison.py:1530\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1508\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1509\u001b[0m     actual,\n\u001b[1;32m   1510\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1526\u001b[0m )\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not equal!\n\nMismatched elements: 27697 / 57344 (48.3%)\nGreatest absolute difference: 0.03125 at index (0, 2, 1874)\nGreatest relative difference: inf at index (0, 0, 533)\n\nThe failure occurred for item [0]"
     ]
    }
   ],
   "source": [
    "torch.testing.assert_close(out_dec_merged, out_dec_1, atol=0, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "295f82f1-92fb-49aa-ba2a-73c77be96a2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not equal!\n\nMismatched elements: 38040 / 57344 (66.3%)\nGreatest absolute difference: 3.0517578125e-05 at index (0, 1, 71)\nGreatest relative difference: inf at index (0, 1, 926)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m out_mlp_merged \u001b[38;5;241m=\u001b[39m mlp_forward(decoder_merged\u001b[38;5;241m.\u001b[39mmlp, model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m out_mlp_1 \u001b[38;5;241m=\u001b[39m mlp_forward(decoder_1\u001b[38;5;241m.\u001b[39mmlp, model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_mlp_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_mlp_merged\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/testing/_comparison.py:1530\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1508\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1509\u001b[0m     actual,\n\u001b[1;32m   1510\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1526\u001b[0m )\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not equal!\n\nMismatched elements: 38040 / 57344 (66.3%)\nGreatest absolute difference: 3.0517578125e-05 at index (0, 1, 71)\nGreatest relative difference: inf at index (0, 1, 926)"
     ]
    }
   ],
   "source": [
    "out_mlp_merged = mlp_forward(decoder_merged.mlp, model_inputs[\"hidden_states\"])\n",
    "out_mlp_1 = mlp_forward(decoder_1.mlp, model_inputs[\"hidden_states\"])\n",
    "torch.testing.assert_close(out_mlp_1[\"output\"], out_mlp_merged[\"output\"], atol=0, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff490425-2c75-4ec7-818f-77f225f736b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 (input) -> step 1 (gate) -> step 2 (up) -> step 3 (activation) -> step 4 (act_up) -> step 5 (down - output)\n",
      "------------------------------\n",
      "step 0 (input) passed!\n",
      "FAIL AT step 1 (gate)\n",
      "FAIL AT step 2 (up)\n",
      "FAIL AT step 3 (activation)\n",
      "FAIL AT step 4 (act_up)\n",
      "FAIL AT step 5 (down - output)\n"
     ]
    }
   ],
   "source": [
    "step_keys = sorted(out_mlp_merged[\"steps\"].keys())\n",
    "print(\" -> \".join(step_keys))\n",
    "print(\"---\" * 10)\n",
    "for key in step_keys:\n",
    "    tensor_1 = out_mlp_1[\"steps\"][key]\n",
    "    tensor_merged = out_mlp_merged[\"steps\"][key]\n",
    "    if torch.allclose(tensor_1, tensor_merged, atol=0, rtol=0):\n",
    "        print(f\"{key} passed!\")\n",
    "    else:\n",
    "        print(f\"FAIL AT {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac5b7e70-f76a-48d5-9925-220db1ae3bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0179,  0.0153,  0.0164,  ..., -0.0186, -0.0030,  0.0052],\n",
       "         [ 0.0194, -0.0164,  0.0254,  ...,  0.0014, -0.0325,  0.0043],\n",
       "         [ 0.0347, -0.0330, -0.0273,  ...,  0.0630,  0.0035, -0.0332],\n",
       "         ...,\n",
       "         [-0.0179,  0.0153,  0.0164,  ..., -0.0186, -0.0030,  0.0052],\n",
       "         [ 0.0344,  0.0045,  0.0212,  ...,  0.0070,  0.0222,  0.0010],\n",
       "         [ 0.0347, -0.0330, -0.0273,  ...,  0.0630,  0.0035, -0.0332]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_1.mlp.gate_proj(model_inputs[\"hidden_states\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14487826-6d30-4f24-bc90-4dbf8fef5da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0179,  0.0153,  0.0162,  ..., -0.0186, -0.0031,  0.0052],\n",
       "         [ 0.0194, -0.0164,  0.0254,  ...,  0.0014, -0.0325,  0.0042],\n",
       "         [ 0.0349, -0.0327, -0.0273,  ...,  0.0630,  0.0035, -0.0332],\n",
       "         ...,\n",
       "         [-0.0179,  0.0153,  0.0162,  ..., -0.0186, -0.0031,  0.0052],\n",
       "         [ 0.0344,  0.0045,  0.0212,  ...,  0.0070,  0.0221,  0.0011],\n",
       "         [ 0.0349, -0.0327, -0.0273,  ...,  0.0630,  0.0035, -0.0332]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_merged.mlp.gate_proj(model_inputs[\"hidden_states\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "262b2b55-e7ed-4491-8373-aa0a112361d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdecoder_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Define the computation performed at every call.\n",
       "\n",
       "Should be overridden by all subclasses.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within\n",
       "    this function, one should call the :class:`Module` instance afterwards\n",
       "    instead of this since the former takes care of running the\n",
       "    registered hooks while the latter silently ignores them.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                   \u001b[0;32mfor\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_linears\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# merged_weight = torch.sum(torch.stack(weights), dim=0)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmerged_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_linears\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# merged_bias = torch.sum(torch.stack(biases))\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmerged_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mmerged_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_2646984/2665586263.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder_merged.mlp.gate_proj.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fcf7af87-3b4a-42c1-b5bb-384220fc5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_and_bias(masked):\n",
    "    weights = [linear.weight_mask(linear.linear.weight) \n",
    "               for linear in masked.masked_linears]\n",
    "    # merged_weight = torch.sum(torch.stack(weights), dim=0)\n",
    "    merged_weight = sum(weights)\n",
    "    biases = [\n",
    "        linear.bias_mask(linear.linear.bias) \n",
    "        if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "        else linear.linear.bias for linear in masked.masked_linears\n",
    "    ]\n",
    "    if all(b is None for b in biases):\n",
    "        merged_bias = None\n",
    "    else:\n",
    "        biases = [\n",
    "            b if b is not None\n",
    "            else torch.zeros_like(weights[0][:, 0])\n",
    "            for b in biases\n",
    "        ]\n",
    "        merged_bias = sum(biases)\n",
    "\n",
    "\n",
    "    return dict(\n",
    "        weight=merged_weight,\n",
    "        bias=merged_bias\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c800eb8-2428-4431-9fb5-7d5a87692910",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_masked_linears_params = get_weight_and_bias(decoder_merged.mlp.gate_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc663190-bd02-4644-b6d2-b2dda5ad682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(a_masked_linears_params[\"weight\"], decoder_1.mlp.gate_proj.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b44224d9-80ef-436a-823e-a4a546dbdd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert a_masked_linears_params[\"bias\"] == decoder_1.mlp.gate_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0e83c78-b991-4906-ae0e-b7e0be8ca6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0fe5958-2e8b-42c0-a304-838bc27ec04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bounty = \"\"\"\n",
    "Damn there is something wrong at step 6 decoder.\n",
    "Which is the MLP :D\n",
    "I'll fix it tmr.\n",
    "-------------------\n",
    "The error is fixed now. nn.Linear is kinda weird, which results in\n",
    "some number instability. In short, bias=torch.zeros_like() is different\n",
    "from bias=None when initializing an nn.Linear.\n",
    "\n",
    "In specific, in older implementations, I handle biases in LinearsWithMasks \n",
    "like this:\n",
    "```\n",
    "def forward(...):\n",
    "    ...\n",
    "    biases = [\n",
    "        linear.bias_mask(linear.linear.bias) \n",
    "        if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "        else linear.linear.bias for linear in self.masked_linears\n",
    "    ]\n",
    "    biases = [\n",
    "        b if b is not None\n",
    "        else torch.zeros_like(weights[0][:, 0])\n",
    "        for b in biases\n",
    "    ]\n",
    "    # merged_bias = torch.sum(torch.stack(biases))\n",
    "    merged_bias = sum(biases)\n",
    "    if all(b is None for b in biases):\n",
    "        merged_bias = None\n",
    "    ...\n",
    "```\n",
    "This implementation is not accurate as it will always assign torch.zeros_like()\n",
    "to biases when they are supposed to be None, making the last `if` redundant.\n",
    "\n",
    "A quick and accurate fix should be:\n",
    "```\n",
    "def forward(...):\n",
    "    ...\n",
    "    biases = [\n",
    "        linear.bias_mask(linear.linear.bias) \n",
    "        if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "        else linear.linear.bias for linear in self.masked_linears\n",
    "    ]\n",
    "    \n",
    "    if all(b is None for b in biases):\n",
    "        merged_bias = None\n",
    "    else:\n",
    "        biases = [\n",
    "            b if b is not None\n",
    "            else torch.zeros_like(weights[0][:, 0])\n",
    "            for b in biases\n",
    "        ]\n",
    "        merged_bias = sum(biases)\n",
    "    ...\n",
    "```\n",
    "Yeah, this is it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70938add-6ce9-40e9-a4a6-a10ad06ddae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_strategy = \"\"\"\n",
    "One of the cool things I figured out myself is how to do surgeon to class methods.\n",
    "Instead of having to re-implement a class with a modified method, like adding\n",
    "logging statements between lines of codes.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
