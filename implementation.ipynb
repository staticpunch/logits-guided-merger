{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3cdad09-d3c0-45e8-8f7d-1e73cac721b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "# import logging\n",
    "import copy\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from modeling_qwen2 import (\n",
    "    Qwen2RMSNorm, \n",
    "    Qwen2RotaryEmbedding, \n",
    "    Qwen2MLP, \n",
    "    Qwen2Attention, \n",
    "    Qwen2FlashAttention2, \n",
    "    Qwen2SdpaAttention, \n",
    "    Qwen2DecoderLayer, \n",
    "    Qwen2PreTrainedModel, \n",
    "    Qwen2Model, \n",
    "    Qwen2ForCausalLM,\n",
    "    logger\n",
    ")\n",
    "\n",
    "from configuration_qwen2 import Qwen2Config\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    ")\n",
    "\n",
    "# Configure logger\n",
    "# logger.basicConfig(level=logger.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b39d8d-c4dd-4231-89a9-6422aa0de49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import are_tokenizers_same\n",
    "# are_tokenizers_same(\n",
    "#     paths = [\n",
    "#         \"/workspace/models/Arcee-VyLinh/\",\n",
    "#         \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbb9c5f-c766-441f-80f7-c37014ffd078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer(path, layer_idx=33):\n",
    "\tstate_dict = {}\n",
    "\tshard_paths = [f for f in os.listdir(path) if f.endswith('.safetensors')]\n",
    "\tfor shard_path in sorted(shard_paths, key=lambda x: int(x.split('-')[1])):\n",
    "\t\tapath = os.path.join(path, shard_path)\n",
    "\t\twith safe_open(apath, framework=\"pt\", device=\"cpu\") as f:\n",
    "\t\t\tfor key in f.keys():\n",
    "\t\t\t\tif f\"layers.{str(layer_idx)}.\" in key:\n",
    "\t\t\t\t\tstate_dict[key] = f.get_tensor(key)\n",
    "\treturn state_dict\n",
    "\n",
    "def strip_prefix(state_dict, prefix=\"model.layers.\"):\n",
    "    \"\"\"Strips 'model.layers.*.' prefix from 'input_layernorm.weight' keys.\"\"\"\n",
    "    return {\n",
    "      k.replace(f\"{prefix}{k.split('.')[2]}.\", \"\") if k.startswith(prefix)\n",
    "      else k: v for k, v in state_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9560a0b5-7194-4096-8a2c-5d4bddeee216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(\n",
    "    t: float, v0: Union[np.ndarray, torch.Tensor], v1: Union[np.ndarray, torch.Tensor]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    return (1 - t) * v0 + t * v1\n",
    "\n",
    "def weighted_sum(\n",
    "    factors: List[float], \n",
    "    tensors: Union[List[np.ndarray], List[torch.Tensor]]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    result = 0.0\n",
    "    # for factor, tensor in zip(factors, tensors):\n",
    "    #     result += factor * tensor\n",
    "    return sum([tensor * factor for tensor, factor in zip(tensors, factors)])\n",
    "\n",
    "def merge_modules(modules, factors):\n",
    "    \"\"\"\n",
    "    This is only applicable for cases where a static set of scalars\n",
    "    playing as merging factor for every submodules of the passed module.\n",
    "    Not recommend for fine-grained usecases.\n",
    "    \"\"\"\n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out\n",
    "\n",
    "def merge_linears(modules, weight_factors, bias_factors):\n",
    "    param_names = sorted([name for name, _ in modules[0].named_parameters()])\n",
    "    for module in modules:\n",
    "        other_param_names = sorted([name for name, _ in module.named_parameters()])\n",
    "        assert param_names == other_param_names, \"Mismatch tensor names.\"\n",
    "        \n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        if \"weight\" in tensor_name:\n",
    "            factors = weight_factors\n",
    "        elif \"bias\" in tensor_name:\n",
    "            factors = bias_factors\n",
    "        else:\n",
    "            raise ValueError(\"Hey this tensor is neither weight or bias.\")\n",
    "            \n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8d01ac-aef2-440c-86bc-7accd0932b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_masks(target_module, ref_modules, factors):\n",
    "    \"\"\"\n",
    "    Recursively replaces normal components with masked components.\n",
    "    \n",
    "    Args:\n",
    "      module: The module in which to replace layers.\n",
    "    \"\"\"\n",
    "    assert len(ref_modules) == len(factors)\n",
    "    num_components = len(ref_modules)\n",
    "    for name, target_child in target_module.named_children():\n",
    "        ref_children = [getattr(module, name) for module in ref_modules]\n",
    "        modes = [\"scalar\" for _ in ref_children]\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            new_module = LinearsWithMasks(\n",
    "                linears=ref_children,\n",
    "                weight_modes=[\"scalar\"] * num_components,\n",
    "                weight_values=factors,\n",
    "                bias_modes=[\"scalar\"] * num_components,\n",
    "                bias_values=factors,\n",
    "            )\n",
    "            setattr(target_module, name, new_module)\n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            setattr(target_module, name, EmbeddingsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "        elif type(target_child).__name__ == Qwen2RMSNorm.__name__:\n",
    "            # print(\"Hehe placing masks to a cutie RMSNorm\")\n",
    "            setattr(target_module, name, RMSNormsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "        else:\n",
    "            place_masks(target_child, ref_children, factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79dd85d4-c7e9-4d62-9813-5c590b63a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, model, tokenizer, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()\n",
    "\n",
    "def get_logits(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_ids).logits\n",
    "    return logits\n",
    "\n",
    "def get_hidden_states(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids, output_hidden_states=True, use_cache=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ad60458-d15d-4392-85d8-03fda6ebcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward(\n",
    "    decoder,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "    output_attentions: Optional[bool] = False,\n",
    "    use_cache: Optional[bool] = False,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "\n",
    "    steps = {}\n",
    "    logger.warning(f\"-------- Logging hidden_states in decoder forward:\")\n",
    "    residual = hidden_states\n",
    "    logger.warning(f\" hidden_states step 1 (as input): {hidden_states}\")\n",
    "    steps.update({\"step 1\": hidden_states})\n",
    "\n",
    "    hidden_states = decoder.input_layernorm(hidden_states)\n",
    "    logger.warning(f\" hidden_states step 2 (after input_layernorm): {hidden_states}\")\n",
    "    steps.update({\"step 2\": hidden_states})\n",
    "    # Self Attention\n",
    "    hidden_states, self_attn_weights, present_key_value = decoder.self_attn(\n",
    "        hidden_states=hidden_states,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=past_key_value,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "        cache_position=cache_position,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )\n",
    "    logger.warning(f\" hidden_states step 3 (after self_attn): {hidden_states}\")\n",
    "    steps.update({\"step 3\": hidden_states})\n",
    "    \n",
    "    hidden_states = residual + hidden_states\n",
    "    logger.warning(f\" hidden_states step 4 (after first skip connection): {hidden_states}\")\n",
    "    steps.update({\"step 4\": hidden_states})\n",
    "    # Fully Connected\n",
    "    residual = hidden_states\n",
    "    hidden_states = decoder.post_attention_layernorm(hidden_states)\n",
    "    logger.warning(f\" hidden_states step 5 (after post_attention_layernorm): {hidden_states}\")\n",
    "    steps.update({\"step 5\": hidden_states})\n",
    "    \n",
    "    hidden_states = decoder.mlp(hidden_states)\n",
    "    logger.warning(f\" hidden_states step 6 (after mlp): {hidden_states}\")\n",
    "    steps.update({\"step 6\": hidden_states})\n",
    "    \n",
    "    hidden_states = residual + hidden_states\n",
    "    logger.warning(f\" hidden_states step 7 (after second skip connection): {hidden_states}\")\n",
    "    steps.update({\"step 7\": hidden_states})\n",
    "\n",
    "    outputs = (hidden_states,)\n",
    "\n",
    "    if output_attentions:\n",
    "        outputs += (self_attn_weights,)\n",
    "\n",
    "    if use_cache:\n",
    "        outputs += (present_key_value,)\n",
    "\n",
    "    outputs += (steps,)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "835816e2-51ff-407d-bc01-7508c55426fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    past_key_values = None\n",
    "    cache_position = None\n",
    "    position_ids = None\n",
    "    output_hidden_states = True\n",
    "    output_attentions = False\n",
    "    use_cache = False\n",
    "    return_dict = True\n",
    "    #############\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # kept for BC (non `Cache` `past_key_values` inputs)\n",
    "        return_legacy_cache = False\n",
    "        inputs_embeds = model.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = model._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "        all_decoder_steps = ()\n",
    "\n",
    "        for i, decoder_layer in enumerate(model.layers[:2]):\n",
    "            # logger.warning(f\" ---------Inputs to decoder_layer {i}:\")\n",
    "            # logger.warning(f\"  hidden_states: {hidden_states}\")\n",
    "            # logger.warning(f\"  attention_mask: {attention_mask}\")\n",
    "            # logger.warning(f\"  position_ids: {position_ids}\")\n",
    "            # logger.warning(f\"  past_key_value: {past_key_values}\")\n",
    "            # logger.warning(f\"  output_attentions: {output_attentions}\")\n",
    "            # logger.warning(f\"  use_cache: {use_cache}\")\n",
    "            # logger.warning(f\"  cache_position: {cache_position}\")\n",
    "            # logger.warning(f\"  position_embeddings: {position_embeddings}\")     \n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "          \n",
    "            layer_outputs = decoder_forward(\n",
    "                decoder_layer,\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            steps = layer_outputs[-1]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "            all_decoder_steps += (steps,)\n",
    "\n",
    "        hidden_states = model.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if return_legacy_cache:\n",
    "            next_cache = next_cache.to_legacy_cache()\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=(),\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_decoder_steps,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b5bbb8-1a19-4aa7-9d3b-8e6dfc628d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.weight = nn.Parameter(torch.tensor(value))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        if self.size is not None:\n",
    "            try:\n",
    "                self.weight * torch.rand(self.size)\n",
    "            except RuntimeError:\n",
    "                print(\"mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Be really careful here (though I do not think it matters that much),\n",
    "        When testing, it's important that the masking operation is implemented\n",
    "        with `x = self.weight * x` instead of `x = x * self.weight`.\n",
    "\n",
    "        Neither of those two implementation is superior, however I need to be\n",
    "        consistent when doing testing because the phenonmenon above could lead\n",
    "        to some number imprecision, which may fail `torch.testing.assert_close`\n",
    "        \"\"\"\n",
    "        if self.size is None:\n",
    "            return self.weight * x\n",
    "        else:\n",
    "            if self.size != x.shape:\n",
    "                print(\"The shape of input does not match that of the mask.\")\n",
    "            return self.weight * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce5f133-2e26-4855-89c4-10853bd7de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithMask(nn.Module):\n",
    "    def __init__(self, linear, weight_mask_config: MaskConfig, bias_mask_config: MaskConfig = None):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.weight_mask_config = weight_mask_config\n",
    "        self.bias_mask_config = bias_mask_config\n",
    "\n",
    "        if linear.weight.shape != weight_mask_config.size:\n",
    "            print(\"Weight mask shape is not compatible with linear, reinitializing...\")\n",
    "            self.weight_mask_config.size = linear.weight.shape\n",
    "        self.weight_mask = Mask(self.weight_mask_config)\n",
    "\n",
    "        if linear.bias is not None and bias_mask_config is not None:\n",
    "            if linear.bias.shape != bias_mask_config.size:\n",
    "                print(\"Bias mask shape is not compatible with linear, reinitializing...\")\n",
    "                self.bias_mask_config.size = linear.bias.shape\n",
    "            self.bias_mask = Mask(self.bias_mask_config)\n",
    "        else:\n",
    "            self.bias_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        masked_weight = self.weight_mask(self.linear.weight)\n",
    "        if self.linear.bias is not None and self.bias_mask is not None:\n",
    "            masked_bias = self.bias_mask(self.linear.bias)\n",
    "        else:\n",
    "            masked_bias = self.linear.bias\n",
    "        return nn.functional.linear(x, masked_weight, masked_bias)\n",
    "\n",
    "class LinearsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        linears: List[nn.Module],\n",
    "        weight_modes: List[str] = [\"scalar\"],\n",
    "        weight_values: List[float] = None,\n",
    "        bias_modes: List[str] = [\"scalar\"],\n",
    "        bias_values: List[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not all(isinstance(linear, nn.Linear) for linear in linears):\n",
    "            raise ValueError(\"All elements in 'linears' must be instances of nn.Linear.\")\n",
    "\n",
    "        weight_sizes = [linear.weight.shape for linear in linears]\n",
    "        bias_sizes = [linear.bias.shape if linear.bias is not None else None for linear in linears]\n",
    "        \n",
    "        if weight_values is None or len(weight_values) != len(linears):\n",
    "            raise ValueError(f\"weight_values for masks: {weight_values} do not match with linear layers: {linears}\")\n",
    "        if bias_values is None:\n",
    "            bias_values = [None] * len(linears)\n",
    "        if len(bias_values) != len(linears):\n",
    "            raise ValueError(f\"bias_values for masks: {bias_values} do not match with linear layers: {linears}\")\n",
    "\n",
    "        weight_mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(weight_modes, weight_values, weight_sizes)\n",
    "        ]\n",
    "        bias_mask_configs = [\n",
    "            MaskConfig(mode, value, size) if size is not None else None\n",
    "            for mode, value, size in zip(bias_modes, bias_values, bias_sizes)\n",
    "        ]\n",
    "\n",
    "        self.masked_linears = nn.ModuleList(\n",
    "            [LinearWithMask(linear, weight_mask_config, bias_mask_config)\n",
    "             for linear, weight_mask_config, bias_mask_config \n",
    "             in zip(linears, weight_mask_configs, bias_mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = [linear.weight_mask(linear.linear.weight) \n",
    "                   for linear in self.masked_linears]\n",
    "        # merged_weight = torch.sum(torch.stack(weights), dim=0)\n",
    "        merged_weight = sum(weights)\n",
    "\n",
    "        biases = [\n",
    "            linear.bias_mask(linear.linear.bias) \n",
    "            if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "            else linear.linear.bias for linear in self.masked_linears\n",
    "        ]\n",
    "        biases = [\n",
    "            b if b is not None\n",
    "            else torch.zeros_like(weights[0][:, 0])\n",
    "            for b in biases\n",
    "        ]\n",
    "        # merged_bias = torch.sum(torch.stack(biases))\n",
    "        merged_bias = sum(biases)\n",
    "        if all(b is None for b in biases):\n",
    "            merged_bias = None\n",
    "\n",
    "        return nn.functional.linear(x, merged_weight, merged_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8eb68e3-ff6a-46c7-be08-f95e9705a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormWithMask(nn.Module):\n",
    "    def __init__(self, rms_norm: Qwen2RMSNorm, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.rms_norm = rms_norm\n",
    "        self.mask_config = mask_config\n",
    "        if rms_norm.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with RMSNorm, reinitializing...\")\n",
    "        self.mask_config.size = rms_norm.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        masked_weight = self.mask(self.rms_norm.weight)\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.rms_norm.variance_epsilon)\n",
    "        return masked_weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RMSNormsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rms_norms: List[Qwen2RMSNorm],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [rms_norm.weight.shape for rms_norm in rms_norms]\n",
    "        if values is None or len(values) != len(rms_norms):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with RMSNorm layers: {rms_norms}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_rms_norms = nn.ModuleList(\n",
    "            [RMSNormWithMask(rms_norm, mask_config)\n",
    "             for rms_norm, mask_config in zip(rms_norms, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        weights = [rms.mask(rms.rms_norm.weight) for rms in self.masked_rms_norms]\n",
    "        merged_weight = sum(weights)\n",
    "        variance_epsilon = self.masked_rms_norms[0].rms_norm.variance_epsilon\n",
    "        for rms in self.masked_rms_norms:\n",
    "            assert variance_epsilon == rms.rms_norm.variance_epsilon, \"Variance epsilon among models must be consistent\"\n",
    "        \n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "        return merged_weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "240c3b14-7d05-4a09-84c6-12b411f3ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithMask(nn.Module):\n",
    "    def __init__(self, embedding: nn.Embedding, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.mask_config = mask_config\n",
    "        if embedding.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with Embedding, reinitializing...\")\n",
    "        self.mask_config.size = embedding.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        masked_weight = self.mask(self.embedding.weight)\n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            masked_weight,\n",
    "            padding_idx=self.embedding.padding_idx,\n",
    "            max_norm=self.embedding.max_norm,\n",
    "            norm_type=self.embedding.norm_type,\n",
    "            scale_grad_by_freq=self.embedding.scale_grad_by_freq,\n",
    "            sparse=self.embedding.sparse,\n",
    "        )\n",
    "\n",
    "class EmbeddingsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [embedding.weight.shape for embedding in embeddings]\n",
    "        if values is None or len(values) != len(embeddings):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with Embedding layers: {embeddings}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_embeddings = nn.ModuleList(\n",
    "            [EmbeddingWithMask(embedding, mask_config)\n",
    "             for embedding, mask_config in zip(embeddings, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        weights = [emb.mask(emb.embedding.weight) for emb in self.masked_embeddings]\n",
    "        merged_weight = sum(weights)\n",
    "        an_embedding = self.masked_embeddings[0].embedding\n",
    "        for other in self.masked_embeddings:\n",
    "            other_embedding = other.embedding\n",
    "            assert an_embedding.padding_idx == other_embedding.padding_idx\n",
    "            assert an_embedding.max_norm == other_embedding.max_norm\n",
    "            assert an_embedding.norm_type == other_embedding.norm_type\n",
    "            assert an_embedding.scale_grad_by_freq == other_embedding.scale_grad_by_freq\n",
    "            assert an_embedding.sparse == other_embedding.sparse\n",
    "            \n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            merged_weight,\n",
    "            padding_idx=an_embedding.padding_idx,\n",
    "            max_norm=an_embedding.max_norm,\n",
    "            norm_type=an_embedding.norm_type,\n",
    "            scale_grad_by_freq=an_embedding.scale_grad_by_freq,\n",
    "            sparse=an_embedding.sparse,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c979f89-ab78-440f-80c8-157b83014548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.num_models = len(merge_config.model_paths)\n",
    "        self.configs = [\n",
    "            # AutoConfig.from_pretrained(path) \n",
    "            Qwen2Config.from_pretrained(path)\n",
    "            for path in merge_config.model_paths\n",
    "        ]\n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.models = nn.ModuleList([\n",
    "            Qwen2ForCausalLM.from_pretrained(\n",
    "            # AutoModelForCausalLM.from_pretrained(\n",
    "                merge_config.model_paths[i], \n",
    "                config=self.configs[i],\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ) \n",
    "            for i in range(self.num_models)\n",
    "        ])\n",
    "        # self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config, factors):\n",
    "        # dummy_config = copy.deepcopy(self.configs[0])\n",
    "        # dummy_config.update({\"hidden_size\": 1, \"intermediate_size\": 1})\n",
    "        # self.merger = AutoModelForCausalLM.from_config(dummy_config)\n",
    "        self.merger = copy.deepcopy(self.models[0])\n",
    "        place_masks(self.merger, self.models, factors=factors)\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2bbe46e-0b19-48b8-8b3e-e489adbdf0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa47700-3b99-4630-8a59-0d437799977a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7907adf9d65f4a98999ed8b1d591c482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4f9caaa63d4a0c8c4bf131845f5c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99134fb8-068b-4840-b4c6-794b94b859e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merger = merger.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4b98853-8d28-4c26-9d8c-7ab9f5fbe2ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merger.__post_init__(merge_config, factors=[0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bfc6b5f-69c2-4aa8-aaf9-48b10a520737",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5264369e-20c8-4c60-a7ca-356bcf1b4650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nContinue this text: A dog is a cat<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "prompt = \"Continue this text: A dog is a cat\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e82de2e3-d5f8-4d42-81b2-b916e8aeb325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = generate(text, merger.merger, tokenizer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a743fbc-cd0a-4985-9fdb-d2a0ae8cb345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = generate(text, merger.models[1], tokenizer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd49f453-5057-4451-83d0-1d3e5b0b5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_with_masks = merger.merger.model.embed_tokens\n",
    "# embedding1 = merger.models[1].model.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3080324-69cf-4023-8d97-1750de78ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# num_embeddings = embedding1.weight.data.shape[0]\n",
    "# device = merger.device\n",
    "# input_ids = torch.randint(0, num_embeddings, (2, 5)).to(device=device)  # Example input_ids\n",
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62fa2792-af53-4dac-bd6f-9ca3db225a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o_merged = embeddings_with_masks(input_ids)\n",
    "# o1 = embedding1(input_ids)\n",
    "# torch.allclose(o_merged, o1, rtol=1e-10, atol=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b053dd57-8e09-400d-8354-989a8c54bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_merged = get_logits(text, merger.merger, tokenizer)\n",
    "# logits1 = get_logits(text, merger.models[1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6af0edee-f796-4b43-ac8b-7c307587409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_merged, logits1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b58ed39-0aaf-40db-8d96-eb26c2b410ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_merged = get_hidden_states(text, merger.merger, tokenizer)\n",
    "outputs1 = get_hidden_states(text, merger.models[1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e1e4ba2-f1c5-4633-973d-896d985e1e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-------- Logging hidden_states in decoder forward:\n",
      " hidden_states step 1 (as input): tensor([[[ 0.0047,  0.0058,  0.0047,  ...,  0.0160, -0.0043, -0.0048],\n",
      "         [-0.0229,  0.0630,  0.0459,  ..., -0.0048, -0.0018, -0.0004],\n",
      "         [-0.0245, -0.0157, -0.0250,  ..., -0.0303, -0.0023,  0.0110],\n",
      "         ...,\n",
      "         [ 0.0047,  0.0058,  0.0047,  ...,  0.0160, -0.0043, -0.0048],\n",
      "         [ 0.0317,  0.0317,  0.0159,  ...,  0.0092,  0.0179,  0.0215],\n",
      "         [-0.0245, -0.0157, -0.0250,  ..., -0.0303, -0.0023,  0.0110]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 2 (after input_layernorm): tensor([[[ 0.1504,  0.2109,  0.1475,  ...,  0.5039, -0.2490, -0.2158],\n",
      "         [-0.2598,  0.8125,  0.5117,  ..., -0.0530, -0.0366, -0.0071],\n",
      "         [-0.2695, -0.1963, -0.2695,  ..., -0.3242, -0.0449,  0.1680],\n",
      "         ...,\n",
      "         [ 0.1504,  0.2109,  0.1475,  ...,  0.5039, -0.2490, -0.2158],\n",
      "         [ 0.3711,  0.4219,  0.1826,  ...,  0.1050,  0.3770,  0.3496],\n",
      "         [-0.2695, -0.1963, -0.2695,  ..., -0.3242, -0.0449,  0.1680]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 3 (after self_attn): tensor([[[ 0.0967,  0.0830, -0.0332,  ..., -0.2012, -0.3242,  0.0356],\n",
      "         [ 0.2412,  0.2021, -0.0427,  ..., -0.0579, -0.1875,  0.0461],\n",
      "         [ 0.2471,  0.0659, -0.0027,  ..., -0.0864, -0.1089,  0.1641],\n",
      "         ...,\n",
      "         [ 0.0544,  0.0742, -0.0148,  ..., -0.1992, -0.3340,  0.0189],\n",
      "         [ 0.2275,  0.0742, -0.0811,  ..., -0.2500, -0.3672, -0.0752],\n",
      "         [ 0.2100,  0.0588,  0.0374,  ...,  0.0042, -0.0703,  0.1699]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 4 (after first skip connection): tensor([[[ 0.1016,  0.0889, -0.0286,  ..., -0.1855, -0.3281,  0.0308],\n",
      "         [ 0.2188,  0.2656,  0.0032,  ..., -0.0625, -0.1895,  0.0457],\n",
      "         [ 0.2227,  0.0503, -0.0277,  ..., -0.1167, -0.1113,  0.1748],\n",
      "         ...,\n",
      "         [ 0.0591,  0.0801, -0.0101,  ..., -0.1836, -0.3379,  0.0141],\n",
      "         [ 0.2598,  0.1060, -0.0654,  ..., -0.2412, -0.3496, -0.0537],\n",
      "         [ 0.1855,  0.0430,  0.0123,  ..., -0.0261, -0.0728,  0.1807]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 5 (after post_attention_layernorm): tensor([[[ 0.0894,  0.1021, -0.0349,  ..., -0.2285, -0.2734,  0.0393],\n",
      "         [ 0.2754,  0.4355,  0.0055,  ..., -0.1104, -0.2256,  0.0835],\n",
      "         [ 0.3281,  0.0972, -0.0566,  ..., -0.2402, -0.1553,  0.3750],\n",
      "         ...,\n",
      "         [ 0.0532,  0.0942, -0.0126,  ..., -0.2314, -0.2910,  0.0184],\n",
      "         [ 0.3164,  0.1680, -0.1099,  ..., -0.4082, -0.4023, -0.0942],\n",
      "         [ 0.2910,  0.0874,  0.0267,  ..., -0.0571, -0.1074,  0.4102]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 6 (after mlp): tensor([[[-0.0149, -0.1309,  0.0101,  ...,  0.0051, -0.4629, -0.3398],\n",
      "         [-0.2461, -0.1836,  0.3242,  ...,  0.4473, -0.1543, -0.5391],\n",
      "         [-0.3945,  0.3828, -0.1982,  ...,  0.3262,  0.1641, -0.6250],\n",
      "         ...,\n",
      "         [-0.0126, -0.1094,  0.0320,  ..., -0.0654, -0.4805, -0.3281],\n",
      "         [ 0.0464, -0.1562, -0.0645,  ...,  0.2520, -0.0391, -0.3809],\n",
      "         [-0.4277,  0.2227, -0.0591,  ...,  0.1050,  0.1084, -0.5234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 7 (after second skip connection): tensor([[[ 0.0869, -0.0420, -0.0186,  ..., -0.1807, -0.7891, -0.3086],\n",
      "         [-0.0273,  0.0820,  0.3281,  ...,  0.3848, -0.3438, -0.4941],\n",
      "         [-0.1719,  0.4336, -0.2256,  ...,  0.2090,  0.0527, -0.4492],\n",
      "         ...,\n",
      "         [ 0.0464, -0.0293,  0.0219,  ..., -0.2490, -0.8203, -0.3145],\n",
      "         [ 0.3066, -0.0503, -0.1299,  ...,  0.0107, -0.3887, -0.4336],\n",
      "         [-0.2422,  0.2656, -0.0469,  ...,  0.0791,  0.0356, -0.3438]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "-------- Logging hidden_states in decoder forward:\n",
      " hidden_states step 1 (as input): tensor([[[ 0.0869, -0.0420, -0.0186,  ..., -0.1807, -0.7891, -0.3086],\n",
      "         [-0.0273,  0.0820,  0.3281,  ...,  0.3848, -0.3438, -0.4941],\n",
      "         [-0.1719,  0.4336, -0.2256,  ...,  0.2090,  0.0527, -0.4492],\n",
      "         ...,\n",
      "         [ 0.0464, -0.0293,  0.0219,  ..., -0.2490, -0.8203, -0.3145],\n",
      "         [ 0.3066, -0.0503, -0.1299,  ...,  0.0107, -0.3887, -0.4336],\n",
      "         [-0.2422,  0.2656, -0.0469,  ...,  0.0791,  0.0356, -0.3438]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 2 (after input_layernorm): tensor([[[-0.0466, -0.0245,  0.0080,  ...,  0.0771, -0.3965, -0.1982],\n",
      "         [ 0.0132,  0.0430, -0.1260,  ..., -0.1465, -0.1553, -0.2852],\n",
      "         [ 0.0952,  0.2598,  0.1001,  ..., -0.0913,  0.0275, -0.2969],\n",
      "         ...,\n",
      "         [-0.0253, -0.0173, -0.0095,  ...,  0.1069, -0.4160, -0.2041],\n",
      "         [-0.1602, -0.0284,  0.0542,  ..., -0.0044, -0.1895, -0.2695],\n",
      "         [ 0.1631,  0.1953,  0.0254,  ..., -0.0422,  0.0226, -0.2773]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 3 (after self_attn): tensor([[[-0.0225,  0.1924,  0.0396,  ...,  0.0579,  0.1895, -0.3105],\n",
      "         [ 0.0051,  0.0223,  0.0131,  ...,  0.0425,  0.1113, -0.2207],\n",
      "         [-0.0554, -0.1279, -0.0273,  ...,  0.0718,  0.2168, -0.2256],\n",
      "         ...,\n",
      "         [ 0.0535,  0.0615,  0.2207,  ...,  0.0481,  0.2051, -0.1230],\n",
      "         [-0.1099, -0.0337,  0.2383,  ..., -0.0459,  0.1152, -0.1245],\n",
      "         [ 0.1245, -0.1396,  0.0152,  ...,  0.0294,  0.1680, -0.1582]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 4 (after first skip connection): tensor([[[ 0.0645,  0.1504,  0.0210,  ..., -0.1230, -0.6016, -0.6172],\n",
      "         [-0.0222,  0.1045,  0.3418,  ...,  0.4277, -0.2324, -0.7148],\n",
      "         [-0.2275,  0.3047, -0.2539,  ...,  0.2812,  0.2695, -0.6758],\n",
      "         ...,\n",
      "         [ 0.0996,  0.0322,  0.2422,  ..., -0.2012, -0.6172, -0.4375],\n",
      "         [ 0.1973, -0.0840,  0.1084,  ..., -0.0352, -0.2734, -0.5586],\n",
      "         [-0.1177,  0.1260, -0.0317,  ...,  0.1084,  0.2031, -0.5000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 5 (after post_attention_layernorm): tensor([[[ 0.0830,  0.1924,  0.0273,  ..., -0.1699, -0.4590, -0.9844],\n",
      "         [-0.0250,  0.1177,  0.3926,  ...,  0.5195, -0.1562, -1.0000],\n",
      "         [-0.2832,  0.3789, -0.3223,  ...,  0.3789,  0.2002, -1.0547],\n",
      "         ...,\n",
      "         [ 0.1309,  0.0425,  0.3223,  ..., -0.2852, -0.4824, -0.7148],\n",
      "         [ 0.2412, -0.1025,  0.1348,  ..., -0.0464, -0.1992, -0.8516],\n",
      "         [-0.1758,  0.1885, -0.0483,  ...,  0.1748,  0.1807, -0.9258]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 6 (after mlp): tensor([[[-0.2676,  0.0403, -0.3887,  ...,  0.0986,  0.3008,  0.3223],\n",
      "         [-0.3145,  0.0781, -0.0879,  ..., -0.0728, -0.1240,  0.3066],\n",
      "         [-0.2852,  0.0918, -0.0557,  ...,  0.0242, -0.0830,  0.2676],\n",
      "         ...,\n",
      "         [-0.1924,  0.3594, -0.0063,  ...,  0.1895, -0.1318,  0.3516],\n",
      "         [-0.1367,  0.0698,  0.0260,  ..., -0.0811, -0.0244,  0.1133],\n",
      "         [-0.1543,  0.0693, -0.0300,  ..., -0.0111, -0.0302,  0.1396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 7 (after second skip connection): tensor([[[-0.2031,  0.1904, -0.3672,  ..., -0.0244, -0.3008, -0.2949],\n",
      "         [-0.3359,  0.1826,  0.2539,  ...,  0.3555, -0.3555, -0.4082],\n",
      "         [-0.5117,  0.3965, -0.3086,  ...,  0.3047,  0.1865, -0.4082],\n",
      "         ...,\n",
      "         [-0.0928,  0.3906,  0.2363,  ..., -0.0117, -0.7500, -0.0859],\n",
      "         [ 0.0605, -0.0142,  0.1348,  ..., -0.1162, -0.2969, -0.4453],\n",
      "         [-0.2715,  0.1953, -0.0618,  ...,  0.0972,  0.1729, -0.3594]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "outputs_test_merged = model_forward(text, merger.merger.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01a16240-ff0f-4515-aaeb-741705c77864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-------- Logging hidden_states in decoder forward:\n",
      " hidden_states step 1 (as input): tensor([[[ 0.0047,  0.0058,  0.0047,  ...,  0.0160, -0.0043, -0.0048],\n",
      "         [-0.0229,  0.0630,  0.0459,  ..., -0.0048, -0.0018, -0.0004],\n",
      "         [-0.0245, -0.0157, -0.0250,  ..., -0.0303, -0.0023,  0.0110],\n",
      "         ...,\n",
      "         [ 0.0047,  0.0058,  0.0047,  ...,  0.0160, -0.0043, -0.0048],\n",
      "         [ 0.0317,  0.0317,  0.0159,  ...,  0.0092,  0.0179,  0.0215],\n",
      "         [-0.0245, -0.0157, -0.0250,  ..., -0.0303, -0.0023,  0.0110]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 2 (after input_layernorm): tensor([[[ 0.1504,  0.2109,  0.1475,  ...,  0.5039, -0.2490, -0.2158],\n",
      "         [-0.2598,  0.8125,  0.5117,  ..., -0.0530, -0.0366, -0.0071],\n",
      "         [-0.2695, -0.1963, -0.2695,  ..., -0.3242, -0.0449,  0.1680],\n",
      "         ...,\n",
      "         [ 0.1504,  0.2109,  0.1475,  ...,  0.5039, -0.2490, -0.2158],\n",
      "         [ 0.3711,  0.4219,  0.1826,  ...,  0.1050,  0.3770,  0.3496],\n",
      "         [-0.2695, -0.1963, -0.2695,  ..., -0.3242, -0.0449,  0.1680]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 3 (after self_attn): tensor([[[ 0.0967,  0.0830, -0.0332,  ..., -0.2012, -0.3242,  0.0356],\n",
      "         [ 0.2412,  0.2021, -0.0427,  ..., -0.0579, -0.1875,  0.0461],\n",
      "         [ 0.2471,  0.0659, -0.0027,  ..., -0.0864, -0.1089,  0.1641],\n",
      "         ...,\n",
      "         [ 0.0544,  0.0742, -0.0148,  ..., -0.1992, -0.3340,  0.0189],\n",
      "         [ 0.2275,  0.0742, -0.0811,  ..., -0.2500, -0.3672, -0.0752],\n",
      "         [ 0.2100,  0.0588,  0.0374,  ...,  0.0042, -0.0703,  0.1699]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 4 (after first skip connection): tensor([[[ 0.1016,  0.0889, -0.0286,  ..., -0.1855, -0.3281,  0.0308],\n",
      "         [ 0.2188,  0.2656,  0.0032,  ..., -0.0625, -0.1895,  0.0457],\n",
      "         [ 0.2227,  0.0503, -0.0277,  ..., -0.1167, -0.1113,  0.1748],\n",
      "         ...,\n",
      "         [ 0.0591,  0.0801, -0.0101,  ..., -0.1836, -0.3379,  0.0141],\n",
      "         [ 0.2598,  0.1060, -0.0654,  ..., -0.2412, -0.3496, -0.0537],\n",
      "         [ 0.1855,  0.0430,  0.0123,  ..., -0.0261, -0.0728,  0.1807]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 5 (after post_attention_layernorm): tensor([[[ 0.0894,  0.1021, -0.0349,  ..., -0.2285, -0.2734,  0.0393],\n",
      "         [ 0.2754,  0.4355,  0.0055,  ..., -0.1104, -0.2256,  0.0835],\n",
      "         [ 0.3281,  0.0972, -0.0566,  ..., -0.2402, -0.1553,  0.3750],\n",
      "         ...,\n",
      "         [ 0.0532,  0.0942, -0.0126,  ..., -0.2314, -0.2910,  0.0184],\n",
      "         [ 0.3164,  0.1680, -0.1099,  ..., -0.4082, -0.4023, -0.0942],\n",
      "         [ 0.2910,  0.0874,  0.0267,  ..., -0.0571, -0.1074,  0.4102]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 6 (after mlp): tensor([[[-0.0142, -0.1299,  0.0088,  ...,  0.0060, -0.4609, -0.3398],\n",
      "         [-0.2461, -0.1846,  0.3262,  ...,  0.4492, -0.1523, -0.5430],\n",
      "         [-0.3945,  0.3789, -0.1982,  ...,  0.3262,  0.1660, -0.6250],\n",
      "         ...,\n",
      "         [-0.0123, -0.1094,  0.0317,  ..., -0.0645, -0.4824, -0.3281],\n",
      "         [ 0.0476, -0.1572, -0.0649,  ...,  0.2520, -0.0403, -0.3789],\n",
      "         [-0.4277,  0.2227, -0.0598,  ...,  0.1050,  0.1089, -0.5234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 7 (after second skip connection): tensor([[[ 0.0874, -0.0410, -0.0198,  ..., -0.1797, -0.7891, -0.3086],\n",
      "         [-0.0273,  0.0811,  0.3301,  ...,  0.3867, -0.3418, -0.4980],\n",
      "         [-0.1719,  0.4297, -0.2256,  ...,  0.2090,  0.0547, -0.4492],\n",
      "         ...,\n",
      "         [ 0.0469, -0.0293,  0.0216,  ..., -0.2480, -0.8203, -0.3145],\n",
      "         [ 0.3066, -0.0513, -0.1309,  ...,  0.0107, -0.3906, -0.4336],\n",
      "         [-0.2422,  0.2656, -0.0474,  ...,  0.0791,  0.0361, -0.3438]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "-------- Logging hidden_states in decoder forward:\n",
      " hidden_states step 1 (as input): tensor([[[ 0.0874, -0.0410, -0.0198,  ..., -0.1797, -0.7891, -0.3086],\n",
      "         [-0.0273,  0.0811,  0.3301,  ...,  0.3867, -0.3418, -0.4980],\n",
      "         [-0.1719,  0.4297, -0.2256,  ...,  0.2090,  0.0547, -0.4492],\n",
      "         ...,\n",
      "         [ 0.0469, -0.0293,  0.0216,  ..., -0.2480, -0.8203, -0.3145],\n",
      "         [ 0.3066, -0.0513, -0.1309,  ...,  0.0107, -0.3906, -0.4336],\n",
      "         [-0.2422,  0.2656, -0.0474,  ...,  0.0791,  0.0361, -0.3438]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 2 (after input_layernorm): tensor([[[-0.0469, -0.0239,  0.0085,  ...,  0.0762, -0.3965, -0.1982],\n",
      "         [ 0.0132,  0.0425, -0.1270,  ..., -0.1465, -0.1533, -0.2871],\n",
      "         [ 0.0957,  0.2578,  0.1001,  ..., -0.0913,  0.0284, -0.2969],\n",
      "         ...,\n",
      "         [-0.0255, -0.0173, -0.0094,  ...,  0.1064, -0.4160, -0.2041],\n",
      "         [-0.1602, -0.0289,  0.0544,  ..., -0.0044, -0.1904, -0.2695],\n",
      "         [ 0.1631,  0.1953,  0.0255,  ..., -0.0422,  0.0228, -0.2773]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 3 (after self_attn): tensor([[[-0.0232,  0.1914,  0.0391,  ...,  0.0581,  0.1895, -0.3086],\n",
      "         [ 0.0044,  0.0215,  0.0135,  ...,  0.0415,  0.1099, -0.2207],\n",
      "         [-0.0552, -0.1289, -0.0271,  ...,  0.0728,  0.2158, -0.2246],\n",
      "         ...,\n",
      "         [ 0.0532,  0.0610,  0.2207,  ...,  0.0474,  0.2051, -0.1235],\n",
      "         [-0.1108, -0.0332,  0.2383,  ..., -0.0461,  0.1152, -0.1250],\n",
      "         [ 0.1245, -0.1396,  0.0140,  ...,  0.0295,  0.1680, -0.1582]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 4 (after first skip connection): tensor([[[ 0.0645,  0.1504,  0.0193,  ..., -0.1216, -0.6016, -0.6172],\n",
      "         [-0.0229,  0.1025,  0.3438,  ...,  0.4277, -0.2324, -0.7188],\n",
      "         [-0.2266,  0.3008, -0.2520,  ...,  0.2812,  0.2695, -0.6719],\n",
      "         ...,\n",
      "         [ 0.1001,  0.0317,  0.2422,  ..., -0.2012, -0.6172, -0.4375],\n",
      "         [ 0.1953, -0.0845,  0.1074,  ..., -0.0354, -0.2754, -0.5586],\n",
      "         [-0.1177,  0.1260, -0.0334,  ...,  0.1084,  0.2041, -0.5000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 5 (after post_attention_layernorm): tensor([[[ 0.0830,  0.1924,  0.0251,  ..., -0.1680, -0.4590, -0.9844],\n",
      "         [-0.0258,  0.1152,  0.3945,  ...,  0.5195, -0.1562, -1.0078],\n",
      "         [-0.2832,  0.3770, -0.3203,  ...,  0.3789,  0.2002, -1.0469],\n",
      "         ...,\n",
      "         [ 0.1309,  0.0417,  0.3223,  ..., -0.2852, -0.4824, -0.7148],\n",
      "         [ 0.2383, -0.1030,  0.1338,  ..., -0.0466, -0.2002, -0.8516],\n",
      "         [-0.1758,  0.1885, -0.0510,  ...,  0.1748,  0.1816, -0.9336]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 6 (after mlp): tensor([[[-0.2656,  0.0447, -0.3926,  ...,  0.0957,  0.2949,  0.3320],\n",
      "         [-0.3145,  0.0771, -0.0869,  ..., -0.0723, -0.1230,  0.3047],\n",
      "         [-0.2832,  0.0908, -0.0554,  ...,  0.0236, -0.0825,  0.2676],\n",
      "         ...,\n",
      "         [-0.1914,  0.3613, -0.0081,  ...,  0.1885, -0.1318,  0.3477],\n",
      "         [-0.1367,  0.0698,  0.0255,  ..., -0.0811, -0.0236,  0.1118],\n",
      "         [-0.1543,  0.0688, -0.0294,  ..., -0.0115, -0.0305,  0.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      " hidden_states step 7 (after second skip connection): tensor([[[-0.2012,  0.1953, -0.3730,  ..., -0.0259, -0.3066, -0.2852],\n",
      "         [-0.3379,  0.1797,  0.2578,  ...,  0.3555, -0.3555, -0.4141],\n",
      "         [-0.5078,  0.3906, -0.3066,  ...,  0.3047,  0.1875, -0.4043],\n",
      "         ...,\n",
      "         [-0.0913,  0.3926,  0.2344,  ..., -0.0127, -0.7500, -0.0898],\n",
      "         [ 0.0586, -0.0146,  0.1328,  ..., -0.1162, -0.2988, -0.4473],\n",
      "         [-0.2715,  0.1953, -0.0630,  ...,  0.0967,  0.1738, -0.3594]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "outputs_test_1 = model_forward(text, merger.models[1].model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9492ddfc-4f22-47dd-b1d7-ace14c24a2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0, step 1 passed!\n",
      "layer 0, step 2 passed!\n",
      "layer 0, step 3 passed!\n",
      "layer 0, step 4 passed!\n",
      "layer 0, step 5 passed!\n",
      "FAIL AT layer 0, step 6\n",
      "FAIL AT layer 0, step 7\n",
      "FAIL AT layer 1, step 1\n",
      "FAIL AT layer 1, step 2\n",
      "FAIL AT layer 1, step 3\n",
      "FAIL AT layer 1, step 4\n",
      "FAIL AT layer 1, step 5\n",
      "FAIL AT layer 1, step 6\n",
      "FAIL AT layer 1, step 7\n"
     ]
    }
   ],
   "source": [
    "for j, layer_output in enumerate(outputs_test_merged.attentions):\n",
    "    other_output = outputs_test_1.attentions[j]\n",
    "    for i in range(7):\n",
    "        key = f\"step {i+1}\"\n",
    "        if torch.allclose(layer_output[key], other_output[key], atol=0, rtol=0):\n",
    "            print(f\"layer {j}, step {i+1} passed!\")\n",
    "        else:\n",
    "            print(f\"FAIL AT layer {j}, step {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d2564ae-6e59-4b02-acef-7bd06ea494e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoders = len(merger.models[0].model.layers)\n",
    "num_decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e0fe5958-2e8b-42c0-a304-838bc27ec04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bounty = \"\"\"\n",
    "Damn there is something wrong at step 6 decoder.\n",
    "Which is the MLP :D\n",
    "I'll fix it tmr.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bb21d52-97c7-4e71-b475-bcd42774f074",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not equal!\n\nMismatched elements: 27697 / 57344 (48.3%)\nGreatest absolute difference: 0.03125 at index (0, 2, 1874)\nGreatest relative difference: inf at index (0, 0, 533)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m outputs_merged\u001b[38;5;241m.\u001b[39mhidden_states[i]\n\u001b[1;32m      3\u001b[0m b \u001b[38;5;241m=\u001b[39m outputs1\u001b[38;5;241m.\u001b[39mhidden_states[i]\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHidden states at layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of two models are identical.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/testing/_comparison.py:1530\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1508\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1509\u001b[0m     actual,\n\u001b[1;32m   1510\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1526\u001b[0m )\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not equal!\n\nMismatched elements: 27697 / 57344 (48.3%)\nGreatest absolute difference: 0.03125 at index (0, 2, 1874)\nGreatest relative difference: inf at index (0, 0, 533)"
     ]
    }
   ],
   "source": [
    "for i in range(num_decoders):\n",
    "    a = outputs_merged.hidden_states[i]\n",
    "    b = outputs1.hidden_states[i]\n",
    "    torch.testing.assert_close(a, b, rtol=0, atol=0)\n",
    "    logger.info(f\"Hidden states at layer {i} of two models are identical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75c7c227-f52b-4bfa-b1cb-0be472a05be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merger.merger.forward?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
