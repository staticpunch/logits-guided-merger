{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754cc9c2-a335-4612-9409-ab21f754ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = \"\"\"\n",
    "1. prepare data\n",
    "2. define model\n",
    "    - model a, mask a\n",
    "    - model b, mask b\n",
    "make only masks trainable.\n",
    "make sure it has correct inference.\n",
    "3. define loss function in Trainer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4884f-b796-4384-a2ad-177f2314c3e2",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6baca-8a9b-4a41-8e32-01d6987ae939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "332cf659-c413-4f11-889a-79532efcdd97",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6167e22d-21c5-4027-af0b-a029c8acfe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from modeling_qwen2 import (\n",
    "    Qwen2RMSNorm, \n",
    "    Qwen2RotaryEmbedding, \n",
    "    Qwen2MLP, \n",
    "    Qwen2Attention, \n",
    "    Qwen2FlashAttention2, \n",
    "    Qwen2SdpaAttention, \n",
    "    Qwen2DecoderLayer, \n",
    "    Qwen2PreTrainedModel, \n",
    "    Qwen2Model, \n",
    "    Qwen2ForCausalLM\n",
    ")\n",
    "\n",
    "from configuration_qwen2 import Qwen2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b781d7e-4a7b-4714-9e94-08d2458d6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen2_config = Qwen2Config.from_pretrained(\n",
    "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    ")\n",
    "qwen2_attn = Qwen2Attention(qwen2_config, layer_idx=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18fd625a-64ce-4a36-b16d-1b6e61eaa5c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8256a2-89d0-4cfa-8726-1ac197a04470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc68586-edfa-465f-a1e8-513541ccac5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/models/Arcee-VyLinh/', '/workspace/models/Qwen2.5-Coder-3B/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config.model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2eca4d1f-96da-4126-bc64-ed531f46d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.model_paths[0]\n",
    "        )\n",
    "        \n",
    "        self.models = nn.ModuleList([\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map={\"\":0}\n",
    "            ) for model_path in config.model_paths\n",
    "        ])\n",
    "        self.__post_init__()\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        # self.masks = torch.nn\n",
    "        pass\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        \"\"\"\n",
    "        activations = []\n",
    "        for i in range(num_layers):\n",
    "            L1 = models[0].layers[i]\n",
    "            L2 = models[1].layers[i]\n",
    "            Lm = alpha * L1 + beta * L2\n",
    "            h1 = L1(h)\n",
    "            h2 = L2(h)\n",
    "            h = Lm(h)\n",
    "            activations.append({\n",
    "                \"1\": h1, \"2\": h2, \"merged\": copy(h)\n",
    "            })\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        - embed_tokens\n",
    "        - norm\n",
    "        - layers\n",
    "            - input_layernorm\n",
    "            - self_attn\n",
    "            - mlp\n",
    "            - post_attention_norm\n",
    "        - lm_head\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd988e00-b69f-4325-bb22-220a26ef033f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08f9fd928f644f78b889d745defad33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c6256f498f4ba4945141afd50711fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d57ee117-eee6-4233-adfb-58e8b61cd7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e89fa84-fa02-4ea0-847d-1e81d2682ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 16\n",
    "        self.intermediate_size = 32\n",
    "        self.gate_proj = nn.Linear(16, 32, bias=False)\n",
    "        self.up_proj = nn.Linear(16, 32, bias=False)\n",
    "        self.down_proj = nn.Linear(32, 16, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.down_proj(self.gate_proj(x) * self.up_proj(x))\n",
    "        return result\n",
    "\n",
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4912f19f-b0cc-49b1-8a2e-14006fc6f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1 = merger.models[0].model.layers[0].self_attn\n",
    "attn2 = merger.models[1].model.layers[0].self_attn\n",
    "mlp1 = merger.models[0].model.layers[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "999cf77d-18cf-4dfb-ab1a-6dee31b83949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0159, -0.0432, -0.0080,  ...,  0.0081,  0.0096,  0.0132],\n",
      "        [-0.0330,  0.0110,  0.0085,  ...,  0.0226, -0.0082,  0.0457],\n",
      "        [-0.0092,  0.0111, -0.0134,  ...,  0.0298,  0.0113, -0.0038],\n",
      "        ...,\n",
      "        [-0.0085,  0.0601, -0.0325,  ...,  0.0525, -0.0222,  0.0403],\n",
      "        [-0.0374, -0.0325,  0.0620,  ..., -0.0206,  0.0806,  0.0376],\n",
      "        [ 0.0356,  0.0151,  0.0087,  ..., -0.0306, -0.0072,  0.0378]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0811, -2.3125,  2.1406,  ...,  1.0938, -0.2275,  0.6250],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0016, -0.0310, -0.0079,  ..., -0.0417,  0.0113,  0.0325],\n",
      "        [ 0.0315, -0.0189,  0.0791,  ...,  0.0364, -0.0352,  0.0344],\n",
      "        [-0.0115,  0.0466, -0.0225,  ...,  0.0625,  0.0304,  0.0137],\n",
      "        ...,\n",
      "        [-0.0605,  0.0025,  0.0486,  ...,  0.0220, -0.0708, -0.0137],\n",
      "        [ 0.0211,  0.0564,  0.0320,  ...,  0.0352,  0.0991,  0.0146],\n",
      "        [ 0.0618,  0.0339,  0.0016,  ..., -0.0488,  0.0030, -0.0134]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2.9688e+00, -2.5781e-01,  9.3750e-01,  2.7656e+00, -2.7344e+00,\n",
      "         5.7861e-02,  5.3125e-01,  3.7305e-01, -2.4707e-01, -5.4297e-01,\n",
      "        -3.0625e+00, -8.7891e-02,  1.4355e-01, -2.3730e-01, -1.7822e-02,\n",
      "        -3.3789e-01, -1.1230e-01, -3.0273e-01,  1.5234e-01, -1.2158e-01,\n",
      "         1.8672e+00, -5.1270e-02, -3.6719e-01,  6.3965e-02, -1.6113e-01,\n",
      "        -3.6719e-01, -1.2329e-02, -3.0469e+00,  1.2207e-01, -8.3984e-02,\n",
      "        -1.5430e-01, -2.3730e-01,  1.1621e-01, -4.8828e-01,  2.0410e-01,\n",
      "         6.6406e-02,  1.0645e-01,  6.2500e-01, -4.2188e+00,  3.3008e-01,\n",
      "         8.9355e-02, -2.5977e-01,  2.9102e-01,  5.2979e-02,  7.8125e-02,\n",
      "         3.2812e-01, -7.6172e-01,  2.5391e-01,  1.0859e+00, -1.7125e+01,\n",
      "        -4.3125e+00, -6.2109e-01,  6.6875e+00, -6.7812e+00,  4.5625e+00,\n",
      "        -1.1812e+01,  1.4125e+01, -2.3250e+01,  4.6250e+00,  9.3750e+00,\n",
      "        -3.2500e+01,  1.9375e+01,  4.2000e+01,  5.1875e+00,  3.2969e+00,\n",
      "        -2.5625e+00,  2.6562e+00,  2.9883e-01, -1.4062e+00, -3.8086e-01,\n",
      "        -4.4336e-01,  6.2891e-01, -3.4961e-01, -1.9922e-01,  3.5547e-01,\n",
      "        -1.5320e-02, -2.1387e-01, -2.8750e+00,  8.1055e-02,  6.6406e-02,\n",
      "         6.3477e-02,  6.1719e-01, -1.6846e-02,  2.4609e-01,  4.1875e+00,\n",
      "         1.5430e-01, -2.6562e-01, -6.8359e-01,  4.9609e-01, -3.0078e-01,\n",
      "        -6.5430e-02, -4.1875e+00,  8.5449e-02, -4.1992e-01, -3.3984e-01,\n",
      "        -7.6660e-02, -4.4434e-02,  1.7344e+00, -4.2578e-01, -2.2656e-01,\n",
      "        -1.1426e-01,  3.7305e-01,  8.0000e+00, -2.1851e-02,  1.5625e-01,\n",
      "        -2.4707e-01,  2.5000e-01,  4.8242e-01,  2.3047e-01,  9.6875e-01,\n",
      "         7.2656e-01,  1.1406e+00,  1.0234e+00, -3.1750e+01,  1.5859e+00,\n",
      "        -1.3188e+01,  5.2734e-01,  4.4336e-01,  8.1875e+00, -3.6523e-01,\n",
      "         8.5000e+00, -1.7500e+01, -2.3875e+01, -5.3750e+01,  2.5250e+01,\n",
      "         5.7000e+01,  2.5250e+01,  8.7000e+01, -1.8359e+00,  2.0801e-01,\n",
      "         6.9922e-01,  1.4375e+00,  4.3750e-01,  5.9766e-01, -1.6016e-01,\n",
      "        -1.5625e-01,  3.0781e+00, -7.8613e-02, -2.0312e-01, -1.4160e-01,\n",
      "         2.8320e-01, -9.3262e-02,  8.0469e-01,  4.1602e-01,  1.5312e+00,\n",
      "         1.9727e-01,  2.2168e-01, -2.5391e-01,  3.7500e+00,  1.3184e-01,\n",
      "        -5.3955e-02, -6.3672e-01,  1.2422e+00, -2.9102e-01, -5.4932e-02,\n",
      "         3.6719e+00,  5.5859e-01, -3.9258e-01, -1.0625e+00,  4.0625e-01,\n",
      "        -1.8945e-01, -2.4688e+00,  7.8613e-02,  1.2793e-01,  1.8164e-01,\n",
      "        -1.9609e+00,  1.8066e-01, -4.0283e-02,  2.7539e-01,  2.1729e-02,\n",
      "         1.8125e+00, -2.0801e-01, -2.5781e-01,  4.9609e-01, -1.3477e-01,\n",
      "         3.3594e-01, -4.2773e-01, -8.9844e-02, -1.8750e+01,  1.0000e+00,\n",
      "         5.1172e-01, -1.7250e+01, -2.4500e+01, -3.0469e-01, -9.8750e+00,\n",
      "        -3.1000e+01, -2.2250e+01, -3.2500e+01,  4.2188e-01, -3.1375e+01,\n",
      "        -7.4500e+01, -7.2000e+01, -2.8438e+00, -3.4219e+00,  2.0801e-01,\n",
      "        -2.3438e+00, -8.3203e-01, -4.5703e-01,  2.5156e+00, -1.8555e-01,\n",
      "         1.9043e-01, -4.9805e-01,  2.1484e-02, -4.1504e-02, -2.2656e+00,\n",
      "        -6.1523e-02, -4.5312e-01,  5.6641e-01,  3.2227e-02, -1.5039e-01,\n",
      "         2.7148e-01,  1.4746e-01, -1.8203e+00,  1.1768e-01,  2.5195e-01,\n",
      "        -2.1289e-01,  2.9297e-01,  2.2827e-02, -2.5586e-01, -2.5156e+00,\n",
      "        -3.8477e-01, -8.4961e-02, -8.5547e-01,  6.1768e-02,  1.7285e-01,\n",
      "        -3.7031e+00, -1.7871e-01, -2.9492e-01,  1.1230e-02, -1.1094e+00,\n",
      "        -3.1250e-01,  6.6406e-02, -7.7734e-01, -1.5820e-01,  7.3438e-01,\n",
      "         3.0859e-01,  1.6504e-01, -4.1992e-02, -7.7637e-02, -3.0664e-01,\n",
      "         2.9688e-01, -5.2734e-01,  1.8625e+01, -1.0875e+01, -9.3262e-02,\n",
      "         1.4688e+01,  1.9750e+01,  1.4141e+00, -1.2312e+01, -3.1562e+00,\n",
      "         1.6000e+01,  4.3000e+01,  6.2000e+01,  4.6750e+01, -6.9000e+01,\n",
      "         9.1500e+01], device='cuda:0', dtype=torch.bfloat16,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 8.7738e-05,  1.5259e-02, -8.3618e-03,  ..., -1.0803e-02,\n",
      "          1.3000e-02,  1.2817e-02],\n",
      "        [ 6.8054e-03, -2.5391e-02,  1.8677e-02,  ...,  4.3297e-04,\n",
      "          2.0752e-02,  4.3640e-03],\n",
      "        [-3.0518e-02,  2.7954e-02, -2.8076e-02,  ...,  5.2185e-03,\n",
      "         -1.4648e-02,  4.8096e-02],\n",
      "        ...,\n",
      "        [-6.8359e-03,  1.7578e-02, -1.2756e-02,  ...,  2.1118e-02,\n",
      "          1.1902e-02, -8.6670e-03],\n",
      "        [-3.1494e-02, -2.5330e-03, -1.2329e-02,  ...,  1.5869e-02,\n",
      "         -3.9307e-02,  2.1729e-02],\n",
      "        [ 6.2256e-03,  3.2043e-03, -3.5858e-03,  ...,  4.0588e-03,\n",
      "          1.9043e-02,  6.5002e-03]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 6.3965e-02, -1.8555e-02, -1.7212e-02,  8.9722e-03, -2.3438e-02,\n",
      "         6.4392e-03,  1.7456e-02, -2.5513e-02,  1.0254e-02, -5.0049e-02,\n",
      "        -3.2349e-03,  8.9355e-02, -6.6406e-02, -1.7944e-02, -1.0559e-02,\n",
      "        -2.1973e-02,  1.2512e-02,  1.6602e-02,  4.1504e-03,  1.9043e-02,\n",
      "        -3.8330e-02, -1.9531e-02, -6.2500e-02, -1.2329e-02, -9.5825e-03,\n",
      "         2.9663e-02, -1.9165e-02,  7.9727e-04,  2.7924e-03,  6.8848e-02,\n",
      "         6.4453e-02, -3.7598e-02, -3.7598e-02, -2.1484e-02, -6.7383e-02,\n",
      "        -1.5381e-02,  1.2131e-03, -2.3651e-03,  2.2339e-02, -8.9722e-03,\n",
      "         6.5002e-03,  1.0254e-01, -6.9824e-02,  3.8086e-02,  7.4707e-02,\n",
      "        -2.4048e-02,  2.4048e-02, -7.6904e-03,  4.4678e-02, -5.8594e-02,\n",
      "        -1.0498e-01,  2.1973e-02, -2.5269e-02, -1.9409e-02,  7.6599e-03,\n",
      "         7.2937e-03,  8.9355e-02,  6.5918e-02, -2.5391e-02, -3.3936e-02,\n",
      "         1.3123e-02, -3.8757e-03, -6.6406e-02, -6.6895e-02, -3.1982e-02,\n",
      "         6.5918e-02,  1.0559e-02,  5.7983e-04, -2.4780e-02, -4.7852e-02,\n",
      "        -3.0518e-03,  3.6163e-03, -6.7871e-02, -1.7212e-02,  7.2327e-03,\n",
      "         1.8188e-02,  4.9219e-01,  2.7100e-02,  3.6621e-02,  1.0449e-01,\n",
      "         3.3936e-02, -1.9836e-03,  1.0254e-02,  6.1768e-02,  5.1514e-02,\n",
      "         8.4473e-02,  8.1177e-03,  1.6968e-02,  1.2024e-02, -1.2390e-02,\n",
      "        -1.2894e-03,  6.5002e-03,  1.8768e-03,  1.8311e-02,  6.7871e-02,\n",
      "        -1.9897e-02,  3.9062e-02,  3.0762e-02,  4.9316e-02, -3.2471e-02,\n",
      "        -4.0283e-03,  3.0273e-02, -3.2227e-02,  3.7231e-03,  3.2227e-02,\n",
      "         2.6489e-02, -8.8867e-02, -5.1270e-02,  1.0303e-01, -2.4292e-02,\n",
      "        -7.7148e-02, -5.2002e-02, -5.7617e-02,  2.7832e-02, -4.1992e-02,\n",
      "         3.5889e-02,  2.2949e-02,  3.1433e-03, -7.5989e-03,  3.3691e-02,\n",
      "         1.0010e-02, -1.4221e-02,  3.4180e-02, -1.2024e-02, -5.2002e-02,\n",
      "         3.8330e-02,  1.7578e-02, -6.4087e-03, -2.4048e-02, -6.0059e-02,\n",
      "         1.6724e-02, -2.2217e-02,  1.4526e-02,  5.1025e-02, -3.7354e-02,\n",
      "        -8.2397e-04, -1.1047e-02, -6.2180e-04, -8.2520e-02, -8.7280e-03,\n",
      "         1.8188e-02,  1.9287e-02, -9.3384e-03,  3.8574e-02,  5.6396e-02,\n",
      "         5.3711e-02, -6.1646e-03, -4.8218e-03, -2.2095e-02, -2.5391e-02,\n",
      "        -1.0938e+00, -1.0071e-02,  3.9062e-02,  1.7456e-02,  9.1553e-03,\n",
      "         9.4604e-03, -1.5442e-02,  3.0518e-02,  4.1016e-02,  6.4087e-03,\n",
      "         2.9175e-02,  3.6774e-03,  3.1128e-02, -1.9775e-02,  2.7222e-02,\n",
      "        -1.1292e-02,  2.2095e-02,  2.4170e-02, -6.6895e-02, -2.3926e-02,\n",
      "         2.6550e-03, -4.2480e-02,  6.8359e-03,  5.0537e-02,  2.1515e-03,\n",
      "         1.3733e-02, -1.8433e-02, -4.1260e-02, -7.9590e-02, -1.2131e-03,\n",
      "         2.6123e-02, -4.8096e-02, -6.5430e-02,  9.8419e-04,  7.3853e-03,\n",
      "        -5.0781e-02,  6.1279e-02,  9.5825e-03, -2.0142e-03,  1.9653e-02,\n",
      "        -3.6316e-03, -1.2451e-02,  9.6436e-03, -8.5449e-03, -4.0771e-02,\n",
      "         7.2021e-03,  1.6357e-02,  5.7983e-03,  1.8677e-02, -1.0010e-02,\n",
      "         1.7944e-02, -5.2734e-02,  5.8594e-03, -5.3223e-02, -2.5269e-02,\n",
      "         1.7212e-02, -6.3477e-02,  2.0630e-02, -2.6245e-02, -4.5410e-02,\n",
      "        -6.1646e-03,  5.5542e-03,  2.0386e-02, -2.8809e-02,  2.2656e+00,\n",
      "        -5.2795e-03, -1.8799e-02, -1.3794e-02, -2.7832e-02,  2.5269e-02,\n",
      "         2.4536e-02,  4.5654e-02,  1.6602e-02,  1.3855e-02,  1.5198e-02,\n",
      "        -1.1597e-02, -2.2095e-02, -4.5654e-02,  1.0910e-03, -1.6968e-02,\n",
      "         4.8340e-02,  1.1536e-02,  2.3682e-02, -7.1106e-03,  6.3965e-02,\n",
      "        -5.4932e-02, -9.7168e-02, -3.6133e-02, -8.3496e-02,  1.4954e-02,\n",
      "        -5.9082e-02,  1.1475e-01,  5.4199e-02, -2.4414e-02,  1.6602e-02,\n",
      "         6.3477e-03,  1.5564e-02,  3.2471e-02, -5.1270e-02,  1.9775e-02,\n",
      "        -1.8692e-03,  7.0496e-03, -7.0312e-02,  7.1106e-03,  9.8419e-04,\n",
      "         1.0071e-02], device='cuda:0', dtype=torch.bfloat16,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0049, -0.0255, -0.0054,  ..., -0.0152, -0.0078, -0.0137],\n",
      "        [-0.0238, -0.0102,  0.0184,  ...,  0.0080,  0.0049, -0.0505],\n",
      "        [ 0.0474, -0.0060, -0.0162,  ...,  0.0242,  0.0034,  0.0054],\n",
      "        ...,\n",
      "        [-0.0085, -0.0286, -0.0160,  ..., -0.0047,  0.0146,  0.0009],\n",
      "        [ 0.0021, -0.0066, -0.0010,  ...,  0.0132,  0.0386,  0.0267],\n",
      "        [-0.0461,  0.0118, -0.0084,  ..., -0.0115,  0.0100,  0.0300]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for x in attn1.parameters():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087146ef-d65d-4d6d-9536-026dc97ebaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def merge_mini(net1, net2):\n",
    "    net = deepcopy(net1)\n",
    "    for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "286a1532-4644-4142-ab2b-029b4d4fefd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3750, -0.3301, -0.0016,  ..., -0.0093, -0.2695,  0.0986],\n",
       "          [ 0.2930, -0.2949,  0.0933,  ...,  0.0483, -0.2949, -0.0649],\n",
       "          [ 0.2676, -0.3789,  0.1973,  ...,  0.0134, -0.7227,  0.1367],\n",
       "          [ 0.2715, -0.2988, -0.0547,  ..., -0.1777, -0.7695,  0.0850]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda:0\"\n",
    "h = torch.rand(1, 4, 2048, dtype=torch.bfloat16).to(device)\n",
    "p = torch.arange(4, dtype=torch.bfloat16, device=device).unsqueeze(0)\n",
    "attn1.forward(h, position_ids=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93147db6-84be-43db-9559-dd03c4c5988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, param_bits):\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    non_trainable_params = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        num_params = param.numel()  # Get the number of elements in the parameter\n",
    "        total_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "        else:\n",
    "            non_trainable_params += num_params\n",
    "\n",
    "    total_gigabytes = total_params * (param_bits / 8) / (1024**3)\n",
    "    memory = f\"{total_gigabytes:.2f} GB\"\n",
    "    \n",
    "    return total_params, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a75cc9e3-c80c-4a70-9d0c-b71da5fa1cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9439744, '0.02 GB'), (67633152, '0.13 GB'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(attn1, 16), count_parameters(mlp1, 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
