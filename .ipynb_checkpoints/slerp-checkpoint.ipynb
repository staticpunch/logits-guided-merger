{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72349745-521e-4d25-8385-c7fff456a919",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfef20f-ccb1-4818-9fe0-46e986ceb67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8121cb467b4647aef02ac03c8caea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "\tAutoModelForCausalLM, \n",
    "\tAutoTokenizer, \n",
    "\tAutoConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from peft import PeftModel\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# model_id = \"../model_hub/Qwen2.5-3B-Instruct/\"\n",
    "model_id = \"../models/merged-test2/\"\n",
    "# model_id = \"../models/Llama-3.1-8B-Stheno-v3.4\"\n",
    "# model_id = \"../models/Meta-Llama-3.1-8B-Instruct/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # config=AutoConfig.from_pretrained(model_id),\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch_dtype,\n",
    "    # load_in_8bit=True\n",
    ")\n",
    "# model = PeftModel.from_pretrained(model, peft_dolphin)\n",
    "# model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d1cac5-d6c7-4b94-9696-9482c83b0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb6740ee-8eea-41a1-b271-682f9c30f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who created you?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "747975d7-662d-4b32-867d-55240458bfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was created by Meta AI, which is a part of Meta Platforms, Inc. My primary function is to assist users with information and tasks to the best of my abilities based on my training data. I'm constantly learning and improving through machine learning algorithms.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8745f3ea-36fd-43e8-b832-659d43d8b663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# np.degrees(0.00004)\n",
    "np.arccos(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37975fe-0a23-46f1-bd50-7fefe1b241bb",
   "metadata": {},
   "source": [
    "## Mergekit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61266a2a-7a05-4c5f-8c5a-a5ede4771f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually do merge\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.merge import MergeOptions, run_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a2811a-bcc7-42bf-a2c4-7fb22af3b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../models/merged-test\"  # folder to store the result in\n",
    "LORA_MERGE_CACHE = \"/tmp\"  # change if you want to keep these for some reason\n",
    "CONFIG_YML = \"slerp-config.yaml\"  # merge configuration file\n",
    "COPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\n",
    "LAZY_UNPICKLE = False  # experimental low-memory model loader\n",
    "LOW_CPU_MEMORY = False  # enable if you somehow have more VRAM than RAM+swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71e44dd-c558-4e6f-befe-6d8df94e0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
    "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\n",
    "\n",
    "options=MergeOptions(\n",
    "\tlora_merge_cache=LORA_MERGE_CACHE,\n",
    "\t# cuda=torch.cuda.is_available(),\n",
    "\tcopy_tokenizer=COPY_TOKENIZER,\n",
    "\tlazy_unpickle=LAZY_UNPICKLE,\n",
    "\tlow_cpu_memory=LOW_CPU_MEMORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204cee81-58bb-46ad-90cc-8f3c8628b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup loader cache: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 728.49it/s]\n",
      "Executing graph: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1457/1457 [03:49<00:00,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_merge(\n",
    "    merge_config,\n",
    "    out_path=OUTPUT_PATH,\n",
    "    options=MergeOptions(\n",
    "        lora_merge_cache=LORA_MERGE_CACHE,\n",
    "        # cuda=torch.cuda.is_available(),\n",
    "        copy_tokenizer=COPY_TOKENIZER,\n",
    "        lazy_unpickle=LAZY_UNPICKLE,\n",
    "        low_cpu_memory=LOW_CPU_MEMORY,\n",
    "    ),\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cb2aa-2334-434c-8f49-cd3d22f97135",
   "metadata": {},
   "source": [
    "## From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd26f11-709c-4037-9936-39281d651bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d60c19-017b-4da0-988f-ad669ca2bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(\n",
    "    t: float, v0: Union[np.ndarray, torch.Tensor], v1: Union[np.ndarray, torch.Tensor]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    return (1 - t) * v0 + t * v1\n",
    "\n",
    "\n",
    "def slerp(\n",
    "    t: Union[float, np.ndarray],\n",
    "    v0: Union[np.ndarray, torch.Tensor],\n",
    "    v1: Union[np.ndarray, torch.Tensor],\n",
    "    DOT_THRESHOLD: float = 0.9995,\n",
    "    eps: float = 1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Spherical linear interpolation\n",
    "\n",
    "    From: https://gist.github.com/dvschultz/3af50c40df002da3b751efab1daddf2c\n",
    "    Args:\n",
    "        t (float/np.ndarray): Float value between 0.0 and 1.0\n",
    "        v0 (np.ndarray): Starting vector\n",
    "        v1 (np.ndarray): Final vector\n",
    "        DOT_THRESHOLD (float): Threshold for considering the two vectors as\n",
    "                               colinear. Not recommended to alter this.\n",
    "    Returns:\n",
    "        v2 (np.ndarray): Interpolation vector between v0 and v1\n",
    "    \"\"\"\n",
    "    is_torch = False\n",
    "    if not isinstance(v0, np.ndarray):\n",
    "        is_torch = True\n",
    "        v0 = v0.detach().cpu().float().numpy()\n",
    "    if not isinstance(v1, np.ndarray):\n",
    "        is_torch = True\n",
    "        v1 = v1.detach().cpu().float().numpy()\n",
    "\n",
    "    # Copy the vectors to reuse them later\n",
    "    v0_copy = np.copy(v0)\n",
    "    v1_copy = np.copy(v1)\n",
    "\n",
    "    # Normalize the vectors to get the directions and angles\n",
    "    v0 = normalize(v0, eps)\n",
    "    v1 = normalize(v1, eps)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "\n",
    "    # Dot product with the normalized vectors (can't use np.dot in W)\n",
    "    dot = np.sum(v0 * v1)\n",
    "\n",
    "    # If absolute value of dot product is almost 1, vectors are ~colinear, so use lerp\n",
    "    if np.abs(dot) > DOT_THRESHOLD:\n",
    "        res = lerp(t, v0_copy, v1_copy)\n",
    "        return maybe_torch(res, is_torch)\n",
    "\n",
    "    # Calculate initial angle between v0 and v1\n",
    "    theta_0 = np.arccos(dot)\n",
    "    sin_theta_0 = np.sin(theta_0)\n",
    "\n",
    "    # Angle at timestep t\n",
    "    theta_t = theta_0 * t\n",
    "    sin_theta_t = np.sin(theta_t)\n",
    "\n",
    "    # Finish the slerp algorithm\n",
    "    s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
    "    s1 = sin_theta_t / sin_theta_0\n",
    "    res = s0 * v0_copy + s1 * v1_copy\n",
    "\n",
    "    return maybe_torch(res, is_torch)\n",
    "\n",
    "\n",
    "def maybe_torch(v: np.ndarray, is_torch: bool):\n",
    "    if is_torch:\n",
    "        return torch.from_numpy(v)\n",
    "    return v\n",
    "\n",
    "\n",
    "def normalize(v: np.ndarray, eps: float):\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_v > eps:\n",
    "        v = v / norm_v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0923421d-d2e4-42c2-8e40-3b6292611013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensors(path, signature):\n",
    "    state_dict = {}\n",
    "    shard_paths = [f for f in os.listdir(path) if f.endswith('.safetensors')]\n",
    "    for shard_path in sorted(shard_paths, key=lambda x: int(x.split('-')[1])):\n",
    "        apath = os.path.join(path, shard_path)\n",
    "        with safe_open(apath, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for key in f.keys():\n",
    "                if signature in key:\n",
    "                    state_dict[key] = f.get_tensor(key)\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40d21f8-a5bd-437a-a85a-3a3fbf489001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sources': [{'model': '/workspace/models/Meta-Llama-3.1-8B-Instruct'},\n",
       "  {'model': '/workspace/models/Llama-3.1-8B-Stheno-v3.4'}],\n",
       " 'layer_range': [0, 32],\n",
       " 'merge_method': 'slerp',\n",
       " 'base_model': '/workspace/models/Meta-Llama-3.1-8B-Instruct',\n",
       " 'output_dir': '/workspace/models/merged-test2',\n",
       " 'parameters': {'t': [{'filter': 'self_attn', 'value': [0, 0.5, 0.3, 0.7, 1]},\n",
       "   {'filter': 'mlp', 'value': [1, 0.5, 0.7, 0.3, 0]},\n",
       "   {'filter': 'fallback_value', 'value': 0.5}]},\n",
       " 'dtype': 'bfloat16'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "CONFIG_FILE = \"slerp-config-custom.yaml\"\n",
    "\n",
    "with open(CONFIG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    merge_config = yaml.safe_load(f)\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6654287-a206-4e66-b506-9f1f218b09e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.embed_tokens',\n",
       " 'lm_head',\n",
       " 'model.norm',\n",
       " 'layers.0.',\n",
       " 'layers.1.',\n",
       " 'layers.2.',\n",
       " 'layers.3.',\n",
       " 'layers.4.',\n",
       " 'layers.5.',\n",
       " 'layers.6.',\n",
       " 'layers.7.',\n",
       " 'layers.8.',\n",
       " 'layers.9.',\n",
       " 'layers.10.',\n",
       " 'layers.11.',\n",
       " 'layers.12.',\n",
       " 'layers.13.',\n",
       " 'layers.14.',\n",
       " 'layers.15.',\n",
       " 'layers.16.',\n",
       " 'layers.17.',\n",
       " 'layers.18.',\n",
       " 'layers.19.',\n",
       " 'layers.20.',\n",
       " 'layers.21.',\n",
       " 'layers.22.',\n",
       " 'layers.23.',\n",
       " 'layers.24.',\n",
       " 'layers.25.',\n",
       " 'layers.26.',\n",
       " 'layers.27.',\n",
       " 'layers.28.',\n",
       " 'layers.29.',\n",
       " 'layers.30.',\n",
       " 'layers.31.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from safetensors import safe_open\n",
    "\n",
    "def get_layer_signatures(path):\n",
    "    \"\"\"\n",
    "    Extracts and organizes layer signatures from safetensors files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the directory containing safetensors files.\n",
    "\n",
    "    Returns:\n",
    "        list: A combined list of non-layer and sorted layer signatures.\n",
    "    \"\"\"\n",
    "    def extract_layer_signature(key):\n",
    "        key = key.strip(\".weight\")\n",
    "        match = re.search(r\"layers\\.[^\\.]*\\.\", key)\n",
    "        return match.group(0) if match else key\n",
    "\n",
    "    # Collect all keys from safetensors files\n",
    "    shard_paths = sorted(\n",
    "        (f for f in os.listdir(path) if f.endswith('.safetensors')),\n",
    "        key=lambda x: int(x.split('-')[1])\n",
    "    )\n",
    "    keys = []\n",
    "    for shard_path in shard_paths:\n",
    "        full_path = os.path.join(path, shard_path)\n",
    "        with safe_open(full_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            keys.extend(f.keys())\n",
    "\n",
    "    # Separate and process keys\n",
    "    block_signatures = sorted(\n",
    "        {extract_layer_signature(key) for key in keys if \"layers\" in key},\n",
    "        key=lambda x: int(x.split(\".\")[-2])\n",
    "    )\n",
    "    other_signatures = [extract_layer_signature(key) for \n",
    "                        key in keys if \"layers\" not in key]\n",
    "\n",
    "    return other_signatures + block_signatures\n",
    "\n",
    "\n",
    "layer_signatures = get_layer_signatures('/workspace/models/Meta-Llama-3.1-8B-Instruct')\n",
    "layer_signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e2c859-69e4-4fd9-88ae-74b662e3b4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filter': 'self_attn', 'value': [0, 0.5, 0.3, 0.7, 1]},\n",
       " {'filter': 'mlp', 'value': [1, 0.5, 0.7, 0.3, 0]},\n",
       " {'filter': 'fallback_value', 'value': 0.5}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config[\"parameters\"]['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952714a1-904d-46c0-b73f-5ee1d0389653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_size(model_path):\n",
    "    files = [os.path.join(model_path, filename)\n",
    "            for filename in os.listdir(model_path)\n",
    "            if filename.endswith(\".safetensors\")]\n",
    "    \n",
    "    total_size = sum(\n",
    "        f.get_tensor(key).nbytes\n",
    "        for file in files\n",
    "        for f in [safe_open(file, framework=\"pt\", device=\"cpu\")]\n",
    "        for key in f.keys()\n",
    "    )\n",
    "    return total_size\n",
    "\n",
    "def make_weightmap(directory):\n",
    "    files = [f for f in os.listdir(directory) \n",
    "             if f.startswith(\"shard_\") and f.endswith(\".safetensors\")]\n",
    "    num_shards = len(files)\n",
    "    files.sort()  # Ensure proper ordering of shard files\n",
    "    model_index = {\n",
    "        \"metadata\": {\n",
    "            \"total_size\": calculate_model_size(directory)\n",
    "        },\n",
    "        \"weight_map\": {}\n",
    "    }\n",
    "    for index, file in enumerate(files, start=1):\n",
    "        old_path = os.path.join(directory, file)\n",
    "        new_name = f\"model-{index:05d}-of-{num_shards:05d}.safetensors\"\n",
    "        new_path = os.path.join(directory, new_name)\n",
    "        os.rename(old_path, new_path)\n",
    "        \n",
    "        with safe_open(new_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for key in f.keys():\n",
    "                model_index[\"weight_map\"][key] = new_name\n",
    "                \n",
    "    outfile = os.path.join(directory, \"model.safetensors.index.json\")\n",
    "    with open(outfile, \"w\") as f:\n",
    "        json.dump(model_index, f, indent=4)\n",
    "        \n",
    "    print(f\"Saved model weight map to {outfile}.\")\n",
    "    \n",
    "def shard_model(tmp_dir, output_dir, size_limit=2*1024*1024*1024):\n",
    "    files = [os.path.join(tmp_dir, filename)\n",
    "            for filename in sorted(os.listdir(tmp_dir))\n",
    "            if filename.endswith(\".safetensors\")]\n",
    "    \n",
    "    shard, shard_size, idx = {}, 0, 1\n",
    "    for file in files:\n",
    "        with safe_open(file, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for key in f.keys():\n",
    "                tensor = f.get_tensor(key)\n",
    "                tensor_size = tensor.nbytes\n",
    "                if shard_size + tensor_size > size_limit:\n",
    "                    output_file = os.path.join(output_dir, f\"shard_{idx}.safetensors\")\n",
    "                    save_file(shard, output_file, metadata={\"format\":\"pt\"})\n",
    "                    print(f\"Saved {output_file} with size {shard_size / 1024**3:.2f} GB\")\n",
    "\n",
    "                    # start a new shard\n",
    "                    shard, shard_size = {}, 0\n",
    "                    idx += 1\n",
    "                    \n",
    "                # add new tensor to the current shard\n",
    "                shard[key] = tensor\n",
    "                shard_size += tensor_size\n",
    "                \n",
    "    # save the last shard\n",
    "    if shard:\n",
    "        output_file = os.path.join(output_dir, f\"shard_{idx}.safetensors\")\n",
    "        save_file(shard, output_file, metadata={\"format\":\"pt\"})\n",
    "        print(f\"Saved {output_file} with size {shard_size / 1024**3:.2f} GB\")\n",
    "\n",
    "    # rename and add model.safetensors.index.json\n",
    "    make_weightmap(output_dir)\n",
    "\n",
    "def copy_small_files(base_path, output_dir):\n",
    "    print(f\"Copying files to {output_dir}\")\n",
    "    files_to_copy = [\n",
    "        \"config.json\",\n",
    "        \"generation_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"tokenizer.json\"\n",
    "    ]\n",
    "    for filename in files_to_copy:\n",
    "        src_path = os.path.join(base_path, filename)\n",
    "        dst_path = os.path.join(output_dir, filename)\n",
    "        try:\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {filename} not found in {base_path}. Skipping.\")    \n",
    "    \n",
    "def save_checkpoint(config):\n",
    "    base_path = config['sources'][0]['model']\n",
    "    output_dir = config[\"output_dir\"]\n",
    "    tmp_dir = os.path.join(output_dir, \"tmp_dir\")\n",
    "\n",
    "    ## copying files\n",
    "    copy_small_files(base_path, output_dir)\n",
    "\n",
    "    ## sharding model weights\n",
    "    shard_model(tmp_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7043c7f2-f5af-4fe8-8aef-6a50d35c6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_merge(\n",
    "    merge_config\n",
    "):\n",
    "    ## Read configs\n",
    "    model_paths = [x['model'] for x in merge_config['sources']]\n",
    "    layer_signatures = get_layer_signatures(model_paths[0])\n",
    "    output_dir = merge_config[\"output_dir\"]\n",
    "    tmp_dir = os.path.join(output_dir, \"tmp_dir\")\n",
    "        \n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.makedirs(tmp_dir)\n",
    "\n",
    "    ## Merge models\n",
    "    for signature in (\n",
    "        pbar := tqdm(\n",
    "            layer_signatures,\n",
    "            desc=\"Merging ...\",\n",
    "            ncols=150\n",
    "        )\n",
    "    ):\n",
    "        pbar.set_description(f\"Merging {signature}\")\n",
    "        models_tensors = [load_tensors(path, signature) for path in model_paths]\n",
    "        # result = pseudo_merge_layer(models_tensors, merge_config)\n",
    "        # for k, v in result.items():\n",
    "        #     print(f\"{k}: {v}\")\n",
    "        # print(\"---\" * 20)\n",
    "        \n",
    "        merged_tensors = merge_layer(models_tensors, merge_config)\n",
    "        \n",
    "        # label = load_tensors(\"../models/merged-test/\", signature)\n",
    "        # for key in label:\n",
    "        #     try:\n",
    "        #         torch.testing.assert_close(\n",
    "        #             merged_tensors[key],\n",
    "        #             label[key]\n",
    "        #         )\n",
    "        #     except:\n",
    "        #         print(key)\n",
    "        outfile = os.path.join(tmp_dir, f\"{signature.strip('.')}.safetensors\")\n",
    "        save_file(merged_tensors, outfile)\n",
    "\n",
    "    ## Save models\n",
    "    save_checkpoint(merge_config)\n",
    "\n",
    "\n",
    "def pseudo_merge_layer(tensors, merge_config):\n",
    "    \"\"\"\n",
    "    Currently merge layer using SLERP.\n",
    "    \"\"\"\n",
    "    assert len(tensors) == 2 \n",
    "    conditions = merge_config[\"parameters\"][\"t\"]\n",
    "    conditions = {c['filter']: c['value'] for c in conditions}\n",
    "\n",
    "    layer_begin, layer_end = merge_config[\"layer_range\"]\n",
    "    num_layers = layer_end - layer_begin\n",
    "    \n",
    "    weight_names = [key for key in tensors[0].keys()]\n",
    "    result = {}\n",
    "    for weight_name in weight_names:\n",
    "        t = compute_t(weight_name, conditions, num_layers)\n",
    "        result.update({weight_name: t})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f84cb9-b860-43be-bc3d-7e2aeb456b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_layer(tensors, merge_config):\n",
    "    \"\"\"\n",
    "    Currently merge layer using SLERP.\n",
    "    \"\"\"\n",
    "    assert len(tensors) == 2\n",
    "    conditions = merge_config[\"parameters\"][\"t\"]\n",
    "    conditions = {c['filter']: c['value'] for c in conditions}\n",
    "\n",
    "    layer_begin, layer_end = merge_config[\"layer_range\"]\n",
    "    num_layers = layer_end - layer_begin\n",
    "    \n",
    "    weight_names = [key for key in tensors[0].keys()]\n",
    "    \n",
    "    for weight_name in weight_names:\n",
    "        t = compute_t(weight_name, conditions, num_layers)\n",
    "        tensor_a = tensors[0][weight_name]\n",
    "        tensor_b = tensors[1][weight_name]\n",
    "        \n",
    "        # tensor_merged = tensors_merged[name]\n",
    "        tensor_computed = (\n",
    "            slerp(\n",
    "                t,\n",
    "                tensor_a,\n",
    "                tensor_b,\n",
    "            )\n",
    "            .to(tensor_a.dtype)\n",
    "            .to(tensor_a.device)\n",
    "        )\n",
    "        tensors[0][weight_name] = tensor_computed\n",
    "        # torch.testing.assert_close(tensor_merged, tensor_computed)\n",
    "    return tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b5103c-72d0-4fc3-a0ea-84a5b910f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_t(weight_name, conditions, num_layers):\n",
    "    \"\"\"\n",
    "    gradient\n",
    "    \"\"\"\n",
    "    anchors = conditions.get(\"fallback_value\")\n",
    "    if not isinstance(anchors, list):\n",
    "        anchors = [anchors]\n",
    "\n",
    "    for filter_name in conditions.keys():\n",
    "        if filter_name in weight_name:\n",
    "            anchors = conditions.get(filter_name)\n",
    "            break\n",
    "            \n",
    "    match = re.search(r\"layers\\.([^\\.]*)\\.\", weight_name)\n",
    "    if match:\n",
    "        layer_idx = int(match.group(1))\n",
    "        layer_t = layer_idx / (num_layers - 1)\n",
    "        scaled = layer_t * (len(anchors) - 1)\n",
    "        i0 = math.floor(scaled)\n",
    "        i1 = min(len(anchors) - 1, i0 + 1)\n",
    "        frac = scaled - i0\n",
    "        \n",
    "        blend_value = (1 - frac) * anchors[i0] + frac * anchors[i1]\n",
    "    else:\n",
    "        blend_value = anchors[0]\n",
    "        \n",
    "    return blend_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2f81a9b-da78-4ba0-9ae2-abcd001c047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging layers.31.: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [01:52<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying files to /workspace/models/merged-test2\n",
      "Saved /workspace/models/merged-test2/shard_1.safetensors with size 1.99 GB\n",
      "Saved /workspace/models/merged-test2/shard_2.safetensors with size 1.99 GB\n",
      "Saved /workspace/models/merged-test2/shard_3.safetensors with size 1.92 GB\n",
      "Saved /workspace/models/merged-test2/shard_4.safetensors with size 1.92 GB\n",
      "Saved /workspace/models/merged-test2/shard_5.safetensors with size 1.92 GB\n",
      "Saved /workspace/models/merged-test2/shard_6.safetensors with size 1.99 GB\n",
      "Saved /workspace/models/merged-test2/shard_7.safetensors with size 1.26 GB\n",
      "Saved /workspace/models/merged-test2/shard_8.safetensors with size 1.96 GB\n",
      "Saved model weight map to /workspace/models/merged-test2/model.safetensors.index.json.\n"
     ]
    }
   ],
   "source": [
    "run_merge(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07bd9781-9cae-4684-aa17-3c460b3dafc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying files to /workspace/models/merged-test2\n",
      "Saved /workspace/models/merged-test2/shard_1.safetensors with size 1.99 GB\n",
      "Saved /workspace/models/merged-test2/shard_2.safetensors with size 1.99 GB\n",
      "Saved /workspace/models/merged-test2/shard_3.safetensors with size 1.92 GB\n",
      "Saved /workspace/models/merged-test2/shard_4.safetensors with size 1.92 GB\n",
      "Saved /workspace/models/merged-test2/shard_5.safetensors with size 1.92 GB\n",
      "Saved /workspace/models/merged-test2/shard_6.safetensors with size 1.99 GB\n",
      "Saved /workspace/models/merged-test2/shard_7.safetensors with size 1.26 GB\n",
      "Saved /workspace/models/merged-test2/shard_8.safetensors with size 1.96 GB\n",
      "Saved model weight map to /workspace/models/merged-test2/model.safetensors.index.json.\n"
     ]
    }
   ],
   "source": [
    "save_checkpoint(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b8a2e1c-0edd-40f8-838a-8899e9f38945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [x['model'] for x in merge_config['sources']]\n",
    "signature = \"lm_head\"\n",
    "models_tensors = [load_tensors(path, signature) for path in model_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1a0189ff-c07c-4876-a836-4b1f6360c363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensor_result = merge_layer(models_tensors, merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "646f190d-eab1-4ff7-a91f-00e18129ca12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm_head.weight': tensor([[ 0.0082,  0.0072,  0.0125,  ...,  0.0095, -0.0215, -0.0138],\n",
       "         [-0.0070,  0.0041,  0.0190,  ..., -0.0055,  0.0072, -0.0044],\n",
       "         [ 0.0135,  0.0043,  0.0143,  ...,  0.0006, -0.0087, -0.0150],\n",
       "         ...,\n",
       "         [-0.0038,  0.0020,  0.0034,  ...,  0.0003,  0.0081,  0.0083],\n",
       "         [-0.0038,  0.0020,  0.0034,  ...,  0.0003,  0.0081,  0.0083],\n",
       "         [-0.0038,  0.0020,  0.0034,  ...,  0.0003,  0.0081,  0.0083]],\n",
       "        dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "621cb3c6-655a-49df-b42c-5c1020d6b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_label = load_tensors(\"../models/merged-test/\", signature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
