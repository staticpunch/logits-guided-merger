{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cdad09-d3c0-45e8-8f7d-1e73cac721b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from modeling_qwen2 import (\n",
    "    Qwen2RMSNorm, \n",
    "    Qwen2RotaryEmbedding, \n",
    "    Qwen2MLP, \n",
    "    Qwen2Attention, \n",
    "    Qwen2FlashAttention2, \n",
    "    Qwen2SdpaAttention, \n",
    "    Qwen2DecoderLayer, \n",
    "    Qwen2PreTrainedModel, \n",
    "    Qwen2Model, \n",
    "    Qwen2ForCausalLM,\n",
    ")\n",
    "\n",
    "from configuration_qwen2 import Qwen2Config\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    ")\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e21e342-9873-478b-b3f9-a696247da339",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 24\n",
    "out_features = 50\n",
    "lin = nn.Linear(in_features, out_features, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "308051f6-86ed-41c1-a882-fbfd7b1fe9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904, 0.6009, 0.2566, 0.7936, 0.9408,\n",
       "         0.1332, 0.9346, 0.5936, 0.8694, 0.5677, 0.7411, 0.4294, 0.8854, 0.5739,\n",
       "         0.2666, 0.6274, 0.2696, 0.4414, 0.2969, 0.8317]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "mask = torch.rand(1, in_features) \n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f021097e-dc46-4503-b2f2-4caddd1d9aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1422, -0.0861, -0.0221,  ..., -0.0394,  0.0350,  0.0304],\n",
       "        [ 0.0915, -0.1138, -0.0774,  ..., -0.0537,  0.0545,  0.0566],\n",
       "        [ 0.1733, -0.1541, -0.0775,  ...,  0.0612, -0.0440, -0.0906],\n",
       "        ...,\n",
       "        [ 0.0543,  0.0347, -0.0459,  ...,  0.0071,  0.0598,  0.0986],\n",
       "        [ 0.1008,  0.0748,  0.0605,  ..., -0.0351, -0.0046, -0.0413],\n",
       "        [-0.0910, -0.0593, -0.0283,  ...,  0.0228,  0.0181, -0.1110]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask * lin.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b39d8d-c4dd-4231-89a9-6422aa0de49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 10:24:20,672 - INFO - Comparing tokenizer at /workspace/models/Arcee-VyLinh/ with tokenizer at /workspace/models/Qwen2.5-Coder-3B/\n",
      "2024-12-16 10:24:20,675 - INFO - Tokenizer at /workspace/models/Arcee-VyLinh/ and /workspace/models/Qwen2.5-Coder-3B/ are the same based on the defined criteria\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import are_tokenizers_same\n",
    "are_tokenizers_same(\n",
    "    paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbb9c5f-c766-441f-80f7-c37014ffd078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer(path, layer_idx=33):\n",
    "\tstate_dict = {}\n",
    "\tshard_paths = [f for f in os.listdir(path) if f.endswith('.safetensors')]\n",
    "\tfor shard_path in sorted(shard_paths, key=lambda x: int(x.split('-')[1])):\n",
    "\t\tapath = os.path.join(path, shard_path)\n",
    "\t\twith safe_open(apath, framework=\"pt\", device=\"cpu\") as f:\n",
    "\t\t\tfor key in f.keys():\n",
    "\t\t\t\tif f\"layers.{str(layer_idx)}.\" in key:\n",
    "\t\t\t\t\tstate_dict[key] = f.get_tensor(key)\n",
    "\treturn state_dict\n",
    "\n",
    "def strip_prefix(state_dict, prefix=\"model.layers.\"):\n",
    "    \"\"\"Strips 'model.layers.*.' prefix from 'input_layernorm.weight' keys.\"\"\"\n",
    "    return {\n",
    "      k.replace(f\"{prefix}{k.split('.')[2]}.\", \"\") if k.startswith(prefix)\n",
    "      else k: v for k, v in state_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9560a0b5-7194-4096-8a2c-5d4bddeee216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(\n",
    "    t: float, v0: Union[np.ndarray, torch.Tensor], v1: Union[np.ndarray, torch.Tensor]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    return (1 - t) * v0 + t * v1\n",
    "\n",
    "def weighted_sum(\n",
    "    factors: List[float], \n",
    "    tensors: Union[List[np.ndarray], List[torch.Tensor]]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "\n",
    "    return sum([tensor * factor for tensor, factor in zip(tensors, factors)])\n",
    "\n",
    "def merge_tensors(modules, weight_factors, bias_factors):\n",
    "    param_names = sorted([name for name, _ in modules[0].named_parameters()])\n",
    "    for module in modules:\n",
    "        other_param_names = sorted([name for name, _ in module.named_parameters()])\n",
    "        assert param_names == other_param_names, \"Mismatch tensor names.\"\n",
    "        \n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        if \"weight\" in tensor_name:\n",
    "            factors = weight_factors\n",
    "        elif \"bias\" in tensor_name:\n",
    "            factors = bias_factors\n",
    "        else:\n",
    "            raise ValueError(\"Hey this tensor is neither weight or bias.\")\n",
    "            \n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8d01ac-aef2-440c-86bc-7accd0932b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_factors(components: List[torch.Tensor], strategy=\"naive\"):\n",
    "    if strategy == \"naive\":\n",
    "        n_components = len(components) \n",
    "        random_floats = np.random.rand(n_components)\n",
    "        normalized_floats = random_floats / np.sum(random_floats)\n",
    "        factors = normalized_floats.tolist()\n",
    "    elif strategy == \"slerp\":\n",
    "        raise ValueError(f\"Initialization strategy {strategy} has not been implemented.\")\n",
    "        if len(components) != 2:\n",
    "            raise ValueError(f\"Initialization strategy {strategy.upper()} only works for 2 components.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Initialization strategy {strategy} has not been implemented.\")\n",
    "\n",
    "    return factors\n",
    "\n",
    "def find_modules_to_add_masks(target_module):\n",
    "    module_names_to_replace = []\n",
    "    for parent_name, parent_module in target_module.named_modules():\n",
    "        for name, child in parent_module.named_children():\n",
    "            full_child_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            if isinstance(child, (nn.Linear, nn.Embedding)) or \"RMSNorm\" in type(child).__name__:\n",
    "                module_names_to_replace.append(full_child_name)\n",
    "\n",
    "    return module_names_to_replace\n",
    "\n",
    "def initialize_masks(target_module, ref_modules, strategy=\"naive\"):\n",
    "    \"\"\"\n",
    "    Replaces eligible submodules in target_module with masked versions, \n",
    "    using corresponding modules from ref_modules as a reference for weights.\n",
    "\n",
    "    Args:\n",
    "        target_module: The module in which to replace submodules.\n",
    "        ref_modules: A list of modules to use as a reference for weights.\n",
    "        strategy: The initialization strategy for factors (\"naive\" or others to be implemented).\n",
    "    \"\"\"\n",
    "    module_names_to_replace = find_modules_to_add_masks(target_module)\n",
    "    \n",
    "    for module_name in tqdm(module_names_to_replace, desc=\"Initializing masks\"):\n",
    "        module_names = module_name.split(\".\")\n",
    "        target_child = target_module\n",
    "        ref_children = ref_modules\n",
    "\n",
    "        for m_name in module_names:\n",
    "            target_child = getattr(target_child, m_name)\n",
    "            ref_children = [getattr(ref_module, m_name) for ref_module in ref_children]\n",
    "\n",
    "        num_components = len(ref_modules)\n",
    "        modes = [\"scalar\" for _ in ref_children]\n",
    "\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            biases = [ref.bias.data if ref.bias is not None else None for ref in ref_children]\n",
    "\n",
    "            weight_factors = init_factors(weights, strategy=strategy)\n",
    "            bias_factors = init_factors(biases, strategy=strategy)\n",
    "\n",
    "            new_module = LinearsWithMasks(\n",
    "                linears=ref_children,\n",
    "                weight_modes=modes,\n",
    "                weight_values=weight_factors,\n",
    "                bias_modes=modes,\n",
    "                bias_values=bias_factors,\n",
    "            )\n",
    "\n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            factors = init_factors(weights, strategy=strategy)\n",
    "            new_module = EmbeddingsWithMasks(ref_children, modes, factors)\n",
    "\n",
    "        elif \"RMSNorm\" in type(target_child).__name__:\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            factors = init_factors(weights, strategy=strategy)\n",
    "            new_module = RMSNormsWithMasks(ref_children, modes, factors)\n",
    "\n",
    "        # Replace the original module with the new masked module\n",
    "        parent_module = target_module\n",
    "        for m_name in module_names[:-1]:\n",
    "            parent_module = getattr(parent_module, m_name)\n",
    "        setattr(parent_module, module_names[-1], new_module)\n",
    "\n",
    "\n",
    "def initialize_masks_recursive(target_module, ref_modules, strategy=\"naive\"):\n",
    "    \"\"\"\n",
    "    Recursively replaces normal components with masked components.\n",
    "    \n",
    "    Args:\n",
    "      module: The module in which to replace layers.\n",
    "    \"\"\"\n",
    "    num_components = len(ref_modules)\n",
    "    for name, target_child in target_module.named_children():\n",
    "        ref_children = [getattr(module, name) for module in ref_modules]\n",
    "        modes = [\"scalar\" for _ in ref_children]\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            biases = [ref.bias.data if ref.bias is not None else None \n",
    "                      for ref in ref_children]\n",
    "            \n",
    "            weight_factors = init_factors(weights, strategy=strategy)\n",
    "            bias_factors = init_factors(biases, strategy=strategy)\n",
    "            \n",
    "            new_module = LinearsWithMasks(\n",
    "                linears=ref_children,\n",
    "                weight_modes=modes,\n",
    "                weight_values=weight_factors,\n",
    "                bias_modes=modes,\n",
    "                bias_values=bias_factors,\n",
    "            )\n",
    "            setattr(target_module, name, new_module)\n",
    "            \n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            factors = init_factors(weights, strategy=strategy)\n",
    "            setattr(target_module, name, EmbeddingsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "            \n",
    "        elif \"RMSNorm\" in type(target_child).__name__:\n",
    "            weights = [ref.weight.data for ref in ref_children]\n",
    "            factors = init_factors(weights, strategy=strategy)\n",
    "            setattr(target_module, name, RMSNormsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "            \n",
    "        else:\n",
    "            initialize_masks(target_child, ref_children, strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3333bce5-fcf1-4167-b056-4b6c9eb6e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_memory(logger=None):\n",
    "    \"\"\"Frees GPU memory and logs memory usage before and after.\n",
    "\n",
    "    Args:\n",
    "        logger: An optional logging.Logger instance to use for logging.\n",
    "                If None, a default logger will be created.\n",
    "    \"\"\"\n",
    "\n",
    "    if logger is None:\n",
    "        # Create a default logger if one is not provided\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        logger.info(\"CUDA is not available. No GPU memory to free.\")\n",
    "        return\n",
    "\n",
    "    initial_memory = torch.cuda.memory_allocated()\n",
    "    logger.info(f\"Initial GPU memory allocated: {initial_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Empty PyTorch's cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    final_memory = torch.cuda.memory_allocated()\n",
    "    logger.info(f\"Final GPU memory allocated: {final_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "    freed_memory = initial_memory - final_memory\n",
    "    logger.info(f\"Freed GPU memory: {freed_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79dd85d4-c7e9-4d62-9813-5c590b63a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, model, tokenizer, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()\n",
    "\n",
    "def get_logits(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_ids).logits\n",
    "    return logits\n",
    "\n",
    "def get_hidden_states(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids, output_hidden_states=True, use_cache=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea09915-e71d-4178-8ef5-c098b533df91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## debugging utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdce16a2-1565-4d6e-8506-8972d34271f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_input(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    past_key_values = None\n",
    "    cache_position = None\n",
    "    position_ids = None\n",
    "    output_hidden_states = True\n",
    "    output_attentions = False\n",
    "    use_cache = False\n",
    "    return_dict = True\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        return_legacy_cache = False\n",
    "        inputs_embeds = model.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = model._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "    return dict(\n",
    "        hidden_states=hidden_states,\n",
    "        attention_mask=causal_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=past_key_values,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "        cache_position=cache_position,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e422ef-d96f-4ee7-8088-1a9711ade4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(mlp, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    ref: self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n",
    "    \"\"\"\n",
    "    steps = {}\n",
    "    steps.update({\"step 0 (input)\": x})\n",
    "    \n",
    "    gate = mlp.gate_proj(x)\n",
    "    steps.update({\"step 1 (gate)\": gate})\n",
    "    \n",
    "    up = mlp.up_proj(x)\n",
    "    steps.update({\"step 2 (up)\": up})\n",
    "    \n",
    "    act = mlp.act_fn(gate) # The activation function should be applied to the gate projection\n",
    "    steps.update({\"step 3 (activation)\": act})\n",
    "    \n",
    "    act_up = act * up  # Multiply the activated gate with the up projection\n",
    "    steps.update({\"step 4 (act_up)\": act_up})\n",
    "\n",
    "    down = mlp.down_proj(act_up) # Apply the down projection to the result of act * up\n",
    "    steps.update({\"step 5 (down - output)\": down})\n",
    "    \n",
    "    return dict(\n",
    "        outputs=down,\n",
    "        debugging=steps\n",
    "    )\n",
    "\n",
    "# def mlp_forward(self, hidden_state):\n",
    "#     return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "224dad97-c568-422e-b8e4-8b1f6d9726d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward(\n",
    "    decoder,\n",
    "    hidden_states: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "    output_attentions: Optional[bool] = False,\n",
    "    use_cache: Optional[bool] = False,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    "    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "\n",
    "    steps = {}\n",
    "    # logger.warning(f\"-------- Logging hidden_states in decoder forward:\")\n",
    "    residual = hidden_states\n",
    "    # logger.warning(f\" hidden_states step 1 (as input): {hidden_states}\")\n",
    "    steps.update({\"step 1\": hidden_states})\n",
    "\n",
    "    hidden_states = decoder.input_layernorm(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 2 (after input_layernorm): {hidden_states}\")\n",
    "    steps.update({\"step 2\": hidden_states})\n",
    "    # Self Attention\n",
    "    hidden_states, self_attn_weights, present_key_value = decoder.self_attn(\n",
    "        hidden_states=hidden_states,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_value=past_key_value,\n",
    "        output_attentions=output_attentions,\n",
    "        use_cache=use_cache,\n",
    "        cache_position=cache_position,\n",
    "        position_embeddings=position_embeddings,\n",
    "    )\n",
    "    # logger.warning(f\" hidden_states step 3 (after self_attn): {hidden_states}\")\n",
    "    steps.update({\"step 3\": hidden_states})\n",
    "    \n",
    "    hidden_states = residual + hidden_states\n",
    "    # logger.warning(f\" hidden_states step 4 (after first skip connection): {hidden_states}\")\n",
    "    steps.update({\"step 4\": hidden_states})\n",
    "    # Fully Connected\n",
    "    residual = hidden_states\n",
    "    hidden_states = decoder.post_attention_layernorm(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 5 (after post_attention_layernorm): {hidden_states}\")\n",
    "    steps.update({\"step 5\": hidden_states})\n",
    "    \n",
    "    hidden_states = decoder.mlp(hidden_states)\n",
    "    # logger.warning(f\" hidden_states step 6 (after mlp): {hidden_states}\")\n",
    "    steps.update({\"step 6\": hidden_states})\n",
    "    \n",
    "    hidden_states = residual + hidden_states\n",
    "    # logger.warning(f\" hidden_states step 7 (after second skip connection): {hidden_states}\")\n",
    "    steps.update({\"step 7\": hidden_states})\n",
    "\n",
    "    outputs = (hidden_states,)\n",
    "\n",
    "    if output_attentions:\n",
    "        outputs += (self_attn_weights,)\n",
    "\n",
    "    if use_cache:\n",
    "        outputs += (present_key_value,)\n",
    "\n",
    "    return dict(\n",
    "        outputs=outputs,\n",
    "        debugging=steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce23ae57-f8af-4b92-a595-9a3f9544b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    past_key_values = None\n",
    "    cache_position = None\n",
    "    position_ids = None\n",
    "    output_hidden_states = True\n",
    "    output_attentions = False\n",
    "    use_cache = False\n",
    "    return_dict = True\n",
    "    #############\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # kept for BC (non `Cache` `past_key_values` inputs)\n",
    "        return_legacy_cache = False\n",
    "        inputs_embeds = model.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = model._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = model.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "        all_decoder_steps = {}\n",
    "\n",
    "        for i, decoder_layer in enumerate(model.layers[:2]):   \n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "          \n",
    "            layer_outputs = decoder_forward(\n",
    "                decoder_layer,\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            steps = layer_outputs[-1]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "            all_decoder_steps.update({f\"layer {i}\": steps})\n",
    "\n",
    "        hidden_states = model.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if return_legacy_cache:\n",
    "            next_cache = next_cache.to_legacy_cache()\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        outputs = BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=(),\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=()\n",
    "        )\n",
    "        return dict(\n",
    "            outputs=outputs,\n",
    "            debugging=steps\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5a8d8-eef0-4dfd-b26d-8a27d0aed4e5",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a9067c-cd0f-4207-b94b-ec6926fc948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = \"\"\"\n",
    "- add vector mask\n",
    "- add slerp mask\n",
    "- add mask manager (imposing some constraints on mask.)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e64de2db-183b-4a12-96a8-60741e15c1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(4, 3).weight.shape.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b5bbb8-1a19-4aa7-9d3b-8e6dfc628d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.weight = nn.Parameter(torch.tensor(value))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        if self.size is not None:\n",
    "            try:\n",
    "                self.weight * torch.rand(self.size)\n",
    "            except RuntimeError:\n",
    "                raise ValueError(\"Mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Be really careful here (though I do not think it matters that much),\n",
    "        When testing, it's important that the masking operation is implemented\n",
    "        with `x = self.weight * x` instead of `x = x * self.weight`.\n",
    "\n",
    "        Neither of those two implementation is superior, however I need to be\n",
    "        consistent when doing testing because the phenonmenon above could lead\n",
    "        to some number imprecision, which may fail `torch.testing.assert_close`\n",
    "        \"\"\"\n",
    "        if self.size is None:\n",
    "            return self.weight * x\n",
    "        else:\n",
    "            if self.size != x.shape:\n",
    "                print(\"The shape of input does not match that of the mask.\")\n",
    "            return self.weight * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce5f133-2e26-4855-89c4-10853bd7de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithMask(nn.Module):\n",
    "    def __init__(self, linear, weight_mask_config: MaskConfig, bias_mask_config: MaskConfig = None):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.weight_mask_config = weight_mask_config\n",
    "        self.bias_mask_config = bias_mask_config\n",
    "\n",
    "        if linear.weight.shape != weight_mask_config.size:\n",
    "            print(\"Weight mask shape is not compatible with linear, reinitializing...\")\n",
    "            self.weight_mask_config.size = linear.weight.shape\n",
    "        self.weight_mask = Mask(self.weight_mask_config)\n",
    "\n",
    "        if linear.bias is not None and bias_mask_config is not None:\n",
    "            if linear.bias.shape != bias_mask_config.size:\n",
    "                print(\"Bias mask shape is not compatible with linear, reinitializing...\")\n",
    "                self.bias_mask_config.size = linear.bias.shape\n",
    "            self.bias_mask = Mask(self.bias_mask_config)\n",
    "        else:\n",
    "            self.bias_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        masked_weight = self.weight_mask(self.linear.weight)\n",
    "        if self.linear.bias is not None and self.bias_mask is not None:\n",
    "            masked_bias = self.bias_mask(self.linear.bias)\n",
    "        else:\n",
    "            masked_bias = self.linear.bias\n",
    "        return nn.functional.linear(x, masked_weight, masked_bias)\n",
    "\n",
    "class LinearsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        linears: List[nn.Module],\n",
    "        weight_modes: List[str] = [\"scalar\"],\n",
    "        weight_values: List[float] = None,\n",
    "        bias_modes: List[str] = [\"scalar\"],\n",
    "        bias_values: List[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not all(isinstance(linear, nn.Linear) for linear in linears):\n",
    "            raise ValueError(\"All elements in 'linears' must be instances of nn.Linear.\")\n",
    "\n",
    "        weight_sizes = [linear.weight.shape for linear in linears]\n",
    "        bias_sizes = [linear.bias.shape if linear.bias is not None else None for linear in linears]\n",
    "        \n",
    "        if weight_values is None or len(weight_values) != len(linears):\n",
    "            raise ValueError(f\"weight_values for masks: {weight_values} do not match with linear layers: {linears}\")\n",
    "        if bias_values is None:\n",
    "            bias_values = [None] * len(linears)\n",
    "        if len(bias_values) != len(linears):\n",
    "            raise ValueError(f\"bias_values for masks: {bias_values} do not match with linear layers: {linears}\")\n",
    "\n",
    "        weight_mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(weight_modes, weight_values, weight_sizes)\n",
    "        ]\n",
    "        bias_mask_configs = [\n",
    "            MaskConfig(mode, value, size) if size is not None else None\n",
    "            for mode, value, size in zip(bias_modes, bias_values, bias_sizes)\n",
    "        ]\n",
    "\n",
    "        self.masked_linears = nn.ModuleList(\n",
    "            [LinearWithMask(linear, weight_mask_config, bias_mask_config)\n",
    "             for linear, weight_mask_config, bias_mask_config \n",
    "             in zip(linears, weight_mask_configs, bias_mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = [linear.weight_mask(linear.linear.weight) \n",
    "                   for linear in self.masked_linears]\n",
    "        # merged_weight = torch.sum(torch.stack(weights), dim=0)\n",
    "        merged_weight = sum(weights)\n",
    "\n",
    "        biases = [\n",
    "            linear.bias_mask(linear.linear.bias) \n",
    "            if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "            else linear.linear.bias for linear in self.masked_linears\n",
    "        ]\n",
    "        \n",
    "        if all(b is None for b in biases):\n",
    "            merged_bias = None\n",
    "        else:\n",
    "            biases = [\n",
    "                b if b is not None\n",
    "                else torch.zeros_like(weights[0][:, 0])\n",
    "                for b in biases\n",
    "            ]\n",
    "            merged_bias = sum(biases)\n",
    "\n",
    "        return nn.functional.linear(x, merged_weight, merged_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8eb68e3-ff6a-46c7-be08-f95e9705a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormWithMask(nn.Module):\n",
    "    def __init__(self, rms_norm: Qwen2RMSNorm, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.rms_norm = rms_norm\n",
    "        self.mask_config = mask_config\n",
    "        if rms_norm.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with RMSNorm, reinitializing...\")\n",
    "        self.mask_config.size = rms_norm.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        masked_weight = self.mask(self.rms_norm.weight)\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.rms_norm.variance_epsilon)\n",
    "        return masked_weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RMSNormsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rms_norms: List[Qwen2RMSNorm],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [rms_norm.weight.shape for rms_norm in rms_norms]\n",
    "        if values is None or len(values) != len(rms_norms):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with RMSNorm layers: {rms_norms}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_rms_norms = nn.ModuleList(\n",
    "            [RMSNormWithMask(rms_norm, mask_config)\n",
    "             for rms_norm, mask_config in zip(rms_norms, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        weights = [rms.mask(rms.rms_norm.weight) for rms in self.masked_rms_norms]\n",
    "        merged_weight = sum(weights)\n",
    "        variance_epsilon = self.masked_rms_norms[0].rms_norm.variance_epsilon\n",
    "        for rms in self.masked_rms_norms:\n",
    "            assert variance_epsilon == rms.rms_norm.variance_epsilon, \"Variance epsilon among models must be consistent\"\n",
    "        \n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "        return merged_weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "240c3b14-7d05-4a09-84c6-12b411f3ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithMask(nn.Module):\n",
    "    def __init__(self, embedding: nn.Embedding, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.mask_config = mask_config\n",
    "        if embedding.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with Embedding, reinitializing...\")\n",
    "        self.mask_config.size = embedding.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        masked_weight = self.mask(self.embedding.weight)\n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            masked_weight,\n",
    "            padding_idx=self.embedding.padding_idx,\n",
    "            max_norm=self.embedding.max_norm,\n",
    "            norm_type=self.embedding.norm_type,\n",
    "            scale_grad_by_freq=self.embedding.scale_grad_by_freq,\n",
    "            sparse=self.embedding.sparse,\n",
    "        )\n",
    "\n",
    "class EmbeddingsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [embedding.weight.shape for embedding in embeddings]\n",
    "        if values is None or len(values) != len(embeddings):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with Embedding layers: {embeddings}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_embeddings = nn.ModuleList(\n",
    "            [EmbeddingWithMask(embedding, mask_config)\n",
    "             for embedding, mask_config in zip(embeddings, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        weights = [emb.mask(emb.embedding.weight) for emb in self.masked_embeddings]\n",
    "        merged_weight = sum(weights)\n",
    "        an_embedding = self.masked_embeddings[0].embedding\n",
    "        for other in self.masked_embeddings:\n",
    "            other_embedding = other.embedding\n",
    "            assert an_embedding.padding_idx == other_embedding.padding_idx\n",
    "            assert an_embedding.max_norm == other_embedding.max_norm\n",
    "            assert an_embedding.norm_type == other_embedding.norm_type\n",
    "            assert an_embedding.scale_grad_by_freq == other_embedding.scale_grad_by_freq\n",
    "            assert an_embedding.sparse == other_embedding.sparse\n",
    "            \n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            merged_weight,\n",
    "            padding_idx=an_embedding.padding_idx,\n",
    "            max_norm=an_embedding.max_norm,\n",
    "            norm_type=an_embedding.norm_type,\n",
    "            scale_grad_by_freq=an_embedding.scale_grad_by_freq,\n",
    "            sparse=an_embedding.sparse,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c979f89-ab78-440f-80c8-157b83014548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.num_models = len(merge_config.model_paths)\n",
    "        self.configs = [\n",
    "            AutoConfig.from_pretrained(path) \n",
    "            # Qwen2Config.from_pretrained(path)\n",
    "            for path in merge_config.model_paths\n",
    "        ]\n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.models = nn.ModuleList([\n",
    "            # Qwen2ForCausalLM.from_pretrained(\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                merge_config.model_paths[i], \n",
    "                config=self.configs[i],\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ) \n",
    "            for i in range(self.num_models)\n",
    "        ])\n",
    "        # self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config, strategy=\"naive\"):\n",
    "\n",
    "        for model in self.models:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.merger = copy.deepcopy(self.models[0])\n",
    "        initialize_masks(self.merger, self.models, strategy=strategy)\n",
    "        free_memory()\n",
    "        \n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ec8a20-5fcb-4be1-acde-957c239171cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mask_parameter_names(module, mask_param_names_list, parent_name=\"\"):\n",
    "    \"\"\"\n",
    "    Recursively finds full names of parameters that belong to modules of class \"Mask\".\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        full_child_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        if child.__class__.__name__ == \"Mask\":\n",
    "            for param_name, _ in child.named_parameters():\n",
    "                full_param_name = f\"{full_child_name}.{param_name}\"\n",
    "                mask_param_names_list.append(full_param_name)\n",
    "        find_mask_parameter_names(child, mask_param_names_list, full_child_name)\n",
    "        \n",
    "def set_masks(merger, strategy=\"slerp\"):\n",
    "    if strategy.startswith(\"select_\"):\n",
    "        selected_idx = int(strategy[len(\"select_\"):])\n",
    "        assert selected_idx < len(merger.models)\n",
    "        mask_param_names = []\n",
    "        find_mask_parameter_names(merger, mask_param_names)\n",
    "        \n",
    "        for param_name in tqdm(\n",
    "            mask_param_names,\n",
    "            desc=f\"Making self.merger identical to self.models[{selected_idx}]\"\n",
    "        ):\n",
    "            # Get the parameter object using its full name\n",
    "            module_names = param_name.split(\".\")\n",
    "            current_module = merger\n",
    "            for m_name in module_names[:-1]:\n",
    "                current_module = getattr(current_module, m_name)\n",
    "            param = getattr(current_module, module_names[-1])\n",
    "    \n",
    "            # Modify the parameter\n",
    "            with torch.no_grad():\n",
    "                idx = int(param_name.split(\".\")[-3])\n",
    "                value = 1.0 if idx == selected_idx else 0.0\n",
    "                param.data.fill_(value)\n",
    "    elif strategy == \"slerp\":\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2bbe46e-0b19-48b8-8b3e-e489adbdf0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caa47700-3b99-4630-8a59-0d437799977a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef09f19a37c149529101b2399e5d386b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b397c57baf4525a18efe36bf546696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99134fb8-068b-4840-b4c6-794b94b859e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merger = merger.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4b98853-8d28-4c26-9d8c-7ab9f5fbe2ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 327/327 [01:03<00:00,  5.13it/s]\n",
      "2024-12-16 10:30:20,016 - INFO - Initial GPU memory allocated: 11.71 GB\n",
      "2024-12-16 10:30:20,489 - INFO - Final GPU memory allocated: 11.71 GB\n",
      "2024-12-16 10:30:20,490 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "merger.__post_init__(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "280cf940-4a4f-4eef-8d64-b51acca115d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making self.merger identical to self.models[0]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 870/870 [00:00<00:00, 33242.64it/s]\n"
     ]
    }
   ],
   "source": [
    "set_masks(merger, strategy=\"select_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "824c5abf-7ca1-455c-9d7e-d4030358128d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42a429e8-8d3f-4cb5-b2a0-2c2481264ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.merger.model.layers[0].mlp.gate_proj.masked_linears[0].weight_mask.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bfc6b5f-69c2-4aa8-aaf9-48b10a520737",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5264369e-20c8-4c60-a7ca-356bcf1b4650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nBạn là một trợ lý hữu ích.<|im_end|>\\n<|im_start|>user\\nHoàng Sa là của nước nào?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"Bạn là một trợ lý hữu ích.\"\n",
    "prompt = \"Hoàng Sa là của nước nào?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e82de2e3-d5f8-4d42-81b2-b916e8aeb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoàng Sa và Trường Sa đều nằm trong quần thể đảo ngọc Trường Sa, thuộc chủ quyền của Việt Nam. Đây là vùng biển quan trọng với nhiều tài nguyên thiên nhiên quý giá và có ý nghĩa chiến lược địa chính trị đối với Việt Nam.<|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.merger, tokenizer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a743fbc-cd0a-4985-9fdb-d2a0ae8cb345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoàng Sa và Trường Sa đều nằm trong quần thể đảo ngọc Trường Sa, thuộc chủ quyền của Việt Nam. Đây là vùng biển quan trọng với nhiều tài nguyên thiên nhiên quý giá và có ý nghĩa chiến lược địa chính trị đối với Việt Nam.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.models[0], tokenizer, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09286c42-4e7d-4200-9673-9867c8691bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_merged = get_logits(text, merger.merger, tokenizer)\n",
    "logits_0 = get_logits(text, merger.models[0], tokenizer)\n",
    "torch.allclose(logits_merged, logits_0, atol=0, rtol=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
