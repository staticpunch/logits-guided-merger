{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "566a4231-91b2-41a4-a1ac-d87d63078a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union, Mapping\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    default_data_collator,\n",
    "    is_torch_xla_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast\n",
    ")\n",
    "\n",
    "from efficient_masks import (\n",
    "    MergerConfig,\n",
    "    Merger,\n",
    "    init_masks,\n",
    "    set_masks\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    generate, \n",
    "    get_hidden_states, \n",
    "    get_logits,\n",
    "    free_memory\n",
    ")\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da64abe-4bc3-45ad-a13d-e2b01ab32523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "hidden_dim = 100\n",
    "\n",
    "X = torch.rand(batch_size, seq_len, hidden_dim)\n",
    "mask = torch.rand(batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df95ae15-6abb-4523-a0aa-fb51e517ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selective_mask = torch.tensor([1.0, 0.0, 0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b599891e-6965-4b95-aad8-32bd5c556da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_logits = []\n",
    "for i in range(batch_size):\n",
    "    selected = X[i, ...]\n",
    "    selected_logits.append(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "056d6b7f-3818-4498-9b46-89fbe71734b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(selected_logits, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "369a3282-9cac-419f-9b3e-5fdb5aca1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.rand(batch_size, seq_len) * torch.tensor([[1.0], [0.0]])\n",
    "mask[mask != 0] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0e9a2f-d54c-4edb-95c8-b5576decbf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1.0], [0.0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30eae2af-61a9-43e3-93c1-7ff24ebcf373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(batch_size, seq_len).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7a695f-0215-47a4-9da7-3150133eefbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite_train = load_dataset(\"HuggingFaceTB/smoltalk\", \"smol-rewrite\", split=\"train\")\n",
    "# summarize_train = load_dataset(\"HuggingFaceTB/smoltalk\", \"smol-summarize\", split=\"train\")\n",
    "# rewrite_train = rewrite_train.add_column(name=\"data_source\", column=[\"A\" for _ in rewrite_train])\n",
    "# summarize_train = summarize_train.add_column(name=\"data_source\", column=[\"B\" for _ in summarize_train])\n",
    "\n",
    "# train_dataset = datasets.concatenate_datasets([rewrite_train, summarize_train])\n",
    "# train_mini = train_dataset.shuffle().select(range(5000))\n",
    "# train_mini.to_json(\"train_mini.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b02ec5a-3c6a-41c0-9a44-1aa40d640a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mini = load_dataset(\"json\", data_files=[\"train_mini.jsonl\"], split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8fe09e-0549-4dd1-bd9e-7f770c2e43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger_forward(texts, model, tokenizer, data_collator):\n",
    "    tokenized = [tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        add_special_tokens=False,\n",
    "    ) for text in texts]\n",
    "    inputs = data_collator(tokenized).to(model.device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6209d994-0d0a-4b3e-8386-053509d7bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n",
    "    \"\"\"\n",
    "    Pads without triggering the warning about how using the pad function is sub-optimal when using a fast tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid errors when using Feature extractors\n",
    "    if not hasattr(tokenizer, \"deprecation_warnings\"):\n",
    "        return tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "\n",
    "    # Save the state of the warning, then disable it\n",
    "    warning_state = tokenizer.deprecation_warnings.get(\"Asking-to-pad-a-fast-tokenizer\", False)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    try:\n",
    "        padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "    finally:\n",
    "        # Restore the state of the warning.\n",
    "        tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n",
    "\n",
    "    return padded\n",
    "\n",
    "@dataclass\n",
    "class MergerDataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        \"\"\"\n",
    "        copied from DataCollatorForLanguageModeling\n",
    "        examples: List[Union[List[int], Any, Dict[str, Any]]]\n",
    "        \"\"\"\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if not isinstance(examples[0], Mapping):\n",
    "            raise ValueError(\"Data collator only processes list of dictionaries.\")\n",
    "\n",
    "        inputs_ids = []\n",
    "        for i in range(len(examples)):\n",
    "            _ = examples[i].pop(\"attention_mask\")\n",
    "            inputs_ids.append(\n",
    "                {\"input_ids\": examples[i].pop(\"input_ids\")}\n",
    "            )\n",
    "            \n",
    "        batch = pad_without_fast_tokenizer_warning(\n",
    "            self.tokenizer, inputs_ids, return_tensors=\"pt\", \n",
    "            pad_to_multiple_of=self.pad_to_multiple_of\n",
    "        )\n",
    "\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        if self.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        \n",
    "        for key in examples[0]:\n",
    "            if key in batch:\n",
    "                raise ValueError(f\"`{key}` feature is collated. Overriding it with its initial values is prohibitted.\")\n",
    "            else:\n",
    "                batch[key] = [x[key] for x in examples]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdaead7-8100-47f9-8ecb-25b011f5b593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"constrain_mode\": \"01\",\n",
       "  \"mode\": \"vector_input\",\n",
       "  \"model_paths\": [\n",
       "    \"nguyenthanhdo/llama32_smol_rewrite_50k\",\n",
       "    \"nguyenthanhdo/llama32_smol_summarize_50k\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"nguyenthanhdo/llama32_smol_rewrite_50k\",\n",
    "        \"nguyenthanhdo/llama32_smol_summarize_50k\",\n",
    "        # \"/workspace/HUB_LLM/Llama-3.2-3B-Instruct\",\n",
    "    ],\n",
    "    mode = \"vector_input\",\n",
    "    # mode = \"scalar\",\n",
    "    constrain_mode = \"01\"\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b840ebfd-3297-4e44-b6b6-7b0f2efac472",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = MergerDataCollator(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669f4f9c-289a-479f-ac08-ebc00553877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    templated = tokenizer.apply_chat_template(\n",
    "        element[\"messages\"], tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        templated,\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        add_special_tokens=False,\n",
    "        # return_tensors=\"pt\"\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7addc5-dffc-43e0-a232-b30732d28ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_mini = train_mini.map(tokenize, remove_columns=[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae261bfa-7067-4a57-ba63-c13ca96f9147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['data_source', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2b3cbf-6aa1-4d6f-bf57-1e947d003f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b5b8d1cee24d339e1c7ce918bb33f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60daae3673148cf8c3900ded880d572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing masks:   1%|█▎                                                                                                                                                                             | 2/255 [00:12<27:38,  6.56s/it]2024-12-30 10:04:46,129 - WARNING - Though you want to make a masks of modes ['vector_input', 'vector_input'] for RMSNorms' weights, by default a mask only accepts a scalar mask. Converting modes to `scalar`.\n",
      "Initializing masks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:50<00:00,  5.04it/s]\n",
      "2024-12-30 10:05:23,898 - INFO - Initial GPU memory allocated: 0.00 GB\n",
      "2024-12-30 10:05:24,168 - INFO - Final GPU memory allocated: 0.00 GB\n",
      "2024-12-30 10:05:24,168 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "merger = Merger(merge_config)\n",
    "merger.__post_init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89659e8-4aaf-4f75-96e5-566102310733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e8b6a3d-906e-48aa-ba7d-1f1ad8946a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger = merger.to(device=\"cuda:0\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d19b15-1448-4531-8bee-3364419f2c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up masks: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 255/255 [00:00<00:00, 12809.87it/s]\n"
     ]
    }
   ],
   "source": [
    "set_masks(merger.merger, strategy=\"uniform\", factors=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bee11da0-030d-400c-a3f9-d84559bb71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "def compute_clm_loss(model, inputs, return_outputs=False):\n",
    "    \"\"\"\n",
    "    Custom compute_loss function for Causal Language Modeling.\n",
    "\n",
    "    Args:\n",
    "        model: The model to compute the loss for.\n",
    "        inputs: A dictionary of inputs as produced by the `collate_fn`.\n",
    "        return_outputs: Whether or not to return the model outputs in addition to the loss.\n",
    "\n",
    "    Returns:\n",
    "        The loss and the model outputs (if `return_outputs=True`).\n",
    "    \"\"\"\n",
    "\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Shift logits and labels for next token prediction\n",
    "    # We shift the logits to the left and the labels to the right to align them for loss computation\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    # Flatten the tokens\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c65c430c-2a44-497b-97d3-be0e4c0b27d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7f365f0-8f51-48c6-b88d-2d7cdd600c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_loss(logits_merged, logits_target):\n",
    "\n",
    "    probs_merged = F.softmax(logits_merged, -1)\n",
    "\n",
    "    d = probs_merged.log() - F.log_softmax(logits_target, -1)\n",
    "\n",
    "    ## TODO: need to take sum only on not -100 tokens.\n",
    "    return (probs_merged * d).sum(-1).mean()\n",
    "    # return probs_merged * d\n",
    "\n",
    "def kl_torch(s_logits, t_logits, idx_masks=None, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Computes KL(s || t) with temperature scaling.\n",
    "    \n",
    "    Args:\n",
    "        s_logits, t_logits: (batch_size, seq_len, vocab)\n",
    "        temperature: float\n",
    "        indices: effective indices that exclude pad tokens.\n",
    "\n",
    "    Returns:\n",
    "        KL divergence that \n",
    "        sum over vocab, then divided by batch_size * seq_len.\n",
    "        Also do not take into account pad tokens.\n",
    "    \"\"\"\n",
    "    kl_fct = nn.KLDivLoss(reduction=\"none\")\n",
    "    diff = (\n",
    "        kl_fct(\n",
    "            F.log_softmax(t_logits / temperature, dim=-1),\n",
    "            F.softmax(s_logits / temperature, dim=-1)\n",
    "        )\n",
    "        * (temperature) ** 2\n",
    "    )\n",
    "    loss = diff.sum(-1).mean()\n",
    "    # loss = diff.sum(-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e954d697-75f9-4ed9-a0f2-02d71b32dd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou're an AI assistant for text re-writing. Rewrite the input text to make it more friendly and approachable while maintaining its main points.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMr. Thompson,\\n\\nI have been waiting for the finalized schedule for the community outreach program, which was supposed to be ready by last week. I understand that you have a lot on your plate, but your lack of communication is making it very difficult for me to plan effectively. I need this information as soon as possible to ensure we meet our grant requirements.\\n\\nBest regards,\\nEmily Carter<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHi Mr. Thompson,\\n\\nI hope you're doing well! I wanted to check in about the finalized schedule for the community outreach program. I know things can get really busy, and I appreciate all the work you're doing. However, I'm finding it a bit challenging to plan without the schedule, which we were expecting last week. It's really important for us to have this information soon to meet our grant requirements.\\n\\nThanks so much for your help, and I look forward to hearing from you!\\n\\nBest,\\nEmily Carter<|eot_id|>\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [tokenizer.apply_chat_template(\n",
    "    train_mini[i][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ") for i in range(8)]\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ec04013-fb50-488b-bd80-a198d88c03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "collated = data_collator([tokenized_mini[i] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5f639a9-cf17-4340-b6e2-65885b54914d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collated = collated.to(merger.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "145f2d4d-a068-4f3e-8408-455297e92476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "outputs_1 = merger.models[0](**collated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1c537c7-49ea-489c-be24-5a7be8b0714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "outputs_2 = merger.models[1](**collated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7276875-7c69-40b9-9cfb-123c04c443fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "merger_outputs = merger.merger(**collated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49610399-4f03-4162-848f-e90eaa552cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     outputs = merger_forward(texts, merger, tokenizer, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9669485d-857f-45e7-846d-2b8bf6c2f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 10:05:28,317 - INFO - Initial GPU memory allocated: 14.31 GB\n",
      "2024-12-30 10:05:28,671 - INFO - Final GPU memory allocated: 14.31 GB\n",
      "2024-12-30 10:05:28,672 - INFO - Freed GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    logger.info(\"CUDA is not available. No GPU memory to free.\")\n",
    "    \n",
    "initial_memory = torch.cuda.memory_allocated()\n",
    "logger.info(f\"Initial GPU memory allocated: {initial_memory / 1024**3:.2f} GB\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "final_memory = torch.cuda.memory_allocated()\n",
    "logger.info(f\"Final GPU memory allocated: {final_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "freed_memory = initial_memory - final_memory\n",
    "logger.info(f\"Freed GPU memory: {freed_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de7f2f5c-745b-401e-be1f-a582f2359408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_kl_out = kl_torch(outputs_1.logits, outputs_2.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7b1aa8d-f2f3-4118-a5a8-1262900fd5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_idxs = collated['labels'].clone()\n",
    "effective_idxs[effective_idxs != -100] = 1.0\n",
    "effective_idxs[effective_idxs == -100] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c315c724-a35a-4618-8cd2-4dd096586652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3594, 1.7500], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_kl_out * effective_idxs).sum(dim=-1) / effective_idxs.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6e8ac31-dab8-4686-9389-67a562ee0cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([528., 362.], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_kl_out * effective_idxs).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e97642d1-e44f-49e6-a6ed-b03d71126657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([224, 207], device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effective_idxs.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6045f05-1d1d-47b0-88fd-3268046573e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=tensor(2.9416, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ 5.2812,  7.5625, 13.3125,  ..., -6.1875, -6.1875, -6.1875],\n",
       "         [ 4.5625, -0.0386,  0.2432,  ...,  1.7500,  1.7500,  1.7500],\n",
       "         [ 3.8125,  3.1719,  2.7500,  ..., -0.8438, -0.8438, -0.8438],\n",
       "         ...,\n",
       "         [ 6.0000,  5.3438,  3.9375,  ..., -8.1250, -8.1250, -8.1250],\n",
       "         [ 2.3906,  3.9062,  2.3281,  ..., -1.8438, -1.8438, -1.8438],\n",
       "         [ 0.4688, -1.8281, -0.2285,  ...,  3.2812,  3.2812,  3.2812]],\n",
       "\n",
       "        [[ 5.2812,  7.5625, 13.3125,  ..., -6.1875, -6.1875, -6.1875],\n",
       "         [ 4.5625, -0.0386,  0.2432,  ...,  1.7500,  1.7500,  1.7500],\n",
       "         [ 3.8125,  3.1719,  2.7500,  ..., -0.8438, -0.8438, -0.8438],\n",
       "         ...,\n",
       "         [ 1.4062,  2.7188,  3.5000,  ..., -1.7188, -1.7188, -1.7188],\n",
       "         [ 1.4453,  2.7969,  3.4219,  ..., -1.7344, -1.7344, -1.7344],\n",
       "         [ 1.4219,  2.7969,  3.3438,  ..., -1.7656, -1.7656, -1.7656]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), past_key_values=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "738af44f-3a77-42c1-b7aa-698d9500f4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated[\"attention_mask\"][0] == collated[\"attention_mask\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c742dfc-492b-4d8f-8f6f-a026d22dc82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'data_source'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d628e4fa-248b-4fc3-80cd-2c64d94b2b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1336, 128256])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"merger_outputs\"].logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abd9615c-43b9-44a2-9122-c9c9cfa8d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger_logits = outputs[\"merger_outputs\"].logits\n",
    "components_logits = [x.logits for x in outputs[\"component_outputs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "313da39f-131b-4829-805f-39e4ae973638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 320, 128256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "87e53945-2ccb-4b94-8c83-98528b9458c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.2190e-10, device='cuda:4')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_torch(components_logits[1].float(), components_logits[1].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "28b8158b-c6b1-4245-be0e-45594c316edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.3787e-10, device='cuda:4')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_kl_loss(components_logits[1].float(), components_logits[1].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e01ccbe-9891-4cd4-9497-33aec8290bf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_kl_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompute_kl_loss\u001b[49m(\u001b[38;5;241m*\u001b[39mcomponents_logits) \u001b[38;5;241m==\u001b[39m kl_torch(\u001b[38;5;241m*\u001b[39mcomponents_logits)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_kl_loss' is not defined"
     ]
    }
   ],
   "source": [
    "compute_kl_loss(*components_logits) == kl_torch(*components_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e39e7ad4-7811-4aee-8260-898dc9bedcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8359, device='cuda:4', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4a86d-b536-417c-98ad-713a2d076ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Trainer with Custom compute_loss ---\n",
    "def get_entropy_weights(logits_a, logits_b, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Calculates entropy-based weights for merging two sets of logits.\n",
    "\n",
    "    This function efficiently computes the weights for logits_a and logits_b\n",
    "    based on their respective entropies. It combines the functionality of\n",
    "    calculating entropy, normalizing weights, and handling potential\n",
    "    division-by-zero issues.\n",
    "\n",
    "    Args:\n",
    "        logits_a: A PyTorch tensor representing the first set of logits.\n",
    "        logits_b: A PyTorch tensor representing the second set of logits.\n",
    "        epsilon: A small value to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two tensors: (weight_a, weight_b), representing the\n",
    "        normalized entropy-based weights for logits_a and logits_b, respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probs_a = F.softmax(logits_a, dim=-1)\n",
    "    probs_b = F.softmax(logits_b, dim=-1)\n",
    "\n",
    "    # Calculate entropies with epsilon for numerical stability\n",
    "    entropy_a = -(probs_a * probs_a.log()).sum(dim=-1, keepdim=True)\n",
    "    entropy_b = -(probs_b * probs_b.log()).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # Calculate inverse entropies (weights)\n",
    "    inv_entropy_a = 1.0 / (entropy_a + epsilon)\n",
    "    inv_entropy_b = 1.0 / (entropy_b + epsilon)\n",
    "\n",
    "    # Normalize weights\n",
    "    total_inv_entropy = inv_entropy_a + inv_entropy_b\n",
    "    weight_a = inv_entropy_a / (total_inv_entropy + epsilon)  \n",
    "    weight_b = inv_entropy_b / (total_inv_entropy + epsilon) \n",
    "\n",
    "    return weight_a, weight_b\n",
    "    \n",
    "def compute_logits_target(logits_components):\n",
    "    assert len(logits_components) == 2\n",
    "    logits_a, logits_b = logits_components\n",
    "    weight_a, weight_b = get_entropy_weights(logits_a, logits_b)\n",
    "\n",
    "    logits_target = weight_a * logits_a + weight_b * logits_b\n",
    "\n",
    "    return logits_target\n",
    "    \n",
    "class Mergerrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        ## Read inputs. Compute a simple mask `effective_idxs`\n",
    "        ## to exclude non-trainable tokens (e.g., PAD tokens).\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        effective_idxs = labels.clone()\n",
    "        effective_idxs[effective_idxs != -100] = 1.0\n",
    "        effective_idxs[effective_idxs == -100] = 0.0\n",
    "\n",
    "        ## Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits_merged = outputs[\"merger_outputs\"].logits\n",
    "        logits_components = [x.logits for x in outputs[\"components_outputs\"]]\n",
    "\n",
    "        ## Compute targt logits\n",
    "        logits_target = compute_logits_target(logits_components)\n",
    "\n",
    "        ## Compute KL divergence\n",
    "        temperature = 1.0\n",
    "        kl_fct = nn.KLDivLoss(reduction=\"none\")\n",
    "        diff = (\n",
    "            kl_fct(\n",
    "                F.log_softmax(logits_target / temperature, dim=-1),\n",
    "                F.softmax(logits_merged / temperature, dim=-1)\n",
    "            )\n",
    "            * (temperature) ** 2\n",
    "        )\n",
    "        \n",
    "        ### Exclude non-trainable tokens from taking loss\n",
    "        loss = (diff * effective_idxs).sum(dim=-1)\n",
    "        loss = (loss / effective_idxs.sum(dim=-1)).mean()\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329d3c1-5296-4224-bb94-a8dcbdcb0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    model_name: str = \"...\"  # You can replace this with any causal language model from HuggingFace\n",
    "    dataset_name: str = \"...\"  # Replace with your dataset name (e.g., \"your_username/your_dataset\")\n",
    "    train_split: str = \"train\"  # e.g., \"train[:80%]\" for an 80/20 train/validation split\n",
    "    validation_split: str = None  # e.g., \"train[80%:]\"\n",
    "    output_dir: str = \"./trained_masks\"\n",
    "    per_device_train_batch_size: int = 8\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    learning_rate: float = 2e-5\n",
    "    num_train_epochs: int = 3\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 500\n",
    "    logging_steps: int = 100\n",
    "    logging_dir: str = \"./trained_masks/logs\"\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    report_to: str = \"tensorboard\"\n",
    "    remove_unused_columns: bool = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "\n",
    "# --- Load Tokenizer and Model ---\n",
    "# tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.output_dir,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    learning_rate=args.learning_rate,\n",
    "    num_train_epochs=args.num_train_epochs,\n",
    "    save_steps=args.save_steps,\n",
    "    evaluation_strategy=args.evaluation_strategy if args.validation_split else \"no\",\n",
    "    eval_steps=args.eval_steps if args.validation_split else None,\n",
    "    logging_steps=args.logging_steps,\n",
    "    logging_dir=args.logging_dir,\n",
    "    report_to=args.report_to,  # Enable TensorBoard logging\n",
    "    remove_unused_columns=args.remove_unused_columns\n",
    ")\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator = MergerDataCollator(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "trainer = Mergerrainer(\n",
    "    model=merger,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_mini,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c98648-1943-441a-bd87-f53c6e989b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train the Model ---\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a68a28-1cee-435a-b6d8-26649bf97c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the Model and Tokenizer ---\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model and tokenizer saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27614cd-1cb0-47c7-a7e9-25623df6d373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
