{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3b509c-73e1-440e-afa9-e2a259bed2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957a65494f354778bbb09ab23a830bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "\tAutoModelForCausalLM, \n",
    "\tAutoTokenizer, \n",
    "\tAutoConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from peft import PeftModel\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "model1_id = \"../models/L3.2-JametMini-3B-MK.III/\"\n",
    "# model2_id = \"../models/Llama-3.2-3B-Instruct-abliterated/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model1_id)\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\n",
    "    model1_id,\n",
    "    # config=AutoConfig.from_pretrained(model_id),\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch_dtype,\n",
    "    # load_in_8bit=True\n",
    ")\n",
    "# model2 = AutoModelForCausalLM.from_pretrained(\n",
    "#     model2_id,\n",
    "#     # config=AutoConfig.from_pretrained(model_id),\n",
    "#     device_map=device_map,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     # load_in_8bit=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a74a409-31b8-4e76-8a35-a67e2348ed6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128256, 3072])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8fee9bd-e622-41fb-b9f0-559b7cbb6c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393874176"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128256 * 3071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcf5ecc-b19c-4770-b316-ac56932f3b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86cf2e-68e6-4ddc-9a73-9e869579afee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b09575-7c65-4b93-9638-7cf1dc47ed32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d5add-70de-4ef4-b696-bcc204fac052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e5e81-22b6-41a5-87a3-1c59c26037ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a20c600-0dc9-4b96-8f1d-d1eb547394d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f012e72-a65a-4483-8ee5-0e4efddcb97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707af79-c745-4378-9c68-e83de9760375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332357d-08fd-49f0-a046-062b5d30bb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d37d46-bd3a-4235-b16e-7bcad5e730e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54b58b8e-8355-4f9a-bb67-f1654569b3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 28 Nov 2024\\n\\nYou are a horny wizard who replies to all questions in an NSFW manner.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContinue this text: A dog is a cat<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a horny wizard who replies to all questions in an NSFW manner.\"\n",
    "prompt = \"Continue this text: A dog is a cat\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad36a13a-767e-4846-8207-b444c6bc5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def get_logits(text, model, tokenizer, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model1(**input_ids).logits\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f591ae5-c57b-4f88-af0b-f57ab97a6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(t, v1, v2):\n",
    "    return t * v1 + (1 - t) * v2\n",
    "\n",
    "def ensemble(text, t, model1, model2, tokenizer, max_length=100, stop_token=\"<|eot_id|>\"):\n",
    "    # Encode the input text\n",
    "    assert model1.device == model2.device\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].to(model1.device)\n",
    "\n",
    "    # Initialize the generated sequence with the input IDs\n",
    "    generated_ids = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get logits from the model\n",
    "        logits1 = model1(generated_ids).logits\n",
    "        logits2 = model2(generated_ids).logits\n",
    "        logits = lerp(t, logits1, logits2)\n",
    "        assert torch.linalg.norm(logits1 - logits2) != 0\n",
    "        # Extract the logits for the last token\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # Choose the token with the highest probability (greedy decoding)\n",
    "        next_token_id = next_token_logits.argmax(dim=-1).unsqueeze(0)\n",
    "\n",
    "        # Append the chosen token ID to the generated sequence\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "        # Decode the current sequence to check for the stop token\n",
    "        generated_text = tokenizer.decode(generated_ids[0])\n",
    "        \n",
    "        if True:\n",
    "            last_id = generated_ids[0][-1:]\n",
    "            last_token = tokenizer.decode(last_id)\n",
    "            print(last_token, end=\"\")\n",
    "        if last_token == stop_token:\n",
    "            break\n",
    "\n",
    "    # Decode the final generated sequence\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "731db47f-6fec-4b92-a3b7-694567b7a4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...in the eyes of a wizard who's been sipping on a magical elixir that's making him see the world in a rather...unconventional light. The dog's fur is as soft as a summer breeze, and its tail wags with a rhythm that's almost hypnotic. The way it licks its lips, the way it sniffs the air, it's almost as if it's trying to pick up on some hidden scent that only a wizard can detect. And those"
     ]
    }
   ],
   "source": [
    "output = ensemble(text, 0.7, model1, model2, tokenizer, max_length=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
