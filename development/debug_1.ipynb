{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754cc9c2-a335-4612-9409-ab21f754ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = \"\"\"\n",
    "1. prepare data\n",
    "2. define model\n",
    "    - model a, mask a\n",
    "    - model b, mask b\n",
    "make only masks trainable.\n",
    "make sure it has correct inference.\n",
    "3. define loss function in Trainer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4884f-b796-4384-a2ad-177f2314c3e2",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6baca-8a9b-4a41-8e32-01d6987ae939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "332cf659-c413-4f11-889a-79532efcdd97",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6167e22d-21c5-4027-af0b-a029c8acfe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from modeling_qwen2 import (\n",
    "    Qwen2RMSNorm, \n",
    "    Qwen2RotaryEmbedding, \n",
    "    Qwen2MLP, \n",
    "    Qwen2Attention, \n",
    "    Qwen2FlashAttention2, \n",
    "    Qwen2SdpaAttention, \n",
    "    Qwen2DecoderLayer, \n",
    "    Qwen2PreTrainedModel, \n",
    "    Qwen2Model, \n",
    "    Qwen2ForCausalLM\n",
    ")\n",
    "\n",
    "from configuration_qwen2 import Qwen2Config\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c823e218-fb6e-4d99-9ff5-17f9e794a79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 09:04:19,501 - INFO - Comparing tokenizer at /workspace/models/Arcee-VyLinh/ with tokenizer at /workspace/models/Qwen2.5-Coder-3B/\n",
      "2024-12-15 09:04:19,505 - INFO - Tokenizer at /workspace/models/Arcee-VyLinh/ and /workspace/models/Qwen2.5-Coder-3B/ are the same based on the defined criteria\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import are_tokenizers_same\n",
    "are_tokenizers_same(\n",
    "    paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc9a79e-4ec9-493e-a3da-44bb18c6ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer(path, layer_idx=33):\n",
    "\tstate_dict = {}\n",
    "\tshard_paths = [f for f in os.listdir(path) if f.endswith('.safetensors')]\n",
    "\tfor shard_path in sorted(shard_paths, key=lambda x: int(x.split('-')[1])):\n",
    "\t\tapath = os.path.join(path, shard_path)\n",
    "\t\twith safe_open(apath, framework=\"pt\", device=\"cpu\") as f:\n",
    "\t\t\tfor key in f.keys():\n",
    "\t\t\t\tif f\"layers.{str(layer_idx)}.\" in key:\n",
    "\t\t\t\t\tstate_dict[key] = f.get_tensor(key)\n",
    "\treturn state_dict\n",
    "\n",
    "def strip_prefix(state_dict, prefix=\"model.layers.\"):\n",
    "    \"\"\"Strips 'model.layers.*.' prefix from 'input_layernorm.weight' keys.\"\"\"\n",
    "    return {\n",
    "      k.replace(f\"{prefix}{k.split('.')[2]}.\", \"\") if k.startswith(prefix)\n",
    "      else k: v for k, v in state_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4bdc5b-af91-4b40-b38c-64a50d49281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(\n",
    "    t: float, v0: Union[np.ndarray, torch.Tensor], v1: Union[np.ndarray, torch.Tensor]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    return (1 - t) * v0 + t * v1\n",
    "\n",
    "def weighted_sum(\n",
    "    factors: List[float], \n",
    "    tensors: Union[List[np.ndarray], List[torch.Tensor]]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    result = 0.0\n",
    "    # for factor, tensor in zip(factors, tensors):\n",
    "    #     result += factor * tensor\n",
    "    return sum([tensor * factor for tensor, factor in zip(tensors, factors)])\n",
    "\n",
    "def merge_modules(modules, factors):\n",
    "    \"\"\"\n",
    "    This is only applicable for cases where a static set of scalars\n",
    "    playing as merging factor for every submodules of the passed module.\n",
    "    Not recommend for fine-grained usecases.\n",
    "    \"\"\"\n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out\n",
    "\n",
    "def merge_linears(modules, weight_factors, bias_factors):\n",
    "    param_names = sorted([name for name, _ in modules[0].named_parameters()])\n",
    "    for module in modules:\n",
    "        other_param_names = sorted([name for name, _ in module.named_parameters()])\n",
    "        assert param_names == other_param_names, \"Mismatch tensor names.\"\n",
    "        \n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        if \"weight\" in tensor_name:\n",
    "            factors = weight_factors\n",
    "        elif \"bias\" in tensor_name:\n",
    "            factors = bias_factors\n",
    "        else:\n",
    "            raise ValueError(\"Hey this tensor is neither weight or bias.\")\n",
    "            \n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b59dc3-83fc-4345-b4f6-5cf7665fa607",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fd625a-64ce-4a36-b16d-1b6e61eaa5c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8256a2-89d0-4cfa-8726-1ac197a04470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc68586-edfa-465f-a1e8-513541ccac5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/models/Arcee-VyLinh/', '/workspace/models/Qwen2.5-Coder-3B/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config.model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eca4d1f-96da-4126-bc64-ed531f46d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.model_paths[0]\n",
    "        )\n",
    "        \n",
    "        self.models = nn.ModuleList([\n",
    "            Qwen2ForCausalLM.from_pretrained(\n",
    "            # AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                # torch_dtype=torch.bfloat16,\n",
    "                # device_map={\"\":0}\n",
    "            ) for model_path in config.model_paths\n",
    "        ])\n",
    "        self.__post_init__()\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        # self.masks = torch.nn\n",
    "        pass\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        \"\"\"\n",
    "        activations = []\n",
    "        for i in range(num_layers):\n",
    "            L1 = models[0].layers[i]\n",
    "            L2 = models[1].layers[i]\n",
    "            Lm = alpha * L1 + beta * L2\n",
    "            h1 = L1(h)\n",
    "            h2 = L2(h)\n",
    "            h = Lm(h)\n",
    "            activations.append({\n",
    "                \"1\": h1, \"2\": h2, \"merged\": copy(h)\n",
    "            })\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        - embed_tokens\n",
    "        - norm\n",
    "        - layers\n",
    "            - input_layernorm\n",
    "            - self_attn\n",
    "            - mlp\n",
    "            - post_attention_norm\n",
    "        - lm_head\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd988e00-b69f-4325-bb22-220a26ef033f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336da23a9c3f412085476e73089dc5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7626fe82064cfd822b91b96860eb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57ee117-eee6-4233-adfb-58e8b61cd7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.models[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4912f19f-b0cc-49b1-8a2e-14006fc6f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1 = merger.models[0].model.layers[0].self_attn\n",
    "attn2 = merger.models[1].model.layers[0].self_attn\n",
    "mlp1 = merger.models[0].model.layers[0].mlp\n",
    "mlp2 = merger.models[1].model.layers[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1116d86-d228-4aed-b6ac-1098c044ea62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0159, -0.0432, -0.0080,  ...,  0.0081,  0.0096,  0.0132],\n",
       "        [-0.0330,  0.0110,  0.0085,  ...,  0.0226, -0.0082,  0.0457],\n",
       "        [-0.0092,  0.0111, -0.0134,  ...,  0.0298,  0.0113, -0.0038],\n",
       "        ...,\n",
       "        [-0.0085,  0.0601, -0.0325,  ...,  0.0525, -0.0222,  0.0403],\n",
       "        [-0.0374, -0.0325,  0.0620,  ..., -0.0206,  0.0806,  0.0376],\n",
       "        [ 0.0356,  0.0151,  0.0087,  ..., -0.0306, -0.0072,  0.0378]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn1.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ceff9f2-8598-4fa4-99e4-f584ec495af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from typing import List, Dict\n",
    "\n",
    "def merge_linear(weights: List[nn.Linear], factors: List[float]) -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Merges multiple linear layers by taking a weighted average of their weights and biases.\n",
    "\n",
    "    Args:\n",
    "        weights: A list of nn.Linear layers to merge.\n",
    "        factors: A list of scaling factors corresponding to each layer in 'weights'.\n",
    "\n",
    "    Returns:\n",
    "        A new nn.Linear layer that is the weighted average of the input layers.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of weights and factors don't match, or if the\n",
    "                    layers have incompatible dimensions, device or dtype.\n",
    "    \"\"\"\n",
    "    if len(weights) != len(factors):\n",
    "        raise ValueError(\"The number of weights and factors must be equal.\")\n",
    "\n",
    "    # Check for compatibility, device, and dtype\n",
    "    device = weights[0].weight.device\n",
    "    dtype = weights[0].weight.dtype\n",
    "    if not all(\n",
    "        w.in_features == weights[0].in_features\n",
    "        and w.out_features == weights[0].out_features\n",
    "        and w.weight.device == device\n",
    "        and w.weight.dtype == dtype\n",
    "        for w in weights\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Incompatible linear layers for merging. They must have the same in_features, out_features, device, and dtype.\"\n",
    "        )\n",
    "\n",
    "    # Create a new linear layer with the same dimensions, device and dtype\n",
    "    merged_linear = nn.Linear(\n",
    "        in_features=weights[0].in_features,\n",
    "        out_features=weights[0].out_features,\n",
    "        bias=False,\n",
    "        device=device,\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "    # Calculate the merged weight and bias\n",
    "    merged_weight = torch.zeros_like(weights[0].weight)\n",
    "    merged_bias = (\n",
    "        torch.zeros_like(weights[0].bias, device=device, dtype=dtype)\n",
    "        if weights[0].bias is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        merged_weight += factors[i] * w.weight\n",
    "        if w.bias is not None:\n",
    "            if merged_bias is None:\n",
    "                raise ValueError(\"Cannot merge linear layers if only some have biases.\")\n",
    "            merged_bias += factors[i] * w.bias\n",
    "\n",
    "    # Assign the merged weight and bias to the new layer\n",
    "    with torch.no_grad():\n",
    "        merged_linear.weight.copy_(merged_weight)\n",
    "        if merged_bias is not None:\n",
    "            merged_linear.bias = nn.Parameter(merged_bias)\n",
    "\n",
    "    return merged_linear\n",
    "\n",
    "def merge_module_recursive(\n",
    "    target_module: nn.Module, modules_dict: Dict[str, List[nn.Module]], factors: List[float]\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively merges multiple modules by taking a weighted average of their Linear layer weights and biases.\n",
    "\n",
    "    Args:\n",
    "        target_module: The target module where the merged weights will be stored.\n",
    "        modules_dict: A dictionary where keys are module names and values are lists of modules to merge.\n",
    "        factors: A list of scaling factors corresponding to each list of modules.\n",
    "    \"\"\"\n",
    "\n",
    "    for name, module in target_module.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if name not in modules_dict:\n",
    "                raise ValueError(\n",
    "                    f\"Missing module {name} in modules_dict. Make sure all linear layer weights are provided\"\n",
    "                )\n",
    "            merged_linear = merge_linear(modules_dict[name], factors)\n",
    "            # Find the parent module\n",
    "            parent_module_name = \".\".join(name.split(\".\")[:-1])\n",
    "            layer_name = name.split(\".\")[-1]\n",
    "\n",
    "            if parent_module_name:\n",
    "                parent_module = target_module.get_submodule(parent_module_name)\n",
    "            else:\n",
    "                parent_module = target_module\n",
    "\n",
    "            # Replace the original Linear layer with merged one\n",
    "            setattr(parent_module, layer_name, merged_linear)\n",
    "\n",
    "def merge_modules(modules: List[nn.Module], factors: List[float]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Merges multiple modules by taking a weighted average of their Linear layer weights and biases.\n",
    "    The merged weights are stored into a deepcopy of the first module in the list.\n",
    "\n",
    "    Args:\n",
    "        modules: A list of nn.Modules to merge.\n",
    "        factors: A list of scaling factors corresponding to each module in 'modules'.\n",
    "\n",
    "    Returns:\n",
    "        A new nn.Module that is the weighted average of the input modules.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of modules and factors don't match.\n",
    "    \"\"\"\n",
    "    if len(modules) != len(factors):\n",
    "        raise ValueError(\"The number of modules and factors must be equal.\")\n",
    "\n",
    "    # Check device and dtype consistency across all modules\n",
    "    device = modules[0].parameters().__next__().device\n",
    "    dtype = modules[0].parameters().__next__().dtype\n",
    "    if not all(p.device == device and p.dtype == dtype for module in modules for p in module.parameters()):\n",
    "        raise ValueError(\"All modules must be on the same device and have the same dtype.\")\n",
    "\n",
    "    # Create a deep copy of the first module to store the merged weights\n",
    "    merged_module = copy.deepcopy(modules[0])\n",
    "\n",
    "    # Dictionary to hold corresponding linear layers from each module\n",
    "    modules_to_merge = {\n",
    "        name: [] for name, _ in merged_module.named_modules() if isinstance(_, nn.Linear)\n",
    "    }\n",
    "    for module in modules:\n",
    "        for name, layer in module.named_modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                modules_to_merge[name].append(layer)\n",
    "\n",
    "    # Merge the modules recursively\n",
    "    merge_module_recursive(merged_module, modules_to_merge, factors)\n",
    "\n",
    "    # Ensure the merged module has the correct device and dtype\n",
    "    merged_module.to(device=device, dtype=dtype)\n",
    "\n",
    "    return merged_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c6e197-7a64-4c61-ae4c-01ec81fc5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mlp = merge_modules(modules=[mlp1, mlp2], factors=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4f12e66-a7cf-40b4-b099-95d18d57aba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mQwen2MLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Define the computation performed at every call.\n",
       "\n",
       "Should be overridden by all subclasses.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within\n",
       "    this function, one should call the :class:`Module` instance afterwards\n",
       "    instead of this since the former takes care of running the\n",
       "    registered hooks while the latter silently ignores them.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /workspace/merge/modeling_qwen2.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Qwen2MLP.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc55cde6-7362-43bc-b36e-c0fd8b6371ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_merged_mlp(x: torch.Tensor, modules: List[nn.Module], factors: List[float]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a forward pass that simulates the behavior of a merged MLP module.\n",
    "\n",
    "    Args:\n",
    "        modules: A list of MLP modules (e.g., Qwen2MLP instances).\n",
    "        factors: A list of scaling factors corresponding to each module in 'modules'.\n",
    "        x: The input tensor.\n",
    "\n",
    "    Returns:\n",
    "        The output tensor after the forward pass.\n",
    "    \"\"\"\n",
    "    factors = torch.tensor(factors).to(x.device, dtype=x.dtype).view(-1, 1, 1, 1)  # Reshape factors for broadcasting\n",
    "    gate_output = torch.stack([m.gate_proj(x) for m in modules]).mul(factors).sum(0)\n",
    "    up_output = torch.stack([m.up_proj(x) for m in modules]).mul(factors).sum(0)\n",
    "    act_output = modules[0].act_fn(gate_output)  # Assuming all modules have the same activation function\n",
    "    result = torch.stack([m.down_proj(act_output * up_output) for m in modules]).mul(factors).sum(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3986afd-841a-4795-b774-2e9758ce1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = merged_mlp.parameters().__next__().device\n",
    "dtype = merged_mlp.parameters().__next__().dtype\n",
    "x = torch.rand(1, 4, 2048).to(device, dtype=dtype)\n",
    "o1 = merged_mlp(x)\n",
    "o2 = forward_merged_mlp(x, modules=[mlp1, mlp2], factors=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d65bd5d4-2ce1-4e0d-a655-254bc03fc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(o1, o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65e11c1a-3abb-43f9-81fa-8c25a0a045bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules[0].down_proj.weight\n",
    "gate_output.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "286a1532-4644-4142-ab2b-029b4d4fefd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3750, -0.3301, -0.0016,  ..., -0.0093, -0.2695,  0.0986],\n",
       "          [ 0.2930, -0.2949,  0.0933,  ...,  0.0483, -0.2949, -0.0649],\n",
       "          [ 0.2676, -0.3789,  0.1973,  ...,  0.0134, -0.7227,  0.1367],\n",
       "          [ 0.2715, -0.2988, -0.0547,  ..., -0.1777, -0.7695,  0.0850]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda:0\"\n",
    "h = torch.rand(1, 4, 2048, dtype=torch.bfloat16).to(device)\n",
    "p = torch.arange(4, dtype=torch.bfloat16, device=device).unsqueeze(0)\n",
    "attn1.forward(h, position_ids=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93147db6-84be-43db-9559-dd03c4c5988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, param_bits):\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    non_trainable_params = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        num_params = param.numel()  # Get the number of elements in the parameter\n",
    "        total_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "        else:\n",
    "            non_trainable_params += num_params\n",
    "\n",
    "    total_gigabytes = total_params * (param_bits / 8) / (1024**3)\n",
    "    memory = f\"{total_gigabytes:.2f} GB\"\n",
    "    \n",
    "    return total_params, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a75cc9e3-c80c-4a70-9d0c-b71da5fa1cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9439744, '0.02 GB'), (67633152, '0.13 GB'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(attn1, 16), count_parameters(mlp1, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59306b-6580-4484-a88a-0c39f30c8bb8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25550585-74ef-4ba2-9e4e-c6f203fbd053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.weight = nn.Parameter(torch.tensor(value)) # Corrected typo here\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        try:\n",
    "            self.weight * torch.rand(self.size)\n",
    "        except RuntimeError:\n",
    "            print(\"mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.weight * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ceee65-d173-4e63-958a-1d7cb423b5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskConfig {\n",
       "  \"mode\": \"scalar\",\n",
       "  \"size\": [\n",
       "    4,\n",
       "    8\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"value\": 0.5\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_config = MaskConfig(\n",
    "    mode=\"scalar\", value=0.5, size=torch.Size((4, 8))\n",
    ")\n",
    "mask_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c1c765-7d62-4000-bbcf-34072ab1947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithMask(nn.Module):\n",
    "    def __init__(self, linear, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.mask_config = mask_config\n",
    "        if linear.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not imcompatible with linear, reinitializing...\")\n",
    "        self.mask_config.size = linear.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        masked_linear = self.mask(self.linear.weight)\n",
    "        return nn.functional.linear(x, masked_linear, self.linear.bias)\n",
    "\n",
    "class LinearsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        linears: List[nn.Module], \n",
    "        modes: List[str] = [\"scalar\"], \n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [linear.weight.shape for linear in linears]\n",
    "        if values is None or len(values) != len(linears):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with linear layers: {linears}\")\n",
    "            \n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size) \n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_linears = nn.ModuleList(\n",
    "            [LinearWithMask(linear, mask_config) \n",
    "             for linear, mask_config in zip(linears, mask_configs)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = 0.0\n",
    "        for masked_linear in self.masked_linears:\n",
    "            output += masked_linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc42d400-1dc8-4413-96de-9b0fb0123c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with 1 Linear components passed!\n",
      "Test with 2 Linear components passed!\n",
      "Test with 3 Linear components passed!\n",
      "Test with 4 Linear components passed!\n",
      "Test with 5 Linear components passed!\n",
      "Test with 6 Linear components passed!\n",
      "Test with 7 Linear components passed!\n",
      "Test with 8 Linear components passed!\n",
      "Test with 9 Linear components passed!\n",
      "Test with 10 Linear components passed!\n",
      "Test with 11 Linear components passed!\n",
      "Test with 12 Linear components passed!\n",
      "Test with 13 Linear components passed!\n",
      "Test with 14 Linear components passed!\n",
      "Test with 15 Linear components passed!\n",
      "Test with 16 Linear components passed!\n",
      "Test with 17 Linear components passed!\n",
      "Test with 18 Linear components passed!\n",
      "Test with 19 Linear components passed!\n",
      "Test with 20 Linear components passed!\n"
     ]
    }
   ],
   "source": [
    "# --- Testing ---\n",
    "def test_multiple_linear_components(input_size: int, output_size: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        linears = [nn.Linear(input_size, output_size, bias=False) for _ in range(num_components)]\n",
    "        x = torch.rand(1, input_size)\n",
    "\n",
    "        for _ in range(10):  # Reduced number of iterations for faster testing in a notebook\n",
    "            values = np.random.rand(num_components).tolist() # cast to list\n",
    "            weights_with_masks = LinearsWithMasks(linears=linears, modes=[\"scalar\"] * num_components, values=values)\n",
    "\n",
    "            individual_outputs = [linear(x) for linear in linears]\n",
    "            expected_output = sum(val * out for val, out in zip(values, individual_outputs))\n",
    "            actual_output = weights_with_masks(x)\n",
    "\n",
    "            torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "        print(f\"Test with {num_components} Linear components passed!\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define input and output sizes\n",
    "input_size = 1024\n",
    "output_size = 1024\n",
    "\n",
    "# Run tests\n",
    "test_multiple_linear_components(input_size, output_size, [i + 1 for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5a66a8-d14b-43eb-84e3-de2fb0d4bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormWithMask(nn.Module):\n",
    "    def __init__(self, rms_norm: Qwen2RMSNorm, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.rms_norm = rms_norm\n",
    "        self.mask_config = mask_config\n",
    "        if rms_norm.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with RMSNorm, reinitializing...\")\n",
    "        self.mask_config.size = rms_norm.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        masked_weight = self.mask(self.rms_norm.weight)\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.rms_norm.variance_epsilon)\n",
    "        return masked_weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RMSNormsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rms_norms: List[Qwen2RMSNorm],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [rms_norm.weight.shape for rms_norm in rms_norms]\n",
    "        if values is None or len(values) != len(rms_norms):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with RMSNorm layers: {rms_norms}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_rms_norms = nn.ModuleList(\n",
    "            [RMSNormWithMask(rms_norm, mask_config)\n",
    "             for rms_norm, mask_config in zip(rms_norms, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        output = 0.0\n",
    "        for masked_rms_norm in self.masked_rms_norms:\n",
    "            output += masked_rms_norm(hidden_states)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cae51d3-6384-48ed-8046-83dc10ce428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with 1 RMSNorm components passed!\n",
      "Test with 2 RMSNorm components passed!\n",
      "Test with 3 RMSNorm components passed!\n",
      "Test with 4 RMSNorm components passed!\n",
      "Test with 5 RMSNorm components passed!\n"
     ]
    }
   ],
   "source": [
    "def test_multiple_rms_norm_components(hidden_size: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        rms_norms = [Qwen2RMSNorm(hidden_size) for _ in range(num_components)]\n",
    "        hidden_states = torch.rand(2, 4, hidden_size)\n",
    "\n",
    "        for _ in range(10):\n",
    "            values = np.random.rand(num_components).tolist()\n",
    "            rms_norms_with_masks = RMSNormsWithMasks(rms_norms=rms_norms, modes=[\"scalar\"] * num_components, values=values)\n",
    "\n",
    "            individual_outputs = [rms_norm(hidden_states) for rms_norm in rms_norms]\n",
    "            expected_output = sum(val * out for val, out in zip(values, individual_outputs))\n",
    "            actual_output = rms_norms_with_masks(hidden_states)\n",
    "            \n",
    "            torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "        print(f\"Test with {num_components} RMSNorm components passed!\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define input and output sizes\n",
    "hidden_size = 2048\n",
    "\n",
    "# Run tests for RMSNormsWithMasks\n",
    "test_multiple_rms_norm_components(hidden_size, [i+1 for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e5927a-b624-4649-b054-040dd0f54f34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingWithMask(nn.Module):\n",
    "    def __init__(self, embedding: nn.Embedding, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.mask_config = mask_config\n",
    "        if embedding.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with Embedding, reinitializing...\")\n",
    "        self.mask_config.size = embedding.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        masked_weight = self.mask(self.embedding.weight)\n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            masked_weight,\n",
    "            padding_idx=self.embedding.padding_idx,\n",
    "            max_norm=self.embedding.max_norm,\n",
    "            norm_type=self.embedding.norm_type,\n",
    "            scale_grad_by_freq=self.embedding.scale_grad_by_freq,\n",
    "            sparse=self.embedding.sparse,\n",
    "        )\n",
    "\n",
    "class EmbeddingsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [embedding.weight.shape for embedding in embeddings]\n",
    "        if values is None or len(values) != len(embeddings):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with Embedding layers: {embeddings}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_embeddings = nn.ModuleList(\n",
    "            [EmbeddingWithMask(embedding, mask_config)\n",
    "             for embedding, mask_config in zip(embeddings, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        output = 0.0\n",
    "        for masked_embedding in self.masked_embeddings:\n",
    "            output += masked_embedding(input_ids)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b93dc2b3-7711-4eec-bd48-349de66498aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with 1 Embedding components passed!\n",
      "Test with 2 Embedding components passed!\n",
      "Test with 3 Embedding components passed!\n",
      "Test with 4 Embedding components passed!\n",
      "Test with 5 Embedding components passed!\n"
     ]
    }
   ],
   "source": [
    "def test_multiple_embedding_components(num_embeddings: int, embedding_dim: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        embeddings = [nn.Embedding(num_embeddings, embedding_dim) for _ in range(num_components)]\n",
    "        input_ids = torch.randint(0, num_embeddings, (2, 5))  # Example input_ids\n",
    "\n",
    "        for _ in range(10):\n",
    "            values = np.random.rand(num_components).tolist()\n",
    "            embeddings_with_masks = EmbeddingsWithMasks(embeddings=embeddings, modes=[\"scalar\"] * num_components, values=values)\n",
    "\n",
    "            individual_outputs = [embedding(input_ids) for embedding in embeddings]\n",
    "            expected_output = sum(val * out for val, out in zip(values, individual_outputs))\n",
    "            actual_output = embeddings_with_masks(input_ids)\n",
    "\n",
    "            torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "        print(f\"Test with {num_components} Embedding components passed!\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define parameters for Embedding\n",
    "num_embeddings = 2048\n",
    "embedding_dim = 2048\n",
    "\n",
    "# Run tests for EmbeddingsWithMasks\n",
    "test_multiple_embedding_components(num_embeddings, embedding_dim, [i + 1 for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf260def-3212-4747-9cd0-cdd07127b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_masks(target_module, ref_modules):\n",
    "    \"\"\"\n",
    "    Recursively replaces normal components with masked components.\n",
    "    \n",
    "    Args:\n",
    "      module: The module in which to replace layers.\n",
    "    \"\"\"\n",
    "    for name, target_child in target_module.named_children():\n",
    "        ref_children = [getattr(module, name) for module in ref_modules]\n",
    "        modes = [\"scalar\" for _ in ref_children]\n",
    "        values = [0.0 for _ in ref_children]\n",
    "        values[0] = 1.0\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            setattr(target_module, name, LinearsWithMasks(\n",
    "                ref_children, modes, values\n",
    "            ))\n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            setattr(target_module, name, EmbeddingsWithMasks(\n",
    "                ref_children, modes, values\n",
    "            ))\n",
    "        elif type(target_child).__name__ == Qwen2RMSNorm.__name__:\n",
    "            setattr(target_module, name, RMSNormsWithMasks(\n",
    "                ref_children, modes, values\n",
    "            ))\n",
    "        else:\n",
    "            place_masks(target_child, ref_children)\n",
    "\n",
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class DecoderMerger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.configs = [Qwen2Config.from_pretrained(path) \n",
    "                        for path in merge_config.model_paths]\n",
    "        \n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.decoders = nn.ModuleList(\n",
    "            Qwen2DecoderLayer(config, layer_idx=1) for config in self.configs\n",
    "        )\n",
    "        for i in range(len(self.decoders)):\n",
    "            path = merge_config.model_paths[i]\n",
    "            state_dict = load_layer(path, layer_idx=1)\n",
    "            state_dict = strip_prefix(state_dict)\n",
    "            self.decoders[i].load_state_dict(\n",
    "                state_dict=state_dict\n",
    "            )\n",
    "        self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config):\n",
    "        self.merger = copy.deepcopy(self.decoders[0])\n",
    "        place_masks(self.merger, self.decoders)\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass\n",
    "        \n",
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.num_models = len(merge_config.model_paths)\n",
    "        self.configs = [\n",
    "            AutoConfig.from_pretrained(path) \n",
    "            for path in merge_config.model_paths\n",
    "        ]\n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.models = nn.ModuleList([\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                merge_config.model_paths[i], \n",
    "                config=self.configs[i],\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ) \n",
    "            for i in range(self.num_models)\n",
    "        ])\n",
    "        self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config):\n",
    "        # dummy_config = copy.deepcopy(self.configs[0])\n",
    "        # dummy_config.update({\"hidden_size\": 1, \"intermediate_size\": 1})\n",
    "        # self.merger = AutoModelForCausalLM.from_config(dummy_config)\n",
    "        self.merger = copy.deepcopy(self.models[0])\n",
    "        place_masks(self.merger, self.models)\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d01ba414-f897-4cbe-8ccd-ce6ae3f721d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4d5091-ec85-4a42-a096-510c88fcaacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa1732391d14111962aacd9192e518d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f2ce0d7a1a4b50b927b12accbb17a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f84a774-2a5c-4e4c-aea1-3f3c0b8669d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.merger.model.embed_tokens.masked_embeddings[1].mask.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3495e227-f1e9-46b6-b08c-30d08cbdbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d9b6c5e-e825-4286-ab6c-87a9d99c361c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Merger(\n",
       "  (models): ModuleList(\n",
       "    (0-1): 2 x Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (merger): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): EmbeddingsWithMasks(\n",
       "        (masked_embeddings): ModuleList(\n",
       "          (0-1): 2 x EmbeddingWithMask(\n",
       "            (embedding): Embedding(151936, 2048)\n",
       "            (mask): Mask()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (k_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (v_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (o_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (up_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (down_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): RMSNormsWithMasks(\n",
       "            (masked_rms_norms): ModuleList(\n",
       "              (0-1): 2 x RMSNormWithMask(\n",
       "                (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                (mask): Mask()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNormsWithMasks(\n",
       "            (masked_rms_norms): ModuleList(\n",
       "              (0-1): 2 x RMSNormWithMask(\n",
       "                (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                (mask): Mask()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNormsWithMasks(\n",
       "        (masked_rms_norms): ModuleList(\n",
       "          (0-1): 2 x RMSNormWithMask(\n",
       "            (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (mask): Mask()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): LinearsWithMasks(\n",
       "      (masked_linears): ModuleList(\n",
       "        (0-1): 2 x LinearWithMask(\n",
       "          (linear): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "          (mask): Mask()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b82f53d-8ca0-4006-80f1-c47863ed9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_modules(module1, module2, rtol=1e-05, atol=1e-08, verbose=True):\n",
    "    \"\"\"\n",
    "    Compares the weights of two modules using torch.testing.assert_close.\n",
    "    Assumes modules have identical keys.\n",
    "    \"\"\"\n",
    "    state_dict1 = module1.state_dict()\n",
    "    state_dict2 = module2.state_dict()\n",
    "\n",
    "    # Iterate directly through the keys of one module's state_dict\n",
    "    for key in state_dict1:  \n",
    "        tensor1 = state_dict1[key]\n",
    "        tensor2 = state_dict2[key]\n",
    "\n",
    "        # No need for shape check, assumed to be identical\n",
    "        try:\n",
    "            torch.testing.assert_close(tensor1, tensor2, rtol=rtol, atol=atol)\n",
    "            if verbose:\n",
    "                print(f\"  OK: Tensor '{key}' is close within tolerance.\")\n",
    "        except AssertionError as e:\n",
    "            if verbose:\n",
    "                print(f\"  ERROR: Tensor '{key}' is NOT close within tolerance.\")\n",
    "            raise AssertionError(f\"Tensor '{key}' comparison failed: {e}\") from e\n",
    "    print(\"--- All tensors are identical! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a906f584-eae4-412c-bf8c-79f58dbcd096",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Merger(\n",
       "  (models): ModuleList(\n",
       "    (0-1): 2 x Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (merger): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): EmbeddingsWithMasks(\n",
       "        (masked_embeddings): ModuleList(\n",
       "          (0-1): 2 x EmbeddingWithMask(\n",
       "            (embedding): Embedding(151936, 2048)\n",
       "            (mask): Mask()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (k_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (v_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (o_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (up_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (down_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): RMSNormsWithMasks(\n",
       "            (masked_rms_norms): ModuleList(\n",
       "              (0-1): 2 x RMSNormWithMask(\n",
       "                (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                (mask): Mask()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNormsWithMasks(\n",
       "            (masked_rms_norms): ModuleList(\n",
       "              (0-1): 2 x RMSNormWithMask(\n",
       "                (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                (mask): Mask()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNormsWithMasks(\n",
       "        (masked_rms_norms): ModuleList(\n",
       "          (0-1): 2 x RMSNormWithMask(\n",
       "            (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (mask): Mask()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): LinearsWithMasks(\n",
       "      (masked_linears): ModuleList(\n",
       "        (0-1): 2 x LinearWithMask(\n",
       "          (linear): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "          (mask): Mask()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b66d55e7-bf40-4788-a8c1-1e8a657c98a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK: Tensor 'self_attn.q_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.q_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.k_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.k_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.v_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.v_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.o_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.gate_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.up_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.down_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'input_layernorm.weight' is close within tolerance.\n",
      "  OK: Tensor 'post_attention_layernorm.weight' is close within tolerance.\n",
      "--- All tensors are identical! ---\n",
      "  OK: Tensor 'self_attn.q_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.q_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.k_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.k_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.v_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.v_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.o_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.gate_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.up_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.down_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'input_layernorm.weight' is close within tolerance.\n",
      "  OK: Tensor 'post_attention_layernorm.weight' is close within tolerance.\n",
      "--- All tensors are identical! ---\n"
     ]
    }
   ],
   "source": [
    "m1 = merger.models[0].model.layers[1]\n",
    "m2 = merger.models[1].model.layers[2]\n",
    "\n",
    "mo = merge_modules([m1, m2], [0.0, 1.0])\n",
    "compare_modules(mo, m2, verbose=True)\n",
    "\n",
    "mo = merge_modules([m1, m2], [1.0, 0.0])\n",
    "compare_modules(mo, m1, verbose=True)\n",
    "\n",
    "# mo = merge_modules([m1, m2], [0.5, 0.5])\n",
    "# compare_modules(mo, m1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74c6db45-87c8-4691-8f8d-e17a52eb68d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.state_dict()['q_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddf601b4-6d8a-402b-8b08-40ba5eaeb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, model, tokenizer, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()\n",
    "\n",
    "def get_logits(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_ids).logits\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6903352-4e32-48f5-bb38-390331434611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nContinue this text: A dog is a cat<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "prompt = \"Continue this text: A dog is a cat\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8678c6df-a9bc-4d5c-a683-fc0742c8e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_merger = get_logits(text, model=merger.merger, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4cfa9d0-f4df-45b0-a4fc-990a22637305",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_1 = get_logits(text, model=merger.models[0], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81bf0bdb-607e-4ec0-b6a7-e9584d817060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 8.0625,  6.9688,  3.6094,  ..., -0.8320, -0.8320, -0.8320],\n",
       "          [ 6.0312,  6.9688,  7.0938,  ..., -1.4219, -1.4219, -1.4219],\n",
       "          [ 5.8438,  8.7500, 11.1875,  ..., -3.9844, -3.9844, -3.9844],\n",
       "          ...,\n",
       "          [ 1.4141,  5.7500, -3.6562,  ..., -0.3164, -0.3164, -0.3164],\n",
       "          [ 9.3750,  7.2500,  8.1250,  ..., -6.0312, -6.0312, -6.0312],\n",
       "          [16.2500, 17.8750,  9.1875,  ..., -1.8438, -1.8438, -1.8438]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[ 2.8438,  2.4844,  2.6719,  ..., -0.5352, -0.5352, -0.5352],\n",
       "          [ 2.7344,  3.3281,  3.1406,  ..., -0.8789, -0.8789, -0.8789],\n",
       "          [ 2.5938,  2.7344,  4.2500,  ..., -2.1562, -2.1562, -2.1562],\n",
       "          ...,\n",
       "          [ 5.8438,  3.5781,  3.8906,  ..., -3.2969, -3.2969, -3.2969],\n",
       "          [ 6.3438,  3.3438,  5.0312,  ..., -3.2031, -3.2031, -3.2031],\n",
       "          [ 5.8750,  3.5469,  5.5000,  ..., -3.0000, -3.0000, -3.0000]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_1, logits_merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16fd13f9-f0bc-4b3a-822d-897b0af54035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", A\n",
      " the cat in the A is an helpful assistant for A and the cat, A.\n",
      " A  help to the A and the cat in A is a helpful assistant\n",
      ". A cat is not a, A, A\n",
      "\n",
      " A\n",
      "\n",
      " the cat is A, A\n",
      " and the cat in A\n",
      " is a useful cat.\n",
      " A is\n",
      " the cat in A is A\n",
      " the A\n",
      " and A cat in A is the A\n",
      " and A cat, A is A\n",
      " in\n"
     ]
    }
   ],
   "source": [
    "answer = generate(merger.merger, text, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d59988-02b4-4cb4-9216-3268006f859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e58f140-df44-4a7c-9ac7-d894b67a4498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2MLP"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(merger.merger.model.layers[0].mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "829aac8d-fb61-4247-ba34-7329e7703287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(merger.models[1].model.norm, Qwen2RMSNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4e701dc-24b4-4181-9564-844f55efda08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(merger.models[1].model.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb862b9a-4bca-44f1-afa4-4b5e1e453728",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Qwen2RMSNorm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[43mQwen2RMSNorm\u001b[49m(\u001b[38;5;241m8\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Qwen2RMSNorm' is not defined"
     ]
    }
   ],
   "source": [
    "(type(Qwen2RMSNorm(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adef19a-a9c4-4b0a-a00b-83bf6489f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate(merger.merger, text, max_new_tokens=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b420065-ba2c-4a8a-8035-95f7eed4320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merger.merger.self_attn.q_proj.masked_linears[1].linear.weight.data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070239c9-72e6-4412-9bbd-cc72de79f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merger.decoders[1].self_attn.q_proj.weight.data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d34500-e783-482d-a86a-708b4bab302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin = merger.decoders[0].mlp.gate_proj.weight\n",
    "# ref = merger.merger.mlp.gate_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf242f4-a5ff-4eec-83ff-4d633fefc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = Qwen2ForCausalLM.from_pretrained(\n",
    "#     merge_config.model_paths[0]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff26b74-cdc8-4ea2-8ebf-ca09d8268476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.model.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a9f5b-bf20-42f0-8e4c-d2a04d632347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp1 = model1.model.layers[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f49c66-d524-48a1-b8c9-9d7402500088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_mlp1 = replace_linears_with_masked(mlp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ce0e6-286f-422e-a909-10caa0059b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0460d67b-29c3-4c5b-8351-878056ca1249",
   "metadata": {},
   "source": [
    "## attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e9ff5d-7ec3-49ec-b3c3-5f416dea9511",
   "metadata": {},
   "outputs": [],
   "source": [
    "major_revision = \"\"\"\n",
    "In this version, I will try to merge weight before forwarding. This will\n",
    "accurately reflect what happens when using a model with merged weights, avoiding\n",
    "number precision problems.\n",
    "\n",
    "In previous attempts, I do forward with multiple weights to get multiple outputs,\n",
    "and then I merge outputs by taking sum of them.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53eec759-bfb6-465b-9d65-2e4a55c7ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = \"\"\"\n",
    "the order of multiplication operation matters to the number precision.\n",
    "A * B != B * A\n",
    "this subtlety led to some incorrectness when i compared multiple implementation of weight merging.\n",
    "specifically, the difference was traced back to the implementation of \n",
    "weighted_sum function and Mask.forward(). the former used `factor * weight` while the latter used\n",
    "`weight * factor`. it's now fixed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901e231a-9965-4355-ac30-4784d493e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.weight = nn.Parameter(torch.tensor(value)) # Corrected typo here\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        try:\n",
    "            self.weight * torch.rand(self.size)\n",
    "        except RuntimeError:\n",
    "            print(\"mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Be really careful here (though I do not think it matters that much),\n",
    "        When testing, it's important that the masking operation is implemented\n",
    "        with `x = self.weight * x` instead of `x = x * self.weight`.\n",
    "\n",
    "        Neither of those two implementation is superior, however I need to be\n",
    "        consistent when doing testing because the phenonmenon above could lead\n",
    "        to some number imprecision, which may fail `torch.testing.assert_close`\n",
    "        \"\"\"\n",
    "        x = self.weight * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad488f6a-8d6a-4e10-addd-7e45772a0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.weight = nn.Parameter(torch.tensor(value))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        if self.size is not None:\n",
    "            try:\n",
    "                self.weight * torch.rand(self.size)\n",
    "            except RuntimeError:\n",
    "                print(\"mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.size is None:\n",
    "            return self.weight * x\n",
    "        else:\n",
    "            if self.size != x.shape:\n",
    "                print(\"The shape of input does not match that of the mask.\")\n",
    "            return self.weight * x\n",
    "\n",
    "class LinearWithMask(nn.Module):\n",
    "    def __init__(self, linear, weight_mask_config: MaskConfig, bias_mask_config: MaskConfig = None):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.weight_mask_config = weight_mask_config\n",
    "        self.bias_mask_config = bias_mask_config\n",
    "\n",
    "        if linear.weight.shape != weight_mask_config.size:\n",
    "            print(\"Weight mask shape is not compatible with linear, reinitializing...\")\n",
    "            self.weight_mask_config.size = linear.weight.shape\n",
    "        self.weight_mask = Mask(self.weight_mask_config)\n",
    "\n",
    "        if linear.bias is not None and bias_mask_config is not None:\n",
    "            if linear.bias.shape != bias_mask_config.size:\n",
    "                print(\"Bias mask shape is not compatible with linear, reinitializing...\")\n",
    "                self.bias_mask_config.size = linear.bias.shape\n",
    "            self.bias_mask = Mask(self.bias_mask_config)\n",
    "        else:\n",
    "            self.bias_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        masked_weight = self.weight_mask(self.linear.weight)\n",
    "        if self.linear.bias is not None and self.bias_mask is not None:\n",
    "            masked_bias = self.bias_mask(self.linear.bias)\n",
    "        else:\n",
    "            masked_bias = self.linear.bias\n",
    "        return nn.functional.linear(x, masked_weight, masked_bias)\n",
    "\n",
    "class LinearsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        linears: List[nn.Module],\n",
    "        weight_modes: List[str] = [\"scalar\"],\n",
    "        weight_values: List[float] = None,\n",
    "        bias_modes: List[str] = [\"scalar\"],\n",
    "        bias_values: List[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not all(isinstance(linear, nn.Linear) for linear in linears):\n",
    "            raise ValueError(\"All elements in 'linears' must be instances of nn.Linear.\")\n",
    "\n",
    "        weight_sizes = [linear.weight.shape for linear in linears]\n",
    "        bias_sizes = [linear.bias.shape if linear.bias is not None else None for linear in linears]\n",
    "        \n",
    "        if weight_values is None or len(weight_values) != len(linears):\n",
    "            raise ValueError(f\"weight_values for masks: {weight_values} do not match with linear layers: {linears}\")\n",
    "        if bias_values is None:\n",
    "            bias_values = [None] * len(linears)\n",
    "        if len(bias_values) != len(linears):\n",
    "            raise ValueError(f\"bias_values for masks: {bias_values} do not match with linear layers: {linears}\")\n",
    "\n",
    "        weight_mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(weight_modes, weight_values, weight_sizes)\n",
    "        ]\n",
    "        bias_mask_configs = [\n",
    "            MaskConfig(mode, value, size) if size is not None else None\n",
    "            for mode, value, size in zip(bias_modes, bias_values, bias_sizes)\n",
    "        ]\n",
    "\n",
    "        self.masked_linears = nn.ModuleList(\n",
    "            [LinearWithMask(linear, weight_mask_config, bias_mask_config)\n",
    "             for linear, weight_mask_config, bias_mask_config \n",
    "             in zip(linears, weight_mask_configs, bias_mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = [linear.weight_mask(linear.linear.weight) \n",
    "                   for linear in self.masked_linears]\n",
    "        # merged_weight = torch.sum(torch.stack(weights), dim=0)\n",
    "        merged_weight = sum(weights)\n",
    "\n",
    "        biases = [\n",
    "            linear.bias_mask(linear.linear.bias) \n",
    "            if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "            else linear.linear.bias for linear in self.masked_linears\n",
    "        ]\n",
    "        biases = [\n",
    "            b if b is not None\n",
    "            else torch.zeros_like(weights[0][:, 0])\n",
    "            for b in biases\n",
    "        ]\n",
    "        # merged_bias = torch.sum(torch.stack(biases))\n",
    "        merged_bias = sum(biases)\n",
    "        if all(b is None for b in biases):\n",
    "            merged_bias = None\n",
    "\n",
    "        return nn.functional.linear(x, merged_weight, merged_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1544e1-1acb-452f-be2d-5369732d6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_linear_components(input_size: int, output_size: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        # Test with bias\n",
    "        linears_with_bias = [nn.Linear(input_size, output_size, bias=True) for _ in range(num_components)]\n",
    "        # Test without bias\n",
    "        linears_without_bias = [nn.Linear(input_size, output_size, bias=False) for _ in range(num_components)]\n",
    "        \n",
    "        x = torch.rand(1, input_size)\n",
    "\n",
    "        for _ in range(10):  # Reduced number of iterations for faster testing\n",
    "            weight_values = np.random.rand(num_components).tolist()\n",
    "            bias_values = np.random.rand(num_components).tolist()\n",
    "\n",
    "            # Test with bias\n",
    "            weights_with_masks_bias = LinearsWithMasks(\n",
    "                linears=linears_with_bias,\n",
    "                weight_modes=[\"scalar\"] * num_components,\n",
    "                weight_values=weight_values,\n",
    "                bias_modes=[\"scalar\"] * num_components,\n",
    "                bias_values=bias_values,\n",
    "            )\n",
    "            if True:\n",
    "                outs_weights_only = [torch.matmul(x, w_mask * lin.weight.T) \n",
    "                                     for w_mask, lin in zip(weight_values, linears_with_bias)]\n",
    "                outs_biases_only = [b_mask * lin.bias.T for b_mask, lin in zip(bias_values, linears_with_bias)]\n",
    "                expected_output_bias = sum([w + b for w, b in zip(outs_weights_only, outs_biases_only)])\n",
    "                \n",
    "                actual_output_bias = weights_with_masks_bias(x)\n",
    "    \n",
    "                torch.testing.assert_close(actual_output_bias, expected_output_bias, rtol=1e-6, atol=1e-6)\n",
    "\n",
    "            # Test with merged weights\n",
    "            if True:\n",
    "                merged_linear = merge_linears(linears_with_bias, weight_factors=weight_values, bias_factors=bias_values)\n",
    "                expected_output_bias = merged_linear(x)\n",
    "                actual_output_bias = weights_with_masks_bias(x)\n",
    "                torch.testing.assert_close(actual_output_bias, expected_output_bias, rtol=1e-6, atol=1e-6)\n",
    "            \n",
    "\n",
    "            # Test without bias\n",
    "            weights_with_masks_no_bias = LinearsWithMasks(\n",
    "                linears=linears_without_bias,\n",
    "                weight_modes=[\"scalar\"] * num_components,\n",
    "                weight_values=weight_values,\n",
    "                bias_modes=[\"scalar\"] * num_components,  # This won't be used, but we still need to provide it\n",
    "                bias_values=[None] * num_components, # Indicate no bias masks\n",
    "            )\n",
    "            if True:\n",
    "                outs_weights_only = [torch.matmul(x, w_mask * lin.weight.T) \n",
    "                                     for w_mask, lin in zip(weight_values, linears_without_bias)]\n",
    "                expected_output = sum([o for o in outs_weights_only])\n",
    "                actual_output = weights_with_masks_no_bias(x)\n",
    "    \n",
    "                torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "                \n",
    "            # Test with merged weights\n",
    "            if True:\n",
    "                merged_linear = merge_linears(linears_without_bias, weight_factors=weight_values, bias_factors=bias_values)\n",
    "                expected_output = merged_linear(x)\n",
    "                actual_output = weights_with_masks_no_bias(x)\n",
    "                torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "        \n",
    "        logging.info(f\"Test with {num_components} Linear components passed (both with and without bias)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8b6533-bdcf-412d-ab1a-301688f45fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 12:14:17,180 - INFO - LinearsWithMasks Tests\n",
      "/tmp/ipykernel_2640588/1221996431.py:25: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  outs_biases_only = [b_mask * lin.bias.T for b_mask, lin in zip(bias_values, linears_with_bias)]\n",
      "2024-12-14 12:14:17,681 - INFO - Test with 2 Linear components passed (both with and without bias)!\n",
      "2024-12-14 12:14:18,519 - INFO - Test with 4 Linear components passed (both with and without bias)!\n",
      "2024-12-14 12:14:19,565 - INFO - Test with 5 Linear components passed (both with and without bias)!\n",
      "2024-12-14 12:14:21,147 - INFO - Test with 8 Linear components passed (both with and without bias)!\n",
      "2024-12-14 12:14:23,084 - INFO - Test with 10 Linear components passed (both with and without bias)!\n",
      "2024-12-14 12:14:25,308 - INFO - Test with 12 Linear components passed (both with and without bias)!\n",
      "2024-12-14 12:14:28,258 - INFO - Test with 16 Linear components passed (both with and without bias)!\n",
      "2024-12-14 12:14:31,902 - INFO - Test with 20 Linear components passed (both with and without bias)!\n",
      "2024-12-14 12:14:37,421 - INFO - Test with 30 Linear components passed (both with and without bias)!\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "num_components_list = [2, 4, 5, 8, 10, 12, 16, 20, 30]\n",
    "# Linear Test ----------------------------------------------------------------- #\n",
    "logging.info(\"LinearsWithMasks Tests\")\n",
    "input_size = 1024\n",
    "output_size = 1024\n",
    "test_multiple_linear_components(input_size, output_size, num_components_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a26c7c4-d753-4dc0-82a4-e706c91f5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_masks(target_module, ref_modules, factors):\n",
    "    \"\"\"\n",
    "    Recursively replaces normal components with masked components.\n",
    "    \n",
    "    Args:\n",
    "      module: The module in which to replace layers.\n",
    "    \"\"\"\n",
    "    assert len(ref_modules) == len(factors)\n",
    "    num_components = len(ref_modules)\n",
    "    for name, target_child in target_module.named_children():\n",
    "        ref_children = [getattr(module, name) for module in ref_modules]\n",
    "        modes = [\"scalar\" for _ in ref_children]\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            new_module = LinearsWithMasks(\n",
    "                linears=ref_children,\n",
    "                weight_modes=[\"scalar\"] * num_components,\n",
    "                weight_values=factors,\n",
    "                bias_modes=[\"scalar\"] * num_components,\n",
    "                bias_values=factors,\n",
    "            )\n",
    "            setattr(target_module, name, new_module)\n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            setattr(target_module, name, EmbeddingsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "        elif type(target_child).__name__ == Qwen2RMSNorm.__name__:\n",
    "            print(\"Hehe placing masks to a cutie RMSNorm\")\n",
    "            setattr(target_module, name, RMSNormsWithMasks(\n",
    "                ref_children, modes, factors\n",
    "            ))\n",
    "        else:\n",
    "            place_masks(target_child, ref_children, factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5771e2-cc4e-450c-9c39-3f1986700780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_mlp_components(config, num_components_list: List[int]):\n",
    "    with torch.no_grad():\n",
    "        device=\"cuda:0\"\n",
    "        for num_components in num_components_list:\n",
    "            # Test with bias\n",
    "            some_mlps = [Qwen2MLP(config).to(device=device) for _ in range(num_components)]\n",
    "            x = torch.rand(1, config.hidden_size).to(device=device)\n",
    "            for _ in range(3):\n",
    "                masked_mlp = Qwen2MLP(config).to(device=device)\n",
    "                factors = np.random.rand(num_components).tolist()\n",
    "                place_masks(masked_mlp, some_mlps, factors)\n",
    "                merged_mlp = merge_linears(some_mlps, weight_factors=factors, bias_factors=factors)\n",
    "                torch.testing.assert_close(masked_mlp(x), merged_mlp(x), rtol=1e-7, atol=1e-7)\n",
    "            logging.info(f\"Test with {num_components} MLP components passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b6203c8-170d-4342-b7df-25dc0d593c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "981df2a3-2fe8-483c-8b63-1a80567ab94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 12:16:25,865 - INFO - Test with 2 MLP components passed!\n",
      "2024-12-14 12:16:33,842 - INFO - Test with 3 MLP components passed!\n",
      "2024-12-14 12:16:46,099 - INFO - Test with 5 MLP components passed!\n",
      "2024-12-14 12:17:09,057 - INFO - Test with 10 MLP components passed!\n"
     ]
    }
   ],
   "source": [
    "test_multiple_mlp_components(config, num_components_list=[2, 3, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d68229-a6a9-4648-8966-be4ee1f7806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormWithMask(nn.Module):\n",
    "    def __init__(self, rms_norm: Qwen2RMSNorm, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.rms_norm = rms_norm\n",
    "        self.mask_config = mask_config\n",
    "        if rms_norm.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with RMSNorm, reinitializing...\")\n",
    "        self.mask_config.size = rms_norm.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        masked_weight = self.mask(self.rms_norm.weight)\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.rms_norm.variance_epsilon)\n",
    "        return masked_weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RMSNormsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rms_norms: List[Qwen2RMSNorm],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [rms_norm.weight.shape for rms_norm in rms_norms]\n",
    "        if values is None or len(values) != len(rms_norms):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with RMSNorm layers: {rms_norms}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_rms_norms = nn.ModuleList(\n",
    "            [RMSNormWithMask(rms_norm, mask_config)\n",
    "             for rms_norm, mask_config in zip(rms_norms, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        weights = [rms.mask(rms.rms_norm.weight) for rms in self.masked_rms_norms]\n",
    "        merged_weight = sum(weights)\n",
    "        variance_epsilon = self.masked_rms_norms[0].rms_norm.variance_epsilon\n",
    "        for rms in self.masked_rms_norms:\n",
    "            assert variance_epsilon == rms.rms_norm.variance_epsilon, \"Variance epsilon among models must be consistent\"\n",
    "        \n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "        return merged_weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c411bd64-b07b-4c67-b0aa-d83494bb36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_components = 2\n",
    "# hidden_size = 1024\n",
    "# rms_norms = [Qwen2RMSNorm(hidden_size) for _ in range(num_components)]\n",
    "# merged_norm = merge_linears(rms_norms, weight_factors=[0.5, 1.5], bias_factors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a0a1adf-4cc7-4e88-be49-4191f5fb93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_rmsnorm_components(hidden_size: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        rms_norms = [Qwen2RMSNorm(hidden_size) for _ in range(num_components)]\n",
    "        for rms_norm in rms_norms:\n",
    "            rms_norm.weight.data = torch.rand(hidden_size)\n",
    "            \n",
    "        hidden_states = torch.rand(2, 4, hidden_size)\n",
    "\n",
    "        for _ in range(10):\n",
    "            values = np.random.rand(num_components).tolist()\n",
    "            rms_norms_with_masks = RMSNormsWithMasks(\n",
    "                rms_norms=rms_norms, \n",
    "                modes=[\"scalar\"] * num_components, \n",
    "                values=values\n",
    "            )\n",
    "            merged_norm = merge_linears(rms_norms, weight_factors=values, bias_factors=None)\n",
    "            \n",
    "            expected_output = merged_norm(hidden_states)\n",
    "            actual_output = rms_norms_with_masks(hidden_states)\n",
    "            \n",
    "            torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "            \n",
    "        logging.info(f\"Test with {num_components} RMSNorm components passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "793d3ee2-0ab3-4161-88dc-9f6c29c4cedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 12:18:35,508 - INFO - Test with 2 RMSNorm components passed!\n",
      "2024-12-14 12:18:35,525 - INFO - Test with 3 RMSNorm components passed!\n",
      "2024-12-14 12:18:35,544 - INFO - Test with 5 RMSNorm components passed!\n",
      "2024-12-14 12:18:35,573 - INFO - Test with 10 RMSNorm components passed!\n",
      "2024-12-14 12:18:35,626 - INFO - Test with 15 RMSNorm components passed!\n",
      "2024-12-14 12:18:35,685 - INFO - Test with 20 RMSNorm components passed!\n",
      "2024-12-14 12:18:35,755 - INFO - Test with 30 RMSNorm components passed!\n",
      "2024-12-14 12:18:35,890 - INFO - Test with 50 RMSNorm components passed!\n"
     ]
    }
   ],
   "source": [
    "test_multiple_rmsnorm_components(2048, num_components_list = [2, 3, 5, 10, 15, 20, 30, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "64d07534-ddfe-41a9-8920-78ccac6ce630",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5d2e823-4b2f-42aa-a4e7-f572f6fd90ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da50524323147b0b0411f782882162d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a26ac8c1b94fe4adcbe8b786088494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    ")\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/workspace/models/Arcee-VyLinh/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc4179c4-bdf4-4361-94ff-28a1e063329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1 = model1.model.layers[0].self_attn\n",
    "attn2 = model2.model.layers[0].self_attn\n",
    "masked_attn = copy.deepcopy(attn1)\n",
    "factors = [0.3, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2a83250-82f3-497c-8103-722a6288943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_masks(masked_attn, ref_modules=[attn1, attn2], factors=factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4112ec86-2506-4c30-82ba-4ab4a6c48644",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_attn = merge_linears([attn1, attn2], weight_factors=factors, bias_factors=factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "612e388a-9fcc-41a5-bef6-90380d111847",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "hidden_size = 2048\n",
    "seq_len = 16\n",
    "batch_size = 2 \n",
    "position_ids = torch.tensor(\n",
    "    [[i for i in range(seq_len)] for _ in range(batch_size)]\n",
    ")\n",
    "hidden_states = torch.rand(batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e7b43fc-5708-4256-9eb1-9922fe5410e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn1(hidden_states, position_ids=position_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba79ce01-7172-4a89-84ab-f32199a10bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n",
      "2024-12-14 12:20:02,759 - INFO - FUCK YEAH.\n"
     ]
    }
   ],
   "source": [
    "o_masked = masked_attn(hidden_states, position_ids=position_ids)[0]\n",
    "o_merged = merged_attn(hidden_states, position_ids=position_ids)[0]\n",
    "torch.testing.assert_close(o_masked, o_merged, rtol=1e-7, atol=1e-7)\n",
    "logging.info(\"FUCK YEAH.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5bde41e-8e89-4e29-b8ef-025533413cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1 = model1.model.layers[0]\n",
    "decoder2 = model2.model.layers[1]\n",
    "masked_decoder = copy.deepcopy(decoder1)\n",
    "factors = [0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c01b0d88-4803-4aa0-86f2-187ad67c8a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hehe placing masks to a cutie RMSNorm\n",
      "Hehe placing masks to a cutie RMSNorm\n"
     ]
    }
   ],
   "source": [
    "place_masks(masked_decoder, ref_modules=[decoder1, decoder2], factors=factors)\n",
    "merged_decoder = merge_linears([decoder1, decoder2], weight_factors=factors, bias_factors=factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a15714a-c778-4dc3-9c69-4a815e66c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "hidden_size = 2048\n",
    "seq_len = 16\n",
    "batch_size = 2 \n",
    "position_ids = torch.tensor(\n",
    "    [[i for i in range(seq_len)] for _ in range(batch_size)]\n",
    ")\n",
    "hidden_states = torch.rand(batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72280ffd-7ddc-40fc-b264-3dbdf9367803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 12:20:40,892 - INFO - FUCK YEAH.\n"
     ]
    }
   ],
   "source": [
    "o_masked = masked_decoder(hidden_states, position_ids=position_ids)[0]\n",
    "o_merged = merged_decoder(hidden_states, position_ids=position_ids)[0]\n",
    "torch.testing.assert_close(o_masked, o_merged, rtol=1e-7, atol=1e-7)\n",
    "logging.info(\"FUCK YEAH.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f83c22-7046-4623-b593-bcb4e2e456a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_decoder1 = model1.model.layers[:4]\n",
    "stacked_decoder2 = model2.model.layers[:4]\n",
    "masked_stacked_decoder = copy.deepcopy(stacked_decoder1)\n",
    "factors = [0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f5a0a9d-04a9-4b41-bddf-7c87a75fa6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hehe placing masks to a cutie RMSNorm\n",
      "Hehe placing masks to a cutie RMSNorm\n",
      "Hehe placing masks to a cutie RMSNorm\n",
      "Hehe placing masks to a cutie RMSNorm\n",
      "Hehe placing masks to a cutie RMSNorm\n",
      "Hehe placing masks to a cutie RMSNorm\n",
      "Hehe placing masks to a cutie RMSNorm\n",
      "Hehe placing masks to a cutie RMSNorm\n"
     ]
    }
   ],
   "source": [
    "place_masks(masked_stacked_decoder, ref_modules=[stacked_decoder1, stacked_decoder2], factors=factors)\n",
    "merged_stacked_decoder = merge_linears([stacked_decoder1, stacked_decoder2], weight_factors=factors, bias_factors=factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb91d35b-c45c-4441-9a8d-d1bf7a415834",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "hidden_size = 2048\n",
    "seq_len = 16\n",
    "batch_size = 2 \n",
    "position_ids = torch.tensor(\n",
    "    [[i for i in range(seq_len)] for _ in range(batch_size)]\n",
    ")\n",
    "hidden_states = torch.rand(batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98225351-9fad-4a1a-9f7d-4fd561190ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 14:47:03,183 - INFO - FUCK YEAH.\n"
     ]
    }
   ],
   "source": [
    "o_masked = copy.deepcopy(hidden_states)\n",
    "o_merged = copy.deepcopy(hidden_states)\n",
    "for d in masked_stacked_decoder:\n",
    "    o_masked = d(o_masked, position_ids=position_ids)[0]\n",
    "for d in merged_stacked_decoder:\n",
    "    o_merged = d(o_merged, position_ids=position_ids)[0]\n",
    "torch.testing.assert_close(o_masked, o_merged, rtol=1e-8, atol=1e-8)\n",
    "logging.info(\"FUCK YEAH.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47b02b48-93d8-48eb-9aa9-4f0212238275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithMask(nn.Module):\n",
    "    def __init__(self, embedding: nn.Embedding, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.mask_config = mask_config\n",
    "        if embedding.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with Embedding, reinitializing...\")\n",
    "        self.mask_config.size = embedding.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        masked_weight = self.mask(self.embedding.weight)\n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            masked_weight,\n",
    "            padding_idx=self.embedding.padding_idx,\n",
    "            max_norm=self.embedding.max_norm,\n",
    "            norm_type=self.embedding.norm_type,\n",
    "            scale_grad_by_freq=self.embedding.scale_grad_by_freq,\n",
    "            sparse=self.embedding.sparse,\n",
    "        )\n",
    "\n",
    "class EmbeddingsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [embedding.weight.shape for embedding in embeddings]\n",
    "        if values is None or len(values) != len(embeddings):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with Embedding layers: {embeddings}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_embeddings = nn.ModuleList(\n",
    "            [EmbeddingWithMask(embedding, mask_config)\n",
    "             for embedding, mask_config in zip(embeddings, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        weights = [emb.mask(emb.embedding.weight) for emb in self.masked_embeddings]\n",
    "        merged_weight = sum(weights)\n",
    "        an_embedding = self.masked_embeddings[0].embedding\n",
    "        for other in self.masked_embeddings:\n",
    "            other_embedding = other.embedding\n",
    "            assert an_embedding.padding_idx == other_embedding.padding_idx\n",
    "            assert an_embedding.max_norm == other_embedding.max_norm\n",
    "            assert an_embedding.norm_type == other_embedding.norm_type\n",
    "            assert an_embedding.scale_grad_by_freq == other_embedding.scale_grad_by_freq\n",
    "            assert an_embedding.sparse == other_embedding.sparse\n",
    "            \n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            merged_weight,\n",
    "            padding_idx=an_embedding.padding_idx,\n",
    "            max_norm=an_embedding.max_norm,\n",
    "            norm_type=an_embedding.norm_type,\n",
    "            scale_grad_by_freq=an_embedding.scale_grad_by_freq,\n",
    "            sparse=an_embedding.sparse,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0877f6e8-d5d0-44ce-a088-9496662e29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_embedding_components(num_embeddings: int, embedding_dim: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        embeddings = [nn.Embedding(num_embeddings, embedding_dim) for _ in range(num_components)]\n",
    "        input_ids = torch.randint(0, num_embeddings, (2, 5))  # Example input_ids\n",
    "\n",
    "        for _ in range(10):\n",
    "            values = np.random.rand(num_components).tolist()\n",
    "            embeddings_with_masks = EmbeddingsWithMasks(embeddings=embeddings, modes=[\"scalar\"] * num_components, values=values)\n",
    "            merged_embedding = merge_linears(embeddings, weight_factors=values, bias_factors=None)\n",
    "            expected_output = merged_embedding(input_ids)\n",
    "            # individual_outputs = [embedding(input_ids) for embedding in embeddings]\n",
    "            # expected_output = sum(val * out for val, out in zip(values, individual_outputs))\n",
    "            actual_output = embeddings_with_masks(input_ids)\n",
    "\n",
    "            torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "        logging.info(f\"Test with {num_components} Embedding components passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a18aad9a-2cb2-4a48-b214-b7868629f589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 08:54:58,367 - INFO - Test with 1 Embedding components passed!\n",
      "2024-12-15 08:54:59,607 - INFO - Test with 2 Embedding components passed!\n",
      "2024-12-15 08:55:01,647 - INFO - Test with 3 Embedding components passed!\n",
      "2024-12-15 08:55:04,557 - INFO - Test with 4 Embedding components passed!\n",
      "2024-12-15 08:55:07,995 - INFO - Test with 5 Embedding components passed!\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define parameters for Embedding\n",
    "num_embeddings = 4096\n",
    "embedding_dim = 2048\n",
    "\n",
    "# Run tests for EmbeddingsWithMasks\n",
    "test_multiple_embedding_components(num_embeddings, embedding_dim, [i + 1 for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cadb3726-2c72-45b6-8422-399874a39130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7caa29bd-6e4a-4550-8f98-61fce4cc6ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.num_models = len(merge_config.model_paths)\n",
    "        self.configs = [\n",
    "            AutoConfig.from_pretrained(path) \n",
    "            for path in merge_config.model_paths\n",
    "        ]\n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.models = nn.ModuleList([\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                merge_config.model_paths[i], \n",
    "                config=self.configs[i],\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ) \n",
    "            for i in range(self.num_models)\n",
    "        ])\n",
    "        # self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config, factors):\n",
    "        # dummy_config = copy.deepcopy(self.configs[0])\n",
    "        # dummy_config.update({\"hidden_size\": 1, \"intermediate_size\": 1})\n",
    "        # self.merger = AutoModelForCausalLM.from_config(dummy_config)\n",
    "        self.merger = copy.deepcopy(self.models[0])\n",
    "        place_masks(self.merger, self.models, factors=factors)\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "812e5fcb-6523-4062-80da-850fd1739adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "285bee3f-537e-4b22-9f55-0de9463e19e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b071ba952a2342159057bf8891884e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f093a95b7a54f9384c2463ce88322f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "155c315a-b9fd-4ce9-97eb-1dedb1fecf14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Merger(\n",
       "  (models): ModuleList(\n",
       "    (0-1): 2 x Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (merger): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): EmbeddingsWithMasks(\n",
       "        (masked_embeddings): ModuleList(\n",
       "          (0-1): 2 x EmbeddingWithMask(\n",
       "            (embedding): Embedding(151936, 2048)\n",
       "            (mask): Mask()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  (weight_mask): Mask()\n",
       "                  (bias_mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (k_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                  (weight_mask): Mask()\n",
       "                  (bias_mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (v_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                  (weight_mask): Mask()\n",
       "                  (bias_mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (o_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (weight_mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                  (weight_mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (up_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                  (weight_mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (down_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "                  (weight_mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): RMSNormsWithMasks(\n",
       "            (masked_rms_norms): ModuleList(\n",
       "              (0-1): 2 x RMSNormWithMask(\n",
       "                (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                (mask): Mask()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNormsWithMasks(\n",
       "            (masked_rms_norms): ModuleList(\n",
       "              (0-1): 2 x RMSNormWithMask(\n",
       "                (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                (mask): Mask()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNormsWithMasks(\n",
       "        (masked_rms_norms): ModuleList(\n",
       "          (0-1): 2 x RMSNormWithMask(\n",
       "            (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (mask): Mask()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): LinearsWithMasks(\n",
       "      (masked_linears): ModuleList(\n",
       "        (0-1): 2 x LinearWithMask(\n",
       "          (linear): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "          (weight_mask): Mask()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba4cf17f-e4a5-4d65-906f-26d4979e849b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 20.94 MiB is free. Process 89524 has 23.63 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 515.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__post_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 28\u001b[0m, in \u001b[0;36mMerger.__post_init__\u001b[0;34m(self, merge_config, factors)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, merge_config, factors):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# dummy_config = copy.deepcopy(self.configs[0])\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# dummy_config.update({\"hidden_size\": 1, \"intermediate_size\": 1})\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# self.merger = AutoModelForCausalLM.from_config(dummy_config)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerger \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     place_masks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, factors\u001b[38;5;241m=\u001b[39mfactors)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "    \u001b[0;31m[... skipping similar frames: _deepcopy_dict at line 231 (8 times), deepcopy at line 146 (8 times), _reconstruct at line 271 (3 times), deepcopy at line 172 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "    \u001b[0;31m[... skipping similar frames: _deepcopy_dict at line 231 (1 times), deepcopy at line 146 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/parameter.py:68\u001b[0m, in \u001b[0;36mParameter.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\n\u001b[0;32m---> 68\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 20.94 MiB is free. Process 89524 has 23.63 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 515.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "merger.__post_init__(merge_config, factors=[1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c5378-6fac-4321-95ec-bfb0fb05841c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47b50548-fa64-4c40-8dc8-f17888c2c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, model, tokenizer, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()\n",
    "\n",
    "def get_logits(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_ids).logits\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52d5a4f6-a631-44ee-bc76-51382b3e4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d50a341-f936-4212-a658-ba401aad14f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nContinue this text: A dog is a cat<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "prompt = \"Continue this text: A dog is a cat\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c4d0e05-091f-4e56-a671-1e7672f1bfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dog is indeed a type of animal, specifically a mammal belonging to the Canidae family. Dogs have been domesticated for thousands of years and serve various purposes such as companionship, hunting, herding, protection, and assistance in tasks like guide dogs or service animals.\n",
      "\n",
      "Dogs come in different breeds with varying physical characteristics, sizes, colors, and temperaments. Some common types include:\n",
      "\n",
      "1. Labrador Retrievers - known for their friendly nature, intelligence, and love of water.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.merger, tokenizer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be7edaeb-3640-424a-8fcb-62277004df00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dog is indeed a type of animal, specifically a mammal belonging to the Canidae family. Dogs have been domesticated for thousands of years and serve various purposes such as companionship, hunting, herding, protection, and assistance in tasks like guide dogs or service animals.\n",
      "\n",
      "Dogs come in different breeds with varying physical characteristics, sizes, colors, and temperaments. Some common types include:\n",
      "\n",
      "1. Labrador Retrievers - Known for their friendly nature, intelligence, and love for water.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = generate(text, merger.models[1], tokenizer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c80cab1-1aa0-4803-b017-101b6f5f6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_merged = get_logits(text, merger.merger, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0944ba7-3c8d-4a81-8f4e-7f2cad22622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1 = get_logits(text, merger.models[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "091d2cf4-e582-4882-9930-8c2610332624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 8.3750,  6.9062,  3.3906,  ..., -0.7344, -0.7344, -0.7344],\n",
       "          [ 6.1875,  6.9375,  7.0938,  ..., -1.4062, -1.4062, -1.4062],\n",
       "          [ 5.8125,  8.7500, 11.2500,  ..., -3.9688, -3.9688, -3.9688],\n",
       "          ...,\n",
       "          [ 1.1719,  5.5938, -3.7656,  ..., -0.3691, -0.3691, -0.3691],\n",
       "          [ 9.3750,  7.5312,  8.2500,  ..., -6.3125, -6.3125, -6.3125],\n",
       "          [16.2500, 18.0000,  9.2500,  ..., -1.8906, -1.8906, -1.8906]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[ 8.0625,  6.9688,  3.6094,  ..., -0.8320, -0.8320, -0.8320],\n",
       "          [ 6.0312,  6.9688,  7.0938,  ..., -1.4219, -1.4219, -1.4219],\n",
       "          [ 5.8438,  8.7500, 11.1875,  ..., -3.9844, -3.9844, -3.9844],\n",
       "          ...,\n",
       "          [ 1.4141,  5.7500, -3.6562,  ..., -0.3164, -0.3164, -0.3164],\n",
       "          [ 9.3750,  7.2500,  8.1250,  ..., -6.0312, -6.0312, -6.0312],\n",
       "          [16.2500, 17.8750,  9.1875,  ..., -1.8438, -1.8438, -1.8438]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_merged, logits1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a99162-9165-44ea-8dff-f25b86ee1dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
