{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754cc9c2-a335-4612-9409-ab21f754ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = \"\"\"\n",
    "1. prepare data\n",
    "2. define model\n",
    "    - model a, mask a\n",
    "    - model b, mask b\n",
    "make only masks trainable.\n",
    "make sure it has correct inference.\n",
    "3. define loss function in Trainer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4884f-b796-4384-a2ad-177f2314c3e2",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6baca-8a9b-4a41-8e32-01d6987ae939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "332cf659-c413-4f11-889a-79532efcdd97",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6167e22d-21c5-4027-af0b-a029c8acfe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from modeling_qwen2 import (\n",
    "    Qwen2RMSNorm, \n",
    "    Qwen2RotaryEmbedding, \n",
    "    Qwen2MLP, \n",
    "    Qwen2Attention, \n",
    "    Qwen2FlashAttention2, \n",
    "    Qwen2SdpaAttention, \n",
    "    Qwen2DecoderLayer, \n",
    "    Qwen2PreTrainedModel, \n",
    "    Qwen2Model, \n",
    "    Qwen2ForCausalLM\n",
    ")\n",
    "\n",
    "from configuration_qwen2 import Qwen2Config\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c823e218-fb6e-4d99-9ff5-17f9e794a79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 07:19:24,581 - INFO - Comparing tokenizer at /workspace/models/Arcee-VyLinh/ with tokenizer at /workspace/models/Qwen2.5-Coder-3B/\n",
      "2024-12-14 07:19:24,585 - INFO - Tokenizer at /workspace/models/Arcee-VyLinh/ and /workspace/models/Qwen2.5-Coder-3B/ are the same based on the defined criteria\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import are_tokenizers_same\n",
    "are_tokenizers_same(\n",
    "    paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc9a79e-4ec9-493e-a3da-44bb18c6ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layer(path, layer_idx=33):\n",
    "\tstate_dict = {}\n",
    "\tshard_paths = [f for f in os.listdir(path) if f.endswith('.safetensors')]\n",
    "\tfor shard_path in sorted(shard_paths, key=lambda x: int(x.split('-')[1])):\n",
    "\t\tapath = os.path.join(path, shard_path)\n",
    "\t\twith safe_open(apath, framework=\"pt\", device=\"cpu\") as f:\n",
    "\t\t\tfor key in f.keys():\n",
    "\t\t\t\tif f\"layers.{str(layer_idx)}.\" in key:\n",
    "\t\t\t\t\tstate_dict[key] = f.get_tensor(key)\n",
    "\treturn state_dict\n",
    "\n",
    "def strip_prefix(state_dict, prefix=\"model.layers.\"):\n",
    "    \"\"\"Strips 'model.layers.*.' prefix from 'input_layernorm.weight' keys.\"\"\"\n",
    "    return {\n",
    "      k.replace(f\"{prefix}{k.split('.')[2]}.\", \"\") if k.startswith(prefix)\n",
    "      else k: v for k, v in state_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d4bdc5b-af91-4b40-b38c-64a50d49281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(\n",
    "    t: float, v0: Union[np.ndarray, torch.Tensor], v1: Union[np.ndarray, torch.Tensor]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    return (1 - t) * v0 + t * v1\n",
    "\n",
    "def weighted_sum(\n",
    "    factors: List[float], \n",
    "    tensors: Union[List[np.ndarray], List[torch.Tensor]]\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    result = 0.0\n",
    "    for factor, tensor in zip(factors, tensors):\n",
    "        result += factor * tensor\n",
    "    return result\n",
    "\n",
    "def merge_modules(modules, factors):\n",
    "    \"\"\"\n",
    "    This is only applicable for cases where a static set of scalars\n",
    "    playing as merging factor for every submodules of the passed module.\n",
    "    Not recommend for fine-grained usecases.\n",
    "    \"\"\"\n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out\n",
    "\n",
    "def merge_linears(modules, weight_factors, bias_factors):\n",
    "    param_names = sorted([name for name, _ in modules[0].named_parameters()])\n",
    "    for module in modules:\n",
    "        other_param_names = sorted([name for name, _ in module.named_parameters()])\n",
    "        assert param_names == other_param_names, \"Mismatch tensor names.\"\n",
    "        \n",
    "    module_out = copy.deepcopy(modules[0])\n",
    "    out_dict = module_out.state_dict()\n",
    "    \n",
    "    tensor_dicts_list = [m.state_dict() for m in modules]\n",
    "    tensor_names = [key for key in tensor_dicts_list[0].keys()]\n",
    "    \n",
    "    for tensor_name in tensor_names:\n",
    "        tensors_list = [tensor_dicts_list[i][tensor_name]\n",
    "                       for i in range(len(modules))]\n",
    "        if \"weight\" in tensor_name:\n",
    "            factors = weight_factors\n",
    "        elif \"bias\" in tensor_name:\n",
    "            factors = bias_factors\n",
    "        else:\n",
    "            raise ValueError(\"Hey this tensor is neither weight or bias.\")\n",
    "            \n",
    "        tensor_computed = (\n",
    "            weighted_sum(\n",
    "                factors=factors,\n",
    "                tensors=tensors_list\n",
    "            )\n",
    "            .to(tensors_list[0].dtype)\n",
    "            .to(tensors_list[0].device)\n",
    "        )\n",
    "        out_dict[tensor_name] = tensor_computed\n",
    "    module_out.load_state_dict(out_dict)\n",
    "    return module_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4019050e-16d9-4094-bf6c-e063a8b3f1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weight']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name, _ in lin.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b59dc3-83fc-4345-b4f6-5cf7665fa607",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fd625a-64ce-4a36-b16d-1b6e61eaa5c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8256a2-89d0-4cfa-8726-1ac197a04470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc68586-edfa-465f-a1e8-513541ccac5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/models/Arcee-VyLinh/', '/workspace/models/Qwen2.5-Coder-3B/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config.model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eca4d1f-96da-4126-bc64-ed531f46d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.model_paths[0]\n",
    "        )\n",
    "        \n",
    "        self.models = nn.ModuleList([\n",
    "            Qwen2ForCausalLM.from_pretrained(\n",
    "            # AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                # torch_dtype=torch.bfloat16,\n",
    "                # device_map={\"\":0}\n",
    "            ) for model_path in config.model_paths\n",
    "        ])\n",
    "        self.__post_init__()\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        # self.masks = torch.nn\n",
    "        pass\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        \"\"\"\n",
    "        activations = []\n",
    "        for i in range(num_layers):\n",
    "            L1 = models[0].layers[i]\n",
    "            L2 = models[1].layers[i]\n",
    "            Lm = alpha * L1 + beta * L2\n",
    "            h1 = L1(h)\n",
    "            h2 = L2(h)\n",
    "            h = Lm(h)\n",
    "            activations.append({\n",
    "                \"1\": h1, \"2\": h2, \"merged\": copy(h)\n",
    "            })\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        - embed_tokens\n",
    "        - norm\n",
    "        - layers\n",
    "            - input_layernorm\n",
    "            - self_attn\n",
    "            - mlp\n",
    "            - post_attention_norm\n",
    "        - lm_head\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd988e00-b69f-4325-bb22-220a26ef033f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336da23a9c3f412085476e73089dc5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7626fe82064cfd822b91b96860eb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57ee117-eee6-4233-adfb-58e8b61cd7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.models[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4912f19f-b0cc-49b1-8a2e-14006fc6f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1 = merger.models[0].model.layers[0].self_attn\n",
    "attn2 = merger.models[1].model.layers[0].self_attn\n",
    "mlp1 = merger.models[0].model.layers[0].mlp\n",
    "mlp2 = merger.models[1].model.layers[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1116d86-d228-4aed-b6ac-1098c044ea62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0159, -0.0432, -0.0080,  ...,  0.0081,  0.0096,  0.0132],\n",
       "        [-0.0330,  0.0110,  0.0085,  ...,  0.0226, -0.0082,  0.0457],\n",
       "        [-0.0092,  0.0111, -0.0134,  ...,  0.0298,  0.0113, -0.0038],\n",
       "        ...,\n",
       "        [-0.0085,  0.0601, -0.0325,  ...,  0.0525, -0.0222,  0.0403],\n",
       "        [-0.0374, -0.0325,  0.0620,  ..., -0.0206,  0.0806,  0.0376],\n",
       "        [ 0.0356,  0.0151,  0.0087,  ..., -0.0306, -0.0072,  0.0378]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn1.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ceff9f2-8598-4fa4-99e4-f584ec495af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from typing import List, Dict\n",
    "\n",
    "def merge_linear(weights: List[nn.Linear], factors: List[float]) -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Merges multiple linear layers by taking a weighted average of their weights and biases.\n",
    "\n",
    "    Args:\n",
    "        weights: A list of nn.Linear layers to merge.\n",
    "        factors: A list of scaling factors corresponding to each layer in 'weights'.\n",
    "\n",
    "    Returns:\n",
    "        A new nn.Linear layer that is the weighted average of the input layers.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of weights and factors don't match, or if the\n",
    "                    layers have incompatible dimensions, device or dtype.\n",
    "    \"\"\"\n",
    "    if len(weights) != len(factors):\n",
    "        raise ValueError(\"The number of weights and factors must be equal.\")\n",
    "\n",
    "    # Check for compatibility, device, and dtype\n",
    "    device = weights[0].weight.device\n",
    "    dtype = weights[0].weight.dtype\n",
    "    if not all(\n",
    "        w.in_features == weights[0].in_features\n",
    "        and w.out_features == weights[0].out_features\n",
    "        and w.weight.device == device\n",
    "        and w.weight.dtype == dtype\n",
    "        for w in weights\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Incompatible linear layers for merging. They must have the same in_features, out_features, device, and dtype.\"\n",
    "        )\n",
    "\n",
    "    # Create a new linear layer with the same dimensions, device and dtype\n",
    "    merged_linear = nn.Linear(\n",
    "        in_features=weights[0].in_features,\n",
    "        out_features=weights[0].out_features,\n",
    "        bias=False,\n",
    "        device=device,\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "    # Calculate the merged weight and bias\n",
    "    merged_weight = torch.zeros_like(weights[0].weight)\n",
    "    merged_bias = (\n",
    "        torch.zeros_like(weights[0].bias, device=device, dtype=dtype)\n",
    "        if weights[0].bias is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        merged_weight += factors[i] * w.weight\n",
    "        if w.bias is not None:\n",
    "            if merged_bias is None:\n",
    "                raise ValueError(\"Cannot merge linear layers if only some have biases.\")\n",
    "            merged_bias += factors[i] * w.bias\n",
    "\n",
    "    # Assign the merged weight and bias to the new layer\n",
    "    with torch.no_grad():\n",
    "        merged_linear.weight.copy_(merged_weight)\n",
    "        if merged_bias is not None:\n",
    "            merged_linear.bias = nn.Parameter(merged_bias)\n",
    "\n",
    "    return merged_linear\n",
    "\n",
    "def merge_module_recursive(\n",
    "    target_module: nn.Module, modules_dict: Dict[str, List[nn.Module]], factors: List[float]\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively merges multiple modules by taking a weighted average of their Linear layer weights and biases.\n",
    "\n",
    "    Args:\n",
    "        target_module: The target module where the merged weights will be stored.\n",
    "        modules_dict: A dictionary where keys are module names and values are lists of modules to merge.\n",
    "        factors: A list of scaling factors corresponding to each list of modules.\n",
    "    \"\"\"\n",
    "\n",
    "    for name, module in target_module.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if name not in modules_dict:\n",
    "                raise ValueError(\n",
    "                    f\"Missing module {name} in modules_dict. Make sure all linear layer weights are provided\"\n",
    "                )\n",
    "            merged_linear = merge_linear(modules_dict[name], factors)\n",
    "            # Find the parent module\n",
    "            parent_module_name = \".\".join(name.split(\".\")[:-1])\n",
    "            layer_name = name.split(\".\")[-1]\n",
    "\n",
    "            if parent_module_name:\n",
    "                parent_module = target_module.get_submodule(parent_module_name)\n",
    "            else:\n",
    "                parent_module = target_module\n",
    "\n",
    "            # Replace the original Linear layer with merged one\n",
    "            setattr(parent_module, layer_name, merged_linear)\n",
    "\n",
    "def merge_modules(modules: List[nn.Module], factors: List[float]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Merges multiple modules by taking a weighted average of their Linear layer weights and biases.\n",
    "    The merged weights are stored into a deepcopy of the first module in the list.\n",
    "\n",
    "    Args:\n",
    "        modules: A list of nn.Modules to merge.\n",
    "        factors: A list of scaling factors corresponding to each module in 'modules'.\n",
    "\n",
    "    Returns:\n",
    "        A new nn.Module that is the weighted average of the input modules.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of modules and factors don't match.\n",
    "    \"\"\"\n",
    "    if len(modules) != len(factors):\n",
    "        raise ValueError(\"The number of modules and factors must be equal.\")\n",
    "\n",
    "    # Check device and dtype consistency across all modules\n",
    "    device = modules[0].parameters().__next__().device\n",
    "    dtype = modules[0].parameters().__next__().dtype\n",
    "    if not all(p.device == device and p.dtype == dtype for module in modules for p in module.parameters()):\n",
    "        raise ValueError(\"All modules must be on the same device and have the same dtype.\")\n",
    "\n",
    "    # Create a deep copy of the first module to store the merged weights\n",
    "    merged_module = copy.deepcopy(modules[0])\n",
    "\n",
    "    # Dictionary to hold corresponding linear layers from each module\n",
    "    modules_to_merge = {\n",
    "        name: [] for name, _ in merged_module.named_modules() if isinstance(_, nn.Linear)\n",
    "    }\n",
    "    for module in modules:\n",
    "        for name, layer in module.named_modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                modules_to_merge[name].append(layer)\n",
    "\n",
    "    # Merge the modules recursively\n",
    "    merge_module_recursive(merged_module, modules_to_merge, factors)\n",
    "\n",
    "    # Ensure the merged module has the correct device and dtype\n",
    "    merged_module.to(device=device, dtype=dtype)\n",
    "\n",
    "    return merged_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c6e197-7a64-4c61-ae4c-01ec81fc5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mlp = merge_modules(modules=[mlp1, mlp2], factors=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4f12e66-a7cf-40b4-b099-95d18d57aba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mQwen2MLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Define the computation performed at every call.\n",
       "\n",
       "Should be overridden by all subclasses.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within\n",
       "    this function, one should call the :class:`Module` instance afterwards\n",
       "    instead of this since the former takes care of running the\n",
       "    registered hooks while the latter silently ignores them.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /workspace/merge/modeling_qwen2.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Qwen2MLP.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc55cde6-7362-43bc-b36e-c0fd8b6371ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_merged_mlp(x: torch.Tensor, modules: List[nn.Module], factors: List[float]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a forward pass that simulates the behavior of a merged MLP module.\n",
    "\n",
    "    Args:\n",
    "        modules: A list of MLP modules (e.g., Qwen2MLP instances).\n",
    "        factors: A list of scaling factors corresponding to each module in 'modules'.\n",
    "        x: The input tensor.\n",
    "\n",
    "    Returns:\n",
    "        The output tensor after the forward pass.\n",
    "    \"\"\"\n",
    "    factors = torch.tensor(factors).to(x.device, dtype=x.dtype).view(-1, 1, 1, 1)  # Reshape factors for broadcasting\n",
    "    gate_output = torch.stack([m.gate_proj(x) for m in modules]).mul(factors).sum(0)\n",
    "    up_output = torch.stack([m.up_proj(x) for m in modules]).mul(factors).sum(0)\n",
    "    act_output = modules[0].act_fn(gate_output)  # Assuming all modules have the same activation function\n",
    "    result = torch.stack([m.down_proj(act_output * up_output) for m in modules]).mul(factors).sum(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3986afd-841a-4795-b774-2e9758ce1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = merged_mlp.parameters().__next__().device\n",
    "dtype = merged_mlp.parameters().__next__().dtype\n",
    "x = torch.rand(1, 4, 2048).to(device, dtype=dtype)\n",
    "o1 = merged_mlp(x)\n",
    "o2 = forward_merged_mlp(x, modules=[mlp1, mlp2], factors=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d65bd5d4-2ce1-4e0d-a655-254bc03fc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(o1, o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65e11c1a-3abb-43f9-81fa-8c25a0a045bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules[0].down_proj.weight\n",
    "gate_output.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "286a1532-4644-4142-ab2b-029b4d4fefd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3750, -0.3301, -0.0016,  ..., -0.0093, -0.2695,  0.0986],\n",
       "          [ 0.2930, -0.2949,  0.0933,  ...,  0.0483, -0.2949, -0.0649],\n",
       "          [ 0.2676, -0.3789,  0.1973,  ...,  0.0134, -0.7227,  0.1367],\n",
       "          [ 0.2715, -0.2988, -0.0547,  ..., -0.1777, -0.7695,  0.0850]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda:0\"\n",
    "h = torch.rand(1, 4, 2048, dtype=torch.bfloat16).to(device)\n",
    "p = torch.arange(4, dtype=torch.bfloat16, device=device).unsqueeze(0)\n",
    "attn1.forward(h, position_ids=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93147db6-84be-43db-9559-dd03c4c5988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, param_bits):\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    non_trainable_params = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        num_params = param.numel()  # Get the number of elements in the parameter\n",
    "        total_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "        else:\n",
    "            non_trainable_params += num_params\n",
    "\n",
    "    total_gigabytes = total_params * (param_bits / 8) / (1024**3)\n",
    "    memory = f\"{total_gigabytes:.2f} GB\"\n",
    "    \n",
    "    return total_params, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a75cc9e3-c80c-4a70-9d0c-b71da5fa1cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9439744, '0.02 GB'), (67633152, '0.13 GB'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(attn1, 16), count_parameters(mlp1, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59306b-6580-4484-a88a-0c39f30c8bb8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25550585-74ef-4ba2-9e4e-c6f203fbd053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.weight = nn.Parameter(torch.tensor(value)) # Corrected typo here\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        try:\n",
    "            self.weight * torch.rand(self.size)\n",
    "        except RuntimeError:\n",
    "            print(\"mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.weight * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ceee65-d173-4e63-958a-1d7cb423b5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskConfig {\n",
       "  \"mode\": \"scalar\",\n",
       "  \"size\": [\n",
       "    4,\n",
       "    8\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"value\": 0.5\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_config = MaskConfig(\n",
    "    mode=\"scalar\", value=0.5, size=torch.Size((4, 8))\n",
    ")\n",
    "mask_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c1c765-7d62-4000-bbcf-34072ab1947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithMask(nn.Module):\n",
    "    def __init__(self, linear, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.mask_config = mask_config\n",
    "        if linear.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not imcompatible with linear, reinitializing...\")\n",
    "        self.mask_config.size = linear.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        masked_linear = self.mask(self.linear.weight)\n",
    "        return nn.functional.linear(x, masked_linear, self.linear.bias)\n",
    "\n",
    "class LinearsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        linears: List[nn.Module], \n",
    "        modes: List[str] = [\"scalar\"], \n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [linear.weight.shape for linear in linears]\n",
    "        if values is None or len(values) != len(linears):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with linear layers: {linears}\")\n",
    "            \n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size) \n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_linears = nn.ModuleList(\n",
    "            [LinearWithMask(linear, mask_config) \n",
    "             for linear, mask_config in zip(linears, mask_configs)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = 0.0\n",
    "        for masked_linear in self.masked_linears:\n",
    "            output += masked_linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc42d400-1dc8-4413-96de-9b0fb0123c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with 1 Linear components passed!\n",
      "Test with 2 Linear components passed!\n",
      "Test with 3 Linear components passed!\n",
      "Test with 4 Linear components passed!\n",
      "Test with 5 Linear components passed!\n",
      "Test with 6 Linear components passed!\n",
      "Test with 7 Linear components passed!\n",
      "Test with 8 Linear components passed!\n",
      "Test with 9 Linear components passed!\n",
      "Test with 10 Linear components passed!\n",
      "Test with 11 Linear components passed!\n",
      "Test with 12 Linear components passed!\n",
      "Test with 13 Linear components passed!\n",
      "Test with 14 Linear components passed!\n",
      "Test with 15 Linear components passed!\n",
      "Test with 16 Linear components passed!\n",
      "Test with 17 Linear components passed!\n",
      "Test with 18 Linear components passed!\n",
      "Test with 19 Linear components passed!\n",
      "Test with 20 Linear components passed!\n"
     ]
    }
   ],
   "source": [
    "# --- Testing ---\n",
    "def test_multiple_linear_components(input_size: int, output_size: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        linears = [nn.Linear(input_size, output_size, bias=False) for _ in range(num_components)]\n",
    "        x = torch.rand(1, input_size)\n",
    "\n",
    "        for _ in range(10):  # Reduced number of iterations for faster testing in a notebook\n",
    "            values = np.random.rand(num_components).tolist() # cast to list\n",
    "            weights_with_masks = LinearsWithMasks(linears=linears, modes=[\"scalar\"] * num_components, values=values)\n",
    "\n",
    "            individual_outputs = [linear(x) for linear in linears]\n",
    "            expected_output = sum(val * out for val, out in zip(values, individual_outputs))\n",
    "            actual_output = weights_with_masks(x)\n",
    "\n",
    "            torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "        print(f\"Test with {num_components} Linear components passed!\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define input and output sizes\n",
    "input_size = 1024\n",
    "output_size = 1024\n",
    "\n",
    "# Run tests\n",
    "test_multiple_linear_components(input_size, output_size, [i + 1 for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5a66a8-d14b-43eb-84e3-de2fb0d4bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormWithMask(nn.Module):\n",
    "    def __init__(self, rms_norm: Qwen2RMSNorm, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.rms_norm = rms_norm\n",
    "        self.mask_config = mask_config\n",
    "        if rms_norm.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with RMSNorm, reinitializing...\")\n",
    "        self.mask_config.size = rms_norm.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        masked_weight = self.mask(self.rms_norm.weight)\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.rms_norm.variance_epsilon)\n",
    "        return masked_weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RMSNormsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rms_norms: List[Qwen2RMSNorm],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [rms_norm.weight.shape for rms_norm in rms_norms]\n",
    "        if values is None or len(values) != len(rms_norms):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with RMSNorm layers: {rms_norms}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_rms_norms = nn.ModuleList(\n",
    "            [RMSNormWithMask(rms_norm, mask_config)\n",
    "             for rms_norm, mask_config in zip(rms_norms, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        output = 0.0\n",
    "        for masked_rms_norm in self.masked_rms_norms:\n",
    "            output += masked_rms_norm(hidden_states)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cae51d3-6384-48ed-8046-83dc10ce428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with 1 RMSNorm components passed!\n",
      "Test with 2 RMSNorm components passed!\n",
      "Test with 3 RMSNorm components passed!\n",
      "Test with 4 RMSNorm components passed!\n",
      "Test with 5 RMSNorm components passed!\n"
     ]
    }
   ],
   "source": [
    "def test_multiple_rms_norm_components(hidden_size: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        rms_norms = [Qwen2RMSNorm(hidden_size) for _ in range(num_components)]\n",
    "        hidden_states = torch.rand(2, 4, hidden_size)\n",
    "\n",
    "        for _ in range(10):\n",
    "            values = np.random.rand(num_components).tolist()\n",
    "            rms_norms_with_masks = RMSNormsWithMasks(rms_norms=rms_norms, modes=[\"scalar\"] * num_components, values=values)\n",
    "\n",
    "            individual_outputs = [rms_norm(hidden_states) for rms_norm in rms_norms]\n",
    "            expected_output = sum(val * out for val, out in zip(values, individual_outputs))\n",
    "            actual_output = rms_norms_with_masks(hidden_states)\n",
    "            \n",
    "            torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "        print(f\"Test with {num_components} RMSNorm components passed!\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define input and output sizes\n",
    "hidden_size = 2048\n",
    "\n",
    "# Run tests for RMSNormsWithMasks\n",
    "test_multiple_rms_norm_components(hidden_size, [i+1 for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e5927a-b624-4649-b054-040dd0f54f34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingWithMask(nn.Module):\n",
    "    def __init__(self, embedding: nn.Embedding, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.mask_config = mask_config\n",
    "        if embedding.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with Embedding, reinitializing...\")\n",
    "        self.mask_config.size = embedding.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        masked_weight = self.mask(self.embedding.weight)\n",
    "        return nn.functional.embedding(\n",
    "            input_ids,\n",
    "            masked_weight,\n",
    "            padding_idx=self.embedding.padding_idx,\n",
    "            max_norm=self.embedding.max_norm,\n",
    "            norm_type=self.embedding.norm_type,\n",
    "            scale_grad_by_freq=self.embedding.scale_grad_by_freq,\n",
    "            sparse=self.embedding.sparse,\n",
    "        )\n",
    "\n",
    "class EmbeddingsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [embedding.weight.shape for embedding in embeddings]\n",
    "        if values is None or len(values) != len(embeddings):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with Embedding layers: {embeddings}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_embeddings = nn.ModuleList(\n",
    "            [EmbeddingWithMask(embedding, mask_config)\n",
    "             for embedding, mask_config in zip(embeddings, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        output = 0.0\n",
    "        for masked_embedding in self.masked_embeddings:\n",
    "            output += masked_embedding(input_ids)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b93dc2b3-7711-4eec-bd48-349de66498aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with 1 Embedding components passed!\n",
      "Test with 2 Embedding components passed!\n",
      "Test with 3 Embedding components passed!\n",
      "Test with 4 Embedding components passed!\n",
      "Test with 5 Embedding components passed!\n"
     ]
    }
   ],
   "source": [
    "def test_multiple_embedding_components(num_embeddings: int, embedding_dim: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        embeddings = [nn.Embedding(num_embeddings, embedding_dim) for _ in range(num_components)]\n",
    "        input_ids = torch.randint(0, num_embeddings, (2, 5))  # Example input_ids\n",
    "\n",
    "        for _ in range(10):\n",
    "            values = np.random.rand(num_components).tolist()\n",
    "            embeddings_with_masks = EmbeddingsWithMasks(embeddings=embeddings, modes=[\"scalar\"] * num_components, values=values)\n",
    "\n",
    "            individual_outputs = [embedding(input_ids) for embedding in embeddings]\n",
    "            expected_output = sum(val * out for val, out in zip(values, individual_outputs))\n",
    "            actual_output = embeddings_with_masks(input_ids)\n",
    "\n",
    "            torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "        print(f\"Test with {num_components} Embedding components passed!\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define parameters for Embedding\n",
    "num_embeddings = 2048\n",
    "embedding_dim = 2048\n",
    "\n",
    "# Run tests for EmbeddingsWithMasks\n",
    "test_multiple_embedding_components(num_embeddings, embedding_dim, [i + 1 for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf260def-3212-4747-9cd0-cdd07127b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_masks(target_module, ref_modules):\n",
    "    \"\"\"\n",
    "    Recursively replaces normal components with masked components.\n",
    "    \n",
    "    Args:\n",
    "      module: The module in which to replace layers.\n",
    "    \"\"\"\n",
    "    for name, target_child in target_module.named_children():\n",
    "        ref_children = [getattr(module, name) for module in ref_modules]\n",
    "        modes = [\"scalar\" for _ in ref_children]\n",
    "        values = [0.0 for _ in ref_children]\n",
    "        values[0] = 1.0\n",
    "        if isinstance(target_child, nn.Linear):\n",
    "            setattr(target_module, name, LinearsWithMasks(\n",
    "                ref_children, modes, values\n",
    "            ))\n",
    "        elif isinstance(target_child, nn.Embedding):\n",
    "            setattr(target_module, name, EmbeddingsWithMasks(\n",
    "                ref_children, modes, values\n",
    "            ))\n",
    "        elif type(target_child).__name__ == Qwen2RMSNorm.__name__:\n",
    "            setattr(target_module, name, RMSNormsWithMasks(\n",
    "                ref_children, modes, values\n",
    "            ))\n",
    "        else:\n",
    "            place_masks(target_child, ref_children)\n",
    "\n",
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class DecoderMerger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.configs = [Qwen2Config.from_pretrained(path) \n",
    "                        for path in merge_config.model_paths]\n",
    "        \n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.decoders = nn.ModuleList(\n",
    "            Qwen2DecoderLayer(config, layer_idx=1) for config in self.configs\n",
    "        )\n",
    "        for i in range(len(self.decoders)):\n",
    "            path = merge_config.model_paths[i]\n",
    "            state_dict = load_layer(path, layer_idx=1)\n",
    "            state_dict = strip_prefix(state_dict)\n",
    "            self.decoders[i].load_state_dict(\n",
    "                state_dict=state_dict\n",
    "            )\n",
    "        self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config):\n",
    "        self.merger = copy.deepcopy(self.decoders[0])\n",
    "        place_masks(self.merger, self.decoders)\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass\n",
    "        \n",
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.merge_config = merge_config\n",
    "        self.num_models = len(merge_config.model_paths)\n",
    "        self.configs = [\n",
    "            AutoConfig.from_pretrained(path) \n",
    "            for path in merge_config.model_paths\n",
    "        ]\n",
    "        # self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.models = nn.ModuleList([\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                merge_config.model_paths[i], \n",
    "                config=self.configs[i],\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ) \n",
    "            for i in range(self.num_models)\n",
    "        ])\n",
    "        self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config):\n",
    "        # dummy_config = copy.deepcopy(self.configs[0])\n",
    "        # dummy_config.update({\"hidden_size\": 1, \"intermediate_size\": 1})\n",
    "        # self.merger = AutoModelForCausalLM.from_config(dummy_config)\n",
    "        self.merger = copy.deepcopy(self.models[0])\n",
    "        place_masks(self.merger, self.models)\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d01ba414-f897-4cbe-8ccd-ce6ae3f721d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4d5091-ec85-4a42-a096-510c88fcaacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa1732391d14111962aacd9192e518d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f2ce0d7a1a4b50b927b12accbb17a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f84a774-2a5c-4e4c-aea1-3f3c0b8669d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.merger.model.embed_tokens.masked_embeddings[1].mask.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3495e227-f1e9-46b6-b08c-30d08cbdbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(merge_config.model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d9b6c5e-e825-4286-ab6c-87a9d99c361c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Merger(\n",
       "  (models): ModuleList(\n",
       "    (0-1): 2 x Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (merger): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): EmbeddingsWithMasks(\n",
       "        (masked_embeddings): ModuleList(\n",
       "          (0-1): 2 x EmbeddingWithMask(\n",
       "            (embedding): Embedding(151936, 2048)\n",
       "            (mask): Mask()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (k_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (v_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (o_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (up_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (down_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): RMSNormsWithMasks(\n",
       "            (masked_rms_norms): ModuleList(\n",
       "              (0-1): 2 x RMSNormWithMask(\n",
       "                (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                (mask): Mask()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNormsWithMasks(\n",
       "            (masked_rms_norms): ModuleList(\n",
       "              (0-1): 2 x RMSNormWithMask(\n",
       "                (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                (mask): Mask()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNormsWithMasks(\n",
       "        (masked_rms_norms): ModuleList(\n",
       "          (0-1): 2 x RMSNormWithMask(\n",
       "            (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (mask): Mask()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): LinearsWithMasks(\n",
       "      (masked_linears): ModuleList(\n",
       "        (0-1): 2 x LinearWithMask(\n",
       "          (linear): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "          (mask): Mask()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b82f53d-8ca0-4006-80f1-c47863ed9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_modules(module1, module2, rtol=1e-05, atol=1e-08, verbose=True):\n",
    "    \"\"\"\n",
    "    Compares the weights of two modules using torch.testing.assert_close.\n",
    "    Assumes modules have identical keys.\n",
    "    \"\"\"\n",
    "    state_dict1 = module1.state_dict()\n",
    "    state_dict2 = module2.state_dict()\n",
    "\n",
    "    # Iterate directly through the keys of one module's state_dict\n",
    "    for key in state_dict1:  \n",
    "        tensor1 = state_dict1[key]\n",
    "        tensor2 = state_dict2[key]\n",
    "\n",
    "        # No need for shape check, assumed to be identical\n",
    "        try:\n",
    "            torch.testing.assert_close(tensor1, tensor2, rtol=rtol, atol=atol)\n",
    "            if verbose:\n",
    "                print(f\"  OK: Tensor '{key}' is close within tolerance.\")\n",
    "        except AssertionError as e:\n",
    "            if verbose:\n",
    "                print(f\"  ERROR: Tensor '{key}' is NOT close within tolerance.\")\n",
    "            raise AssertionError(f\"Tensor '{key}' comparison failed: {e}\") from e\n",
    "    print(\"--- All tensors are identical! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a906f584-eae4-412c-bf8c-79f58dbcd096",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Merger(\n",
       "  (models): ModuleList(\n",
       "    (0-1): 2 x Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (merger): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): EmbeddingsWithMasks(\n",
       "        (masked_embeddings): ModuleList(\n",
       "          (0-1): 2 x EmbeddingWithMask(\n",
       "            (embedding): Embedding(151936, 2048)\n",
       "            (mask): Mask()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (k_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (v_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (o_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (up_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (down_proj): LinearsWithMasks(\n",
       "              (masked_linears): ModuleList(\n",
       "                (0-1): 2 x LinearWithMask(\n",
       "                  (linear): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "                  (mask): Mask()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): RMSNormsWithMasks(\n",
       "            (masked_rms_norms): ModuleList(\n",
       "              (0-1): 2 x RMSNormWithMask(\n",
       "                (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                (mask): Mask()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNormsWithMasks(\n",
       "            (masked_rms_norms): ModuleList(\n",
       "              (0-1): 2 x RMSNormWithMask(\n",
       "                (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                (mask): Mask()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNormsWithMasks(\n",
       "        (masked_rms_norms): ModuleList(\n",
       "          (0-1): 2 x RMSNormWithMask(\n",
       "            (rms_norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (mask): Mask()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): LinearsWithMasks(\n",
       "      (masked_linears): ModuleList(\n",
       "        (0-1): 2 x LinearWithMask(\n",
       "          (linear): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "          (mask): Mask()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b66d55e7-bf40-4788-a8c1-1e8a657c98a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OK: Tensor 'self_attn.q_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.q_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.k_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.k_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.v_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.v_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.o_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.gate_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.up_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.down_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'input_layernorm.weight' is close within tolerance.\n",
      "  OK: Tensor 'post_attention_layernorm.weight' is close within tolerance.\n",
      "--- All tensors are identical! ---\n",
      "  OK: Tensor 'self_attn.q_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.q_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.k_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.k_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.v_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.v_proj.bias' is close within tolerance.\n",
      "  OK: Tensor 'self_attn.o_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.gate_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.up_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'mlp.down_proj.weight' is close within tolerance.\n",
      "  OK: Tensor 'input_layernorm.weight' is close within tolerance.\n",
      "  OK: Tensor 'post_attention_layernorm.weight' is close within tolerance.\n",
      "--- All tensors are identical! ---\n"
     ]
    }
   ],
   "source": [
    "m1 = merger.models[0].model.layers[1]\n",
    "m2 = merger.models[1].model.layers[2]\n",
    "\n",
    "mo = merge_modules([m1, m2], [0.0, 1.0])\n",
    "compare_modules(mo, m2, verbose=True)\n",
    "\n",
    "mo = merge_modules([m1, m2], [1.0, 0.0])\n",
    "compare_modules(mo, m1, verbose=True)\n",
    "\n",
    "# mo = merge_modules([m1, m2], [0.5, 0.5])\n",
    "# compare_modules(mo, m1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74c6db45-87c8-4691-8f8d-e17a52eb68d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.state_dict()['q_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddf601b4-6d8a-402b-8b08-40ba5eaeb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, model, tokenizer, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.13,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.4,\n",
    "            top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()\n",
    "\n",
    "def get_logits(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_ids).logits\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6903352-4e32-48f5-bb38-390331434611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nContinue this text: A dog is a cat<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "prompt = \"Continue this text: A dog is a cat\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8678c6df-a9bc-4d5c-a683-fc0742c8e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_merger = get_logits(text, model=merger.merger, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4cfa9d0-f4df-45b0-a4fc-990a22637305",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_1 = get_logits(text, model=merger.models[0], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81bf0bdb-607e-4ec0-b6a7-e9584d817060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 8.0625,  6.9688,  3.6094,  ..., -0.8320, -0.8320, -0.8320],\n",
       "          [ 6.0312,  6.9688,  7.0938,  ..., -1.4219, -1.4219, -1.4219],\n",
       "          [ 5.8438,  8.7500, 11.1875,  ..., -3.9844, -3.9844, -3.9844],\n",
       "          ...,\n",
       "          [ 1.4141,  5.7500, -3.6562,  ..., -0.3164, -0.3164, -0.3164],\n",
       "          [ 9.3750,  7.2500,  8.1250,  ..., -6.0312, -6.0312, -6.0312],\n",
       "          [16.2500, 17.8750,  9.1875,  ..., -1.8438, -1.8438, -1.8438]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[ 2.8438,  2.4844,  2.6719,  ..., -0.5352, -0.5352, -0.5352],\n",
       "          [ 2.7344,  3.3281,  3.1406,  ..., -0.8789, -0.8789, -0.8789],\n",
       "          [ 2.5938,  2.7344,  4.2500,  ..., -2.1562, -2.1562, -2.1562],\n",
       "          ...,\n",
       "          [ 5.8438,  3.5781,  3.8906,  ..., -3.2969, -3.2969, -3.2969],\n",
       "          [ 6.3438,  3.3438,  5.0312,  ..., -3.2031, -3.2031, -3.2031],\n",
       "          [ 5.8750,  3.5469,  5.5000,  ..., -3.0000, -3.0000, -3.0000]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_1, logits_merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16fd13f9-f0bc-4b3a-822d-897b0af54035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", A\n",
      " the cat in the A is an helpful assistant for A and the cat, A.\n",
      " A  help to the A and the cat in A is a helpful assistant\n",
      ". A cat is not a, A, A\n",
      "\n",
      " A\n",
      "\n",
      " the cat is A, A\n",
      " and the cat in A\n",
      " is a useful cat.\n",
      " A is\n",
      " the cat in A is A\n",
      " the A\n",
      " and A cat in A is the A\n",
      " and A cat, A is A\n",
      " in\n"
     ]
    }
   ],
   "source": [
    "answer = generate(merger.merger, text, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d59988-02b4-4cb4-9216-3268006f859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e58f140-df44-4a7c-9ac7-d894b67a4498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2MLP"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(merger.merger.model.layers[0].mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "829aac8d-fb61-4247-ba34-7329e7703287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(merger.models[1].model.norm, Qwen2RMSNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4e701dc-24b4-4181-9564-844f55efda08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(merger.models[1].model.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb862b9a-4bca-44f1-afa4-4b5e1e453728",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Qwen2RMSNorm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[43mQwen2RMSNorm\u001b[49m(\u001b[38;5;241m8\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Qwen2RMSNorm' is not defined"
     ]
    }
   ],
   "source": [
    "(type(Qwen2RMSNorm(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adef19a-a9c4-4b0a-a00b-83bf6489f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate(merger.merger, text, max_new_tokens=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b420065-ba2c-4a8a-8035-95f7eed4320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merger.merger.self_attn.q_proj.masked_linears[1].linear.weight.data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070239c9-72e6-4412-9bbd-cc72de79f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merger.decoders[1].self_attn.q_proj.weight.data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d34500-e783-482d-a86a-708b4bab302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin = merger.decoders[0].mlp.gate_proj.weight\n",
    "# ref = merger.merger.mlp.gate_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf242f4-a5ff-4eec-83ff-4d633fefc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = Qwen2ForCausalLM.from_pretrained(\n",
    "#     merge_config.model_paths[0]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff26b74-cdc8-4ea2-8ebf-ca09d8268476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.model.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a9f5b-bf20-42f0-8e4c-d2a04d632347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp1 = model1.model.layers[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f49c66-d524-48a1-b8c9-9d7402500088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_mlp1 = replace_linears_with_masked(mlp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ce0e6-286f-422e-a909-10caa0059b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0460d67b-29c3-4c5b-8351-878056ca1249",
   "metadata": {},
   "source": [
    "## attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e9ff5d-7ec3-49ec-b3c3-5f416dea9511",
   "metadata": {},
   "outputs": [],
   "source": [
    "major_revision = \"\"\"\n",
    "In this version, I will try to merge weight before forwarding. This will\n",
    "accurately reflect what happens when using a model with merged weights, avoiding\n",
    "number precision problems.\n",
    "\n",
    "In previous attempts, I do forward with multiple weights to get multiple outputs,\n",
    "and then I merge outputs by taking sum of them.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901e231a-9965-4355-ac30-4784d493e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.weight = nn.Parameter(torch.tensor(value)) # Corrected typo here\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        try:\n",
    "            self.weight * torch.rand(self.size)\n",
    "        except RuntimeError:\n",
    "            print(\"mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.weight * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad488f6a-8d6a-4e10-addd-7e45772a0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union, List\n",
    "from transformers import PretrainedConfig\n",
    "\n",
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.weight = nn.Parameter(torch.tensor(value))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        if self.size is not None:\n",
    "            try:\n",
    "                self.weight * torch.rand(self.size)\n",
    "            except RuntimeError:\n",
    "                print(\"mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.size is None:\n",
    "            return self.weight * x\n",
    "        else:\n",
    "            if self.size != x.shape:\n",
    "                print(\"The shape of input does not match that of the mask.\")\n",
    "            return self.weight * x\n",
    "\n",
    "class LinearWithMask(nn.Module):\n",
    "    def __init__(self, linear, weight_mask_config: MaskConfig, bias_mask_config: MaskConfig = None):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.weight_mask_config = weight_mask_config\n",
    "        self.bias_mask_config = bias_mask_config\n",
    "\n",
    "        if linear.weight.shape != weight_mask_config.size:\n",
    "            print(\"Weight mask shape is not compatible with linear, reinitializing...\")\n",
    "            self.weight_mask_config.size = linear.weight.shape\n",
    "        self.weight_mask = Mask(self.weight_mask_config)\n",
    "\n",
    "        if linear.bias is not None and bias_mask_config is not None:\n",
    "            if linear.bias.shape != bias_mask_config.size:\n",
    "                print(\"Bias mask shape is not compatible with linear, reinitializing...\")\n",
    "                self.bias_mask_config.size = linear.bias.shape\n",
    "            self.bias_mask = Mask(self.bias_mask_config)\n",
    "        else:\n",
    "            self.bias_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        masked_weight = self.weight_mask(self.linear.weight)\n",
    "        if self.linear.bias is not None and self.bias_mask is not None:\n",
    "            masked_bias = self.bias_mask(self.linear.bias)\n",
    "        else:\n",
    "            masked_bias = self.linear.bias\n",
    "        return nn.functional.linear(x, masked_weight, masked_bias)\n",
    "\n",
    "class LinearsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        linears: List[nn.Module],\n",
    "        weight_modes: List[str] = [\"scalar\"],\n",
    "        weight_values: List[float] = None,\n",
    "        bias_modes: List[str] = [\"scalar\"],\n",
    "        bias_values: List[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not all(isinstance(linear, nn.Linear) for linear in linears):\n",
    "            raise ValueError(\"All elements in 'linears' must be instances of nn.Linear.\")\n",
    "\n",
    "        weight_sizes = [linear.weight.shape for linear in linears]\n",
    "        bias_sizes = [linear.bias.shape if linear.bias is not None else None for linear in linears]\n",
    "        \n",
    "        if weight_values is None or len(weight_values) != len(linears):\n",
    "            raise ValueError(f\"weight_values for masks: {weight_values} do not match with linear layers: {linears}\")\n",
    "        if bias_values is None:\n",
    "            bias_values = [None] * len(linears)\n",
    "        if len(bias_values) != len(linears):\n",
    "            raise ValueError(f\"bias_values for masks: {bias_values} do not match with linear layers: {linears}\")\n",
    "\n",
    "        weight_mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(weight_modes, weight_values, weight_sizes)\n",
    "        ]\n",
    "        bias_mask_configs = [\n",
    "            MaskConfig(mode, value, size) if size is not None else None\n",
    "            for mode, value, size in zip(bias_modes, bias_values, bias_sizes)\n",
    "        ]\n",
    "\n",
    "        self.masked_linears = nn.ModuleList(\n",
    "            [LinearWithMask(linear, weight_mask_config, bias_mask_config)\n",
    "             for linear, weight_mask_config, bias_mask_config \n",
    "             in zip(linears, weight_mask_configs, bias_mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = [linear.weight_mask(linear.linear.weight) \n",
    "                   for linear in self.masked_linears]\n",
    "        merged_weight = torch.sum(torch.stack(weights), dim=0)\n",
    "\n",
    "        biases = [\n",
    "            linear.bias_mask(linear.linear.bias) \n",
    "            if linear.linear.bias is not None and linear.bias_mask is not None \n",
    "            else linear.linear.bias for linear in self.masked_linears\n",
    "        ]\n",
    "        merged_bias = torch.sum(torch.stack(\n",
    "            [b if b is not None \n",
    "             else torch.zeros_like(weights[0][:,0]) \n",
    "             for b in biases]), dim=0\n",
    "        )\n",
    "        if all(b is None for b in biases):\n",
    "            merged_bias = None\n",
    "\n",
    "        return nn.functional.linear(x, merged_weight, merged_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a1544e1-1acb-452f-be2d-5369732d6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_linear_components(input_size: int, output_size: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        # Test with bias\n",
    "        linears_with_bias = [nn.Linear(input_size, output_size, bias=True) for _ in range(num_components)]\n",
    "        # Test without bias\n",
    "        linears_without_bias = [nn.Linear(input_size, output_size, bias=False) for _ in range(num_components)]\n",
    "        \n",
    "        x = torch.rand(1, input_size)\n",
    "\n",
    "        for _ in range(10):  # Reduced number of iterations for faster testing\n",
    "            weight_values = np.random.rand(num_components).tolist()\n",
    "            bias_values = np.random.rand(num_components).tolist()\n",
    "\n",
    "            # Test with bias\n",
    "            weights_with_masks_bias = LinearsWithMasks(\n",
    "                linears=linears_with_bias,\n",
    "                weight_modes=[\"scalar\"] * num_components,\n",
    "                weight_values=weight_values,\n",
    "                bias_modes=[\"scalar\"] * num_components,\n",
    "                bias_values=bias_values,\n",
    "            )\n",
    "            if True:\n",
    "                outs_weights_only = [torch.matmul(x, w_mask * lin.weight.T) \n",
    "                                     for w_mask, lin in zip(weight_values, linears_with_bias)]\n",
    "                outs_biases_only = [b_mask * lin.bias.T for b_mask, lin in zip(bias_values, linears_with_bias)]\n",
    "                expected_output_bias = sum([w + b for w, b in zip(outs_weights_only, outs_biases_only)])\n",
    "                \n",
    "                actual_output_bias = weights_with_masks_bias(x)\n",
    "    \n",
    "                torch.testing.assert_close(actual_output_bias, expected_output_bias, rtol=1e-6, atol=1e-6)\n",
    "\n",
    "            # Test with merged weights\n",
    "            if True:\n",
    "                merged_linear = merge_linears(linears_with_bias, weight_factors=weight_values, bias_factors=bias_values)\n",
    "                expected_output_bias = merged_linear(x)\n",
    "                actual_output_bias = weights_with_masks_bias(x)\n",
    "                torch.testing.assert_close(actual_output_bias, expected_output_bias, rtol=1e-6, atol=1e-6)\n",
    "            \n",
    "\n",
    "            # Test without bias\n",
    "            weights_with_masks_no_bias = LinearsWithMasks(\n",
    "                linears=linears_without_bias,\n",
    "                weight_modes=[\"scalar\"] * num_components,\n",
    "                weight_values=weight_values,\n",
    "                bias_modes=[\"scalar\"] * num_components,  # This won't be used, but we still need to provide it\n",
    "                bias_values=[None] * num_components, # Indicate no bias masks\n",
    "            )\n",
    "            if True:\n",
    "                outs_weights_only = [torch.matmul(x, w_mask * lin.weight.T) \n",
    "                                     for w_mask, lin in zip(weight_values, linears_without_bias)]\n",
    "                expected_output = sum([o for o in outs_weights_only])\n",
    "                actual_output = weights_with_masks_bias(x)\n",
    "    \n",
    "                torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "                \n",
    "            # Test with merged weights\n",
    "            if True:\n",
    "                merged_linear = merge_linears(linears_without_bias, weight_factors=weight_values, bias_factors=bias_values)\n",
    "                expected_output = merged_linear(x)\n",
    "                actual_output = weights_without_masks_bias(x)\n",
    "                torch.testing.assert_close(actual_output, expected_output, rtol=1e-6, atol=1e-6)\n",
    "        \n",
    "        logging.info(f\"Test with {num_components} Linear components passed (both with and without bias)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a72556f8-e54d-4850-93c4-a65e01e4eb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1264,  0.5219, -0.3976,  0.0520, -0.2465,  0.0734, -0.0656, -0.1128]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(input_size, output_size, bias=False)\n",
    "x = torch.rand(1, input_size)\n",
    "torch.matmul(x, lin.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c8b6533-bdcf-412d-ab1a-301688f45fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 08:56:22,229 - INFO - LinearsWithMasks Tests\n",
      "2024-12-14 08:56:22,540 - INFO - Test with 2 Linear components passed (both with and without bias)!\n",
      "2024-12-14 08:56:23,118 - INFO - Test with 4 Linear components passed (both with and without bias)!\n",
      "2024-12-14 08:56:23,832 - INFO - Test with 5 Linear components passed (both with and without bias)!\n",
      "2024-12-14 08:56:24,978 - INFO - Test with 8 Linear components passed (both with and without bias)!\n",
      "2024-12-14 08:56:26,411 - INFO - Test with 10 Linear components passed (both with and without bias)!\n",
      "2024-12-14 08:56:28,094 - INFO - Test with 12 Linear components passed (both with and without bias)!\n",
      "2024-12-14 08:56:30,337 - INFO - Test with 16 Linear components passed (both with and without bias)!\n",
      "2024-12-14 08:56:33,128 - INFO - Test with 20 Linear components passed (both with and without bias)!\n",
      "2024-12-14 08:56:37,483 - INFO - Test with 30 Linear components passed (both with and without bias)!\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "num_components_list = [2, 4, 5, 8, 10, 12, 16, 20, 30]\n",
    "# Linear Test ----------------------------------------------------------------- #\n",
    "logging.info(\"LinearsWithMasks Tests\")\n",
    "input_size = 1024\n",
    "output_size = 1024\n",
    "test_multiple_linear_components(input_size, output_size, num_components_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
