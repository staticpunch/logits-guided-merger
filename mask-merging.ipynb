{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754cc9c2-a335-4612-9409-ab21f754ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = \"\"\"\n",
    "1. prepare data\n",
    "2. define model\n",
    "    - model a, mask a\n",
    "    - model b, mask b\n",
    "make only masks trainable.\n",
    "make sure it has correct inference.\n",
    "3. define loss function in Trainer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4884f-b796-4384-a2ad-177f2314c3e2",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6baca-8a9b-4a41-8e32-01d6987ae939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "332cf659-c413-4f11-889a-79532efcdd97",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6167e22d-21c5-4027-af0b-a029c8acfe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from modeling_qwen2 import (\n",
    "    Qwen2RMSNorm, \n",
    "    Qwen2RotaryEmbedding, \n",
    "    Qwen2MLP, \n",
    "    Qwen2Attention, \n",
    "    Qwen2FlashAttention2, \n",
    "    Qwen2SdpaAttention, \n",
    "    Qwen2DecoderLayer, \n",
    "    Qwen2PreTrainedModel, \n",
    "    Qwen2Model, \n",
    "    Qwen2ForCausalLM\n",
    ")\n",
    "\n",
    "from configuration_qwen2 import Qwen2Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b59dc3-83fc-4345-b4f6-5cf7665fa607",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fd625a-64ce-4a36-b16d-1b6e61eaa5c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8256a2-89d0-4cfa-8726-1ac197a04470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc68586-edfa-465f-a1e8-513541ccac5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/models/Arcee-VyLinh/', '/workspace/models/Qwen2.5-Coder-3B/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config.model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eca4d1f-96da-4126-bc64-ed531f46d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.model_paths[0]\n",
    "        )\n",
    "        \n",
    "        self.models = nn.ModuleList([\n",
    "            Qwen2ForCausalLM.from_pretrained(\n",
    "            # AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                # torch_dtype=torch.bfloat16,\n",
    "                # device_map={\"\":0}\n",
    "            ) for model_path in config.model_paths\n",
    "        ])\n",
    "        self.__post_init__()\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        # self.masks = torch.nn\n",
    "        pass\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        \"\"\"\n",
    "        activations = []\n",
    "        for i in range(num_layers):\n",
    "            L1 = models[0].layers[i]\n",
    "            L2 = models[1].layers[i]\n",
    "            Lm = alpha * L1 + beta * L2\n",
    "            h1 = L1(h)\n",
    "            h2 = L2(h)\n",
    "            h = Lm(h)\n",
    "            activations.append({\n",
    "                \"1\": h1, \"2\": h2, \"merged\": copy(h)\n",
    "            })\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        - embed_tokens\n",
    "        - norm\n",
    "        - layers\n",
    "            - input_layernorm\n",
    "            - self_attn\n",
    "            - mlp\n",
    "            - post_attention_norm\n",
    "        - lm_head\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd988e00-b69f-4325-bb22-220a26ef033f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336da23a9c3f412085476e73089dc5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7626fe82064cfd822b91b96860eb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merger = Merger(merge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57ee117-eee6-4233-adfb-58e8b61cd7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger.models[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4912f19f-b0cc-49b1-8a2e-14006fc6f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1 = merger.models[0].model.layers[0].self_attn\n",
    "attn2 = merger.models[1].model.layers[0].self_attn\n",
    "mlp1 = merger.models[0].model.layers[0].mlp\n",
    "mlp2 = merger.models[1].model.layers[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1116d86-d228-4aed-b6ac-1098c044ea62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0159, -0.0432, -0.0080,  ...,  0.0081,  0.0096,  0.0132],\n",
       "        [-0.0330,  0.0110,  0.0085,  ...,  0.0226, -0.0082,  0.0457],\n",
       "        [-0.0092,  0.0111, -0.0134,  ...,  0.0298,  0.0113, -0.0038],\n",
       "        ...,\n",
       "        [-0.0085,  0.0601, -0.0325,  ...,  0.0525, -0.0222,  0.0403],\n",
       "        [-0.0374, -0.0325,  0.0620,  ..., -0.0206,  0.0806,  0.0376],\n",
       "        [ 0.0356,  0.0151,  0.0087,  ..., -0.0306, -0.0072,  0.0378]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn1.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ceff9f2-8598-4fa4-99e4-f584ec495af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from typing import List, Dict\n",
    "\n",
    "def merge_linear(weights: List[nn.Linear], factors: List[float]) -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Merges multiple linear layers by taking a weighted average of their weights and biases.\n",
    "\n",
    "    Args:\n",
    "        weights: A list of nn.Linear layers to merge.\n",
    "        factors: A list of scaling factors corresponding to each layer in 'weights'.\n",
    "\n",
    "    Returns:\n",
    "        A new nn.Linear layer that is the weighted average of the input layers.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of weights and factors don't match, or if the\n",
    "                    layers have incompatible dimensions, device or dtype.\n",
    "    \"\"\"\n",
    "    if len(weights) != len(factors):\n",
    "        raise ValueError(\"The number of weights and factors must be equal.\")\n",
    "\n",
    "    # Check for compatibility, device, and dtype\n",
    "    device = weights[0].weight.device\n",
    "    dtype = weights[0].weight.dtype\n",
    "    if not all(\n",
    "        w.in_features == weights[0].in_features\n",
    "        and w.out_features == weights[0].out_features\n",
    "        and w.weight.device == device\n",
    "        and w.weight.dtype == dtype\n",
    "        for w in weights\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Incompatible linear layers for merging. They must have the same in_features, out_features, device, and dtype.\"\n",
    "        )\n",
    "\n",
    "    # Create a new linear layer with the same dimensions, device and dtype\n",
    "    merged_linear = nn.Linear(\n",
    "        in_features=weights[0].in_features,\n",
    "        out_features=weights[0].out_features,\n",
    "        bias=False,\n",
    "        device=device,\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "    # Calculate the merged weight and bias\n",
    "    merged_weight = torch.zeros_like(weights[0].weight)\n",
    "    merged_bias = (\n",
    "        torch.zeros_like(weights[0].bias, device=device, dtype=dtype)\n",
    "        if weights[0].bias is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        merged_weight += factors[i] * w.weight\n",
    "        if w.bias is not None:\n",
    "            if merged_bias is None:\n",
    "                raise ValueError(\"Cannot merge linear layers if only some have biases.\")\n",
    "            merged_bias += factors[i] * w.bias\n",
    "\n",
    "    # Assign the merged weight and bias to the new layer\n",
    "    with torch.no_grad():\n",
    "        merged_linear.weight.copy_(merged_weight)\n",
    "        if merged_bias is not None:\n",
    "            merged_linear.bias = nn.Parameter(merged_bias)\n",
    "\n",
    "    return merged_linear\n",
    "\n",
    "def merge_module_recursive(\n",
    "    target_module: nn.Module, modules_dict: Dict[str, List[nn.Module]], factors: List[float]\n",
    "):\n",
    "    \"\"\"\n",
    "    Recursively merges multiple modules by taking a weighted average of their Linear layer weights and biases.\n",
    "\n",
    "    Args:\n",
    "        target_module: The target module where the merged weights will be stored.\n",
    "        modules_dict: A dictionary where keys are module names and values are lists of modules to merge.\n",
    "        factors: A list of scaling factors corresponding to each list of modules.\n",
    "    \"\"\"\n",
    "\n",
    "    for name, module in target_module.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if name not in modules_dict:\n",
    "                raise ValueError(\n",
    "                    f\"Missing module {name} in modules_dict. Make sure all linear layer weights are provided\"\n",
    "                )\n",
    "            merged_linear = merge_linear(modules_dict[name], factors)\n",
    "            # Find the parent module\n",
    "            parent_module_name = \".\".join(name.split(\".\")[:-1])\n",
    "            layer_name = name.split(\".\")[-1]\n",
    "\n",
    "            if parent_module_name:\n",
    "                parent_module = target_module.get_submodule(parent_module_name)\n",
    "            else:\n",
    "                parent_module = target_module\n",
    "\n",
    "            # Replace the original Linear layer with merged one\n",
    "            setattr(parent_module, layer_name, merged_linear)\n",
    "\n",
    "def merge_modules(modules: List[nn.Module], factors: List[float]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Merges multiple modules by taking a weighted average of their Linear layer weights and biases.\n",
    "    The merged weights are stored into a deepcopy of the first module in the list.\n",
    "\n",
    "    Args:\n",
    "        modules: A list of nn.Modules to merge.\n",
    "        factors: A list of scaling factors corresponding to each module in 'modules'.\n",
    "\n",
    "    Returns:\n",
    "        A new nn.Module that is the weighted average of the input modules.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of modules and factors don't match.\n",
    "    \"\"\"\n",
    "    if len(modules) != len(factors):\n",
    "        raise ValueError(\"The number of modules and factors must be equal.\")\n",
    "\n",
    "    # Check device and dtype consistency across all modules\n",
    "    device = modules[0].parameters().__next__().device\n",
    "    dtype = modules[0].parameters().__next__().dtype\n",
    "    if not all(p.device == device and p.dtype == dtype for module in modules for p in module.parameters()):\n",
    "        raise ValueError(\"All modules must be on the same device and have the same dtype.\")\n",
    "\n",
    "    # Create a deep copy of the first module to store the merged weights\n",
    "    merged_module = copy.deepcopy(modules[0])\n",
    "\n",
    "    # Dictionary to hold corresponding linear layers from each module\n",
    "    modules_to_merge = {\n",
    "        name: [] for name, _ in merged_module.named_modules() if isinstance(_, nn.Linear)\n",
    "    }\n",
    "    for module in modules:\n",
    "        for name, layer in module.named_modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                modules_to_merge[name].append(layer)\n",
    "\n",
    "    # Merge the modules recursively\n",
    "    merge_module_recursive(merged_module, modules_to_merge, factors)\n",
    "\n",
    "    # Ensure the merged module has the correct device and dtype\n",
    "    merged_module.to(device=device, dtype=dtype)\n",
    "\n",
    "    return merged_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c6e197-7a64-4c61-ae4c-01ec81fc5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mlp = merge_modules(modules=[mlp1, mlp2], factors=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4f12e66-a7cf-40b4-b099-95d18d57aba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mQwen2MLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Define the computation performed at every call.\n",
       "\n",
       "Should be overridden by all subclasses.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within\n",
       "    this function, one should call the :class:`Module` instance afterwards\n",
       "    instead of this since the former takes care of running the\n",
       "    registered hooks while the latter silently ignores them.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /workspace/merge/modeling_qwen2.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Qwen2MLP.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc55cde6-7362-43bc-b36e-c0fd8b6371ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_merged_mlp(x: torch.Tensor, modules: List[nn.Module], factors: List[float]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a forward pass that simulates the behavior of a merged MLP module.\n",
    "\n",
    "    Args:\n",
    "        modules: A list of MLP modules (e.g., Qwen2MLP instances).\n",
    "        factors: A list of scaling factors corresponding to each module in 'modules'.\n",
    "        x: The input tensor.\n",
    "\n",
    "    Returns:\n",
    "        The output tensor after the forward pass.\n",
    "    \"\"\"\n",
    "    factors = torch.tensor(factors).to(x.device, dtype=x.dtype).view(-1, 1, 1, 1)  # Reshape factors for broadcasting\n",
    "    gate_output = torch.stack([m.gate_proj(x) for m in modules]).mul(factors).sum(0)\n",
    "    up_output = torch.stack([m.up_proj(x) for m in modules]).mul(factors).sum(0)\n",
    "    act_output = modules[0].act_fn(gate_output)  # Assuming all modules have the same activation function\n",
    "    result = torch.stack([m.down_proj(act_output * up_output) for m in modules]).mul(factors).sum(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3986afd-841a-4795-b774-2e9758ce1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = merged_mlp.parameters().__next__().device\n",
    "dtype = merged_mlp.parameters().__next__().dtype\n",
    "x = torch.rand(1, 4, 2048).to(device, dtype=dtype)\n",
    "o1 = merged_mlp(x)\n",
    "o2 = forward_merged_mlp(x, modules=[mlp1, mlp2], factors=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d65bd5d4-2ce1-4e0d-a655-254bc03fc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(o1, o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65e11c1a-3abb-43f9-81fa-8c25a0a045bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules[0].down_proj.weight\n",
    "gate_output.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "286a1532-4644-4142-ab2b-029b4d4fefd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3750, -0.3301, -0.0016,  ..., -0.0093, -0.2695,  0.0986],\n",
       "          [ 0.2930, -0.2949,  0.0933,  ...,  0.0483, -0.2949, -0.0649],\n",
       "          [ 0.2676, -0.3789,  0.1973,  ...,  0.0134, -0.7227,  0.1367],\n",
       "          [ 0.2715, -0.2988, -0.0547,  ..., -0.1777, -0.7695,  0.0850]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda:0\"\n",
    "h = torch.rand(1, 4, 2048, dtype=torch.bfloat16).to(device)\n",
    "p = torch.arange(4, dtype=torch.bfloat16, device=device).unsqueeze(0)\n",
    "attn1.forward(h, position_ids=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93147db6-84be-43db-9559-dd03c4c5988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, param_bits):\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    non_trainable_params = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        num_params = param.numel()  # Get the number of elements in the parameter\n",
    "        total_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "        else:\n",
    "            non_trainable_params += num_params\n",
    "\n",
    "    total_gigabytes = total_params * (param_bits / 8) / (1024**3)\n",
    "    memory = f\"{total_gigabytes:.2f} GB\"\n",
    "    \n",
    "    return total_params, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a75cc9e3-c80c-4a70-9d0c-b71da5fa1cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9439744, '0.02 GB'), (67633152, '0.13 GB'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(attn1, 16), count_parameters(mlp1, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59306b-6580-4484-a88a-0c39f30c8bb8",
   "metadata": {},
   "source": [
    "## attemtp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a618695-55f3-4894-90dc-81a6de0de0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 07:12:16,406 - INFO - Comparing tokenizer at /workspace/models/Arcee-VyLinh/ with tokenizer at /workspace/models/Qwen2.5-Coder-3B/\n",
      "2024-12-12 07:12:16,409 - INFO - Tokenizer at /workspace/models/Arcee-VyLinh/ and /workspace/models/Qwen2.5-Coder-3B/ are the same based on the defined criteria\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import are_tokenizers_same\n",
    "are_tokenizers_same(\n",
    "    paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25550585-74ef-4ba2-9e4e-c6f203fbd053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = None,\n",
    "        value: Union[float, torch.Tensor] = None,\n",
    "        size: torch.Size = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.size = size\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mask_config: MaskConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        now only support mode == scalar\n",
    "        \"\"\"\n",
    "        self.mode = mask_config.mode\n",
    "        if mask_config.mode == \"scalar\":\n",
    "            value = mask_config.value if mask_config.value is not None else 1\n",
    "            self.mask = nn.Parameter(torch.tensor(value)) # Corrected typo here\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask mode: {mask_config.mode}\")\n",
    "            \n",
    "        self.size = mask_config.size ## Full size of the mask after broadcast.\n",
    "        try:\n",
    "            self.mask * torch.rand(self.size)\n",
    "        except RuntimeError:\n",
    "            print(\"mask initialized with an incompatible shape.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mask * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a6d433-7a0f-4b54-afe0-6623ab70c4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Size((4, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ceee65-d173-4e63-958a-1d7cb423b5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskConfig {\n",
       "  \"mode\": \"scalar\",\n",
       "  \"size\": [\n",
       "    4,\n",
       "    8\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"value\": 0.5\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_config = MaskConfig(\n",
    "    mode=\"scalar\", value=0.5, size=torch.Size((4, 8))\n",
    ")\n",
    "mask_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97c1c765-7d62-4000-bbcf-34072ab1947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithMask(nn.Module):\n",
    "    def __init__(self, linear, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.mask_config = mask_config\n",
    "        if linear.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not imcompatible with linear, reinitializing...\")\n",
    "        self.mask_config.size = linear.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        masked_linear = self.mask(self.linear.weight)\n",
    "        return nn.functional.linear(x, masked_linear, self.linear.bias)\n",
    "\n",
    "class LinearsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        linears: List[nn.Module], \n",
    "        modes: List[str] = [\"scalar\"], \n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [linear.weight.shape for linear in linears]\n",
    "        if values is None or len(values) != len(linears):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with linear layers: {linears}\")\n",
    "            \n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size) \n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_linears = nn.ModuleList(\n",
    "            [LinearWithMask(linear, mask_config) \n",
    "             for linear, mask_config in zip(linears, mask_configs)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = 0.0\n",
    "        for masked_linear in self.masked_linears:\n",
    "            output += masked_linear(x)\n",
    "        return output\n",
    "            \n",
    "def replace_linears_with_masked(module):\n",
    "    \"\"\"\n",
    "    Recursively replaces Linear layers in a module with LinearWithMask layers.\n",
    "    \n",
    "    Args:\n",
    "      module: The module in which to replace layers.\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            setattr(module, name, LinearWithMask(child))\n",
    "        else:\n",
    "            replace_linears_with_masked(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc42d400-1dc8-4413-96de-9b0fb0123c44",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with 1 components passed!\n",
      "Test with 2 components passed!\n",
      "Test with 3 components passed!\n",
      "Test with 4 components passed!\n",
      "Test with 5 components passed!\n",
      "Test with 6 components passed!\n",
      "Test with 7 components passed!\n",
      "Test with 8 components passed!\n",
      "Test with 9 components passed!\n",
      "Test with 10 components passed!\n",
      "Test with 11 components passed!\n",
      "Test with 12 components passed!\n",
      "Test with 13 components passed!\n",
      "Test with 14 components passed!\n",
      "Test with 15 components passed!\n",
      "Test with 16 components passed!\n",
      "Test with 17 components passed!\n",
      "Test with 18 components passed!\n",
      "Test with 19 components passed!\n",
      "Test with 20 components passed!\n"
     ]
    }
   ],
   "source": [
    "# --- Testing ---\n",
    "def test_multiple_components(input_size: int, output_size: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        linears = [nn.Linear(input_size, output_size, bias=False) for _ in range(num_components)]\n",
    "        x = torch.rand(1, input_size)\n",
    "\n",
    "        for _ in range(10):  # Reduced number of iterations for faster testing in a notebook\n",
    "            values = np.random.rand(num_components).tolist() # cast to list\n",
    "            weights_with_masks = LinearsWithMasks(linears=linears, modes=[\"scalar\"] * num_components, values=values)\n",
    "\n",
    "            individual_outputs = [linear(x) for linear in linears]\n",
    "            expected_output = sum(val * out for val, out in zip(values, individual_outputs))\n",
    "            actual_output = weights_with_masks(x)\n",
    "\n",
    "            assert torch.allclose(actual_output, expected_output, rtol=1e-5, atol=1e-5), \"Outputs do not match!\"\n",
    "        print(f\"Test with {num_components} components passed!\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define input and output sizes\n",
    "input_size = 4\n",
    "output_size = 8\n",
    "\n",
    "# Run tests\n",
    "test_multiple_components(input_size, output_size, [i + 1 for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b5a66a8-d14b-43eb-84e3-de2fb0d4bd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with 1 RMSNorm components passed!\n",
      "Test with 2 RMSNorm components passed!\n",
      "Test with 3 RMSNorm components passed!\n",
      "Test with 4 RMSNorm components passed!\n",
      "Test with 5 RMSNorm components passed!\n",
      "Test with 6 RMSNorm components passed!\n",
      "Test with 7 RMSNorm components passed!\n",
      "Test with 8 RMSNorm components passed!\n",
      "Test with 9 RMSNorm components passed!\n",
      "Test with 10 RMSNorm components passed!\n",
      "Test with 11 RMSNorm components passed!\n",
      "Test with 12 RMSNorm components passed!\n",
      "Test with 13 RMSNorm components passed!\n",
      "Test with 14 RMSNorm components passed!\n",
      "Test with 15 RMSNorm components passed!\n",
      "Test with 16 RMSNorm components passed!\n",
      "Test with 17 RMSNorm components passed!\n",
      "Test with 18 RMSNorm components passed!\n",
      "Test with 19 RMSNorm components passed!\n",
      "Test with 20 RMSNorm components passed!\n"
     ]
    }
   ],
   "source": [
    "class RMSNormWithMask(nn.Module):\n",
    "    def __init__(self, rms_norm: Qwen2RMSNorm, mask_config: MaskConfig):\n",
    "        super().__init__()\n",
    "        self.rms_norm = rms_norm\n",
    "        self.mask_config = mask_config\n",
    "        if rms_norm.weight.shape != mask_config.size:\n",
    "            print(\"Mask shape is not compatible with RMSNorm, reinitializing...\")\n",
    "        self.mask_config.size = rms_norm.weight.shape\n",
    "        self.mask = Mask(self.mask_config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        masked_weight = self.mask(self.rms_norm.weight)\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.rms_norm.variance_epsilon)\n",
    "        return masked_weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class RMSNormsWithMasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rms_norms: List[Qwen2RMSNorm],\n",
    "        modes: List[str] = [\"scalar\"],\n",
    "        values: List[float] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        sizes = [rms_norm.weight.shape for rms_norm in rms_norms]\n",
    "        if values is None or len(values) != len(rms_norms):\n",
    "            raise ValueError(f\"values for masks: {values} do not match with RMSNorm layers: {rms_norms}\")\n",
    "\n",
    "        mask_configs = [\n",
    "            MaskConfig(mode, value, size)\n",
    "            for mode, value, size in zip(modes, values, sizes)\n",
    "        ]\n",
    "        self.masked_rms_norms = nn.ModuleList(\n",
    "            [RMSNormWithMask(rms_norm, mask_config)\n",
    "             for rms_norm, mask_config in zip(rms_norms, mask_configs)]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        output = 0.0\n",
    "        for masked_rms_norm in self.masked_rms_norms:\n",
    "            output += masked_rms_norm(hidden_states)\n",
    "        return output\n",
    "\n",
    "def test_multiple_rms_norm_components(hidden_size: int, num_components_list: List[int]):\n",
    "    for num_components in num_components_list:\n",
    "        rms_norms = [Qwen2RMSNorm(hidden_size) for _ in range(num_components)]\n",
    "        hidden_states = torch.rand(2, 4, hidden_size)\n",
    "\n",
    "        for _ in range(10):\n",
    "            values = np.random.rand(num_components).tolist()\n",
    "            rms_norms_with_masks = RMSNormsWithMasks(rms_norms=rms_norms, modes=[\"scalar\"] * num_components, values=values)\n",
    "\n",
    "            individual_outputs = [rms_norm(hidden_states) for rms_norm in rms_norms]\n",
    "            expected_output = sum(val * out for val, out in zip(values, individual_outputs))\n",
    "            actual_output = rms_norms_with_masks(hidden_states)\n",
    "            \n",
    "            assert torch.allclose(actual_output, expected_output, rtol=1e-5, atol=1e-5), \"Outputs do not match!\"\n",
    "        print(f\"Test with {num_components} RMSNorm components passed!\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define input and output sizes\n",
    "input_size = 4\n",
    "output_size = 8\n",
    "hidden_size = 16\n",
    "\n",
    "# Run tests for RMSNormsWithMasks\n",
    "test_multiple_rms_norm_components(hidden_size, [i+1 for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf260def-3212-4747-9cd0-cdd07127b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_paths: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.model_paths = model_paths\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "class Merger(PreTrainedModel):\n",
    "    def __init__(self, merge_config):\n",
    "        super().__init__(merge_config)\n",
    "        \"\"\"\n",
    "        Need to check whether models are mergeable (having some sort of the same config)\n",
    "        \"\"\"\n",
    "        self.config = Qwen2Config.from_pretrained(\n",
    "            merge_config.model_paths[0]\n",
    "        )\n",
    "        self.merger = Qwen2ForCausalLM(self.config)\n",
    "        self.__post_init__(merge_config)\n",
    "        \n",
    "    def __post_init__(self, merge_config):\n",
    "        # self.masks = torch.nn\n",
    "        pass\n",
    "        \n",
    "    def forward(self, tensor, labels=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d01ba414-f897-4cbe-8ccd-ce6ae3f721d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MergerConfig {\n",
       "  \"model_paths\": [\n",
       "    \"/workspace/models/Arcee-VyLinh/\",\n",
       "    \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.46.3\"\n",
       "}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_config = MergerConfig(\n",
    "    model_paths = [\n",
    "        \"/workspace/models/Arcee-VyLinh/\",\n",
    "        \"/workspace/models/Qwen2.5-Coder-3B/\"\n",
    "    ]\n",
    ")\n",
    "merge_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bf242f4-a5ff-4eec-83ff-4d633fefc40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4432e93d9dc64430a7287eb3e77a8cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = Qwen2ForCausalLM.from_pretrained(\n",
    "    merge_config.model_paths[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cff26b74-cdc8-4ea2-8ebf-ca09d8268476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2RMSNorm((2048,), eps=1e-06)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "342a9f5b-bf20-42f0-8e4c-d2a04d632347",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp1 = model1.model.layers[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7f49c66-d524-48a1-b8c9-9d7402500088",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_mlp1 = replace_linears_with_masked(mlp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df1ce0e6-286f-422e-a909-10caa0059b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2MLP(\n",
       "  (gate_proj): LinearWithMask(\n",
       "    (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "    (mask): Mask()\n",
       "  )\n",
       "  (up_proj): LinearWithMask(\n",
       "    (linear): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "    (mask): Mask()\n",
       "  )\n",
       "  (down_proj): LinearWithMask(\n",
       "    (linear): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "    (mask): Mask()\n",
       "  )\n",
       "  (act_fn): SiLU()\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b4d5091-ec85-4a42-a096-510c88fcaacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger = Merger(merge_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
